{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x131dee710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## load embeddings #######\n",
    "loc_emb = torch.load('data/locationEmbeddings.pt') \n",
    "des_emb = torch.load('data/descriptionEmbeddings.pt') \n",
    "#twt_emb = torch.load('data/tweetsEmbeddings.pt') \n",
    "\n",
    "#load network embedding\n",
    "#net_emb = gsm.KeyedVectors.load_word2vec_format('data/userNetworkEmd.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user = net_emb ['000mrs000']\n",
    "#print(user)\n",
    "#print(type(net_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 1300 user location, description, yoga tweets, utype, umotivation\n",
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "#print (df) #[1308 rows x 7 columns] name, location, description, text, utype, umotivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create BiLSTMAttention Model for Description\n",
    "class BiLSTMDesAtt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMDesAtt, self).__init__() \n",
    "        self.lstm = nn.LSTM(300, 150//2 , num_layers=1, bidirectional=True ) #BiLSTM with attention \n",
    "        #self.lstm = nn.LSTM(300, 150 , num_layers=1, bidirectional=False) #LSTM with attention\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "        self.attn_fc = torch.nn.Linear(300, 1) #attention layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)\n",
    "        return (torch.zeros(2 * 1, 1, 150//2), torch.zeros(2 * 1, 1, 150//2)) # <- change here: first dim of hidden needs to be doubled\n",
    "        #return (torch.zeros(1 * 1, 1, 150), torch.zeros(1 * 1, 1, 150))#LSTM with attention\n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size()) #torch.Size([13, 1, 150])\n",
    "        #rnn_out = rnn_out.squeeze(0).unsqueeze(1) \n",
    "        #rnn_out = rnn_out.permute(2,0,1) \n",
    "        rnn_out = rnn_out.permute(1,0,2) \n",
    "        #print(\"permute rnn_out\", rnn_out.size()) #torch.Size([150, 13, 1])\n",
    "        #print(\"state\", state.size()) #torch.Size([2, 1, 75])\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        #print(\"merged_state\", merged_state.size()) #torch.Size([1, 150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).size()) #torch.Size([150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).size()) #torch.Size([150, 1])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).unsqueeze(2).size()) # torch.Size([150, 1, 1])\n",
    "        #merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"merged_state2 :\", merged_state.size()) #torch.Size([150, 1, 1])\n",
    "        merged_state = merged_state.permute(1,0,2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([13, 300])\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        lstm_out, hidden = self.lstm(X.view(len(X),1, -1))\n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('hidden[0] = h_n', hidden[0], hidden[0].size()) # torch.Size([2, 1, 75])\n",
    "        #print('hidden[1] = c_n', hidden[1], hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        h_n, c_n = hidden\n",
    "        #print('h_n', h_n, h_n.size()) # torch.Size([2, 1, 75])\n",
    "        #print('c_n', c_n, hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        attn_out = self.attention(lstm_out, h_n)\n",
    "        #print(\"attn_out\", attn_out.size()) #torch.Size([150, 1])\n",
    "        #logits = self.fc2(attn_out)\n",
    "        #logits = self.fc2(attn_out.permute(1,0))\n",
    "        #print(\"logits\", logits, logits.size())\n",
    "        #return logits \n",
    "        return attn_out\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create LSTM Model for Location #############\n",
    "class LSTMLoc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMLoc, self).__init__()\n",
    "        self.lstm = nn.LSTM(300, 150, num_layers=1)\n",
    "        self.fc2 = nn.Linear(150, 50) \n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "\n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)# <- change here: first dim of hidden needs to be doubled\n",
    "        return (torch.zeros(1, 1, 150), torch.zeros(1, 1, 150)) \n",
    "    def forward(self, x):\n",
    "        #x=embeds.permute(1,0,2)\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        #lstm_out, self.hidden = self.lstm(x.view(len(x),1, -1), self.hidden)\n",
    "        lstm_out, _ = self.lstm(x.view(len(x),1, -1))\n",
    "        #lstm_out, _ = self.lstm(x.view(len(x),1,-1)) \n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('self.hidden[0]', self.hidden[0], self.hidden[0].size()) # torch.Size([1, 1, 150])\n",
    "        #print(\"lstm_out[-1]\", lstm_out[-1], lstm_out[-1].size())  # torch.Size([1, 150])\n",
    "        #x = self.fc2(lstm_out[-1])  \n",
    "        #out = F.log_softmax(x, dim = 1)\n",
    "        #return out\n",
    "        #return x\n",
    "        return lstm_out[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create Model \n",
    "class NetworkMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkMLP, self).__init__() \n",
    "        self.fc1 = nn.Linear(300, 150)\n",
    "        \n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        \n",
    "        #self.fc1 = nn.Linear(300, 50)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([300])\n",
    "        #print('resize', X.view(1,len(X)).size()) #resize torch.Size([1, 300])\n",
    "        z1 = self.fc1(X.view(1,len(X)))\n",
    "        #print('z1', z1, z1.size()) # torch.Size([1, 150])\n",
    "        h1 = F.relu(z1) \n",
    "        #logits = self.fc2(h1) #without attention\n",
    "        #print(\"logits\", logits, logits.size()) #torch.Size([1, 3])\n",
    "        #return logits \n",
    "        return h1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointDL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JointDL, self).__init__()\n",
    "        self.model_des = BiLSTMDesAtt()\n",
    "        self.model_loc = LSTMLoc()\n",
    "        #self.model_net = NetworkMLP()\n",
    "        self.fc1 = nn.Linear(300, 200) #2*150 = 300 \n",
    "        self.fc2 = nn.Linear(200, 3) #if we use two-layer classifier\n",
    "#        self.fc1 = nn.Linear(300, 3) #2*150 = 300\n",
    "    def forward(self, x_d, x_l): \n",
    "        prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size()) #torch.Size([1, 3])\n",
    "        prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size()) #torch.Size([1, 3])\n",
    "        #prediction_net = self.model_net(x_n)\n",
    "        #print(prediction_net, prediction_net.size()) #torch.Size([1, 3])\n",
    "        #concat_pred = torch.cat((prediction_des, prediction_loc, prediction_net), 1) #concat with dim= 1\n",
    "        concat_pred = torch.cat((prediction_des, prediction_loc), 1) #concat with dim= 1\n",
    "        #print(concat_pred, concat_pred.size()) #torch.Size([1, 6])\n",
    "        out = self.fc1(concat_pred)\n",
    "        out = self.fc2(F.relu(out))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net input #########\n",
    "def nn_input(train_data,df):\n",
    "    #ground_truths = []\n",
    "    training_data_des =[]\n",
    "    training_data_loc=[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                if (not des_emb[train_data[i]]) and (not loc_emb[train_data[i]]):\n",
    "                    print ('no description and location for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    training_data_loc.append(torch.zeros(1, 300))\n",
    "                    break\n",
    "                \n",
    "                elif (des_emb[train_data[i]]) and (not loc_emb[train_data[i]]): \n",
    "                    print ('no location for user: ', train_data[i])\n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    training_data_loc.append(torch.zeros(1, 300))\n",
    "                    break\n",
    "                    \n",
    "                elif (not des_emb[train_data[i]]) and (loc_emb[train_data[i]]): \n",
    "                    print ('no description for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    sent_tensor_loc = torch.stack(loc_emb[train_data[i]],dim = 1)\n",
    "                    training_data_loc.append(sent_tensor_loc[-1])\n",
    "                    break    \n",
    "               \n",
    "                else:\n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    sent_tensor_loc = torch.stack(loc_emb[train_data[i]],dim = 1)\n",
    "                    training_data_loc.append(sent_tensor_loc[-1])\n",
    "                    break\n",
    "    return training_data_des, training_data_loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net #########\n",
    "def nn_input_network(train_data,df):\n",
    "    training_data =[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                #print(train_data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #print (\"net_emb[train_data[i]] : \", net_emb[train_data[i]], type(net_emb[train_data[i]]), torch.Tensor(net_emb[train_data[i]]), type(torch.Tensor(net_emb[train_data[i]])))\n",
    "                #count = 0\n",
    "                if(train_data[i] not in net_emb ):\n",
    "                    net_emb[train_data[i]] = np.zeros(300) #For users not appearing in the mention network, we set their network embedding vectors as 0.\n",
    "                    #count = count + 1\n",
    "                #print(count)\n",
    "                #print(net_emb[train_data[i]]) #ok\n",
    "                ####.....convert ndarray to torch.tensor........\n",
    "                net_emb_tensor = torch.Tensor(net_emb[train_data[i]])\n",
    "                #print(net_emb_tensor) #ok\n",
    "                training_data.append(net_emb_tensor)\n",
    "                break\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Ground Truth #########\n",
    "def find_groundtruth(data, df):\n",
    "    ground_truths = []\n",
    "    for i in range (0, len(data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (data[i] == df.name[j]):\n",
    "                #print(data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #target_type = torch.tensor(utype, dtype=torch.long) #for user type\n",
    "                target_type = torch.tensor(umotivation, dtype=torch.long) #for user motivation\n",
    "                ground_truths.append(target_type)\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_tr(model, training_data_des, training_data_loc, ground_truths):\n",
    "    predictions =[]\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i])\n",
    "        \n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_val(model, training_data_des, training_data_loc, ground_truths):\n",
    "    predictions =[]\n",
    "    val_losses = []\n",
    "    loss_function = nn.NLLLoss()\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i])\n",
    "        val_loss = loss_function(prediction_joint, ground_truths[i])\n",
    "        val_losses.append(val_loss.item())\n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    #print(type(predictions), type(ground_truths))\n",
    "    #print(\"predictions\", predictions)\n",
    "    #print(\"ground_truths\", ground_truths)\n",
    "    \n",
    "    return accuracy, macro_f1, val_losses\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no description for user:  bchi49\n",
      "no location for user:  Christoph_Tran\n",
      "no description for user:  viecestlavie\n",
      "no description for user:  crystalization_\n",
      "no location for user:  yogitimesonline\n",
      "no description for user:  mimmosamami\n",
      "no description for user:  wenmarbyoga\n",
      "no location for user:  cipherEquality\n",
      "no description for user:  YogaLifeLine\n",
      "no location for user:  thewaywecame\n"
     ]
    }
   ],
   "source": [
    "##########......prepare training and validation data\n",
    "# ground truth training\n",
    "train_gt = find_groundtruth(train_data, df)\n",
    "#####prepare training data for neural net #########\n",
    "#training_data_net =  nn_input_network(train_data,df)\n",
    "#print(training_data_net, len(training_data_net)) #ok\n",
    "training_data_des, training_data_loc =  nn_input(train_data,df)\n",
    "\n",
    "# ground truth validation\n",
    "valid_gt = find_groundtruth(valid_data, df)\n",
    "#####prepare validation data for neural net #########\n",
    "#validation_data_net =  nn_input_network(valid_data,df)\n",
    "validation_data_des, validation_data_loc =  nn_input(valid_data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Starting with epoch:  0 ***********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunaz/miniconda2/envs/py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Train accuracy and macro_f1: 0.7 0.47489437187550393\n",
      "epoch : 0 Validation accuracy, macro_f1: 0.7233009708737864 0.4834108217100768\n",
      "train loss per epoch 0.9424551524670727\n",
      "Validation loss per epoch: 0.8125673710431868\n",
      "*************** Starting with epoch:  1 ***********************\n",
      "epoch : 1 Train accuracy and macro_f1: 0.7 0.48038817324702204\n",
      "epoch : 1 Validation accuracy, macro_f1: 0.7087378640776699 0.4776840540365846\n",
      "train loss per epoch 0.9041096128792648\n",
      "Validation loss per epoch: 0.7751789781653765\n",
      "*************** Starting with epoch:  2 ***********************\n",
      "epoch : 2 Train accuracy and macro_f1: 0.7036144578313253 0.4834461734602078\n",
      "epoch : 2 Validation accuracy, macro_f1: 0.7038834951456311 0.47489047803118156\n",
      "train loss per epoch 0.8796668416165923\n",
      "Validation loss per epoch: 0.7517380779518664\n",
      "*************** Starting with epoch:  3 ***********************\n",
      "epoch : 3 Train accuracy and macro_f1: 0.7096385542168675 0.48745833487458334\n",
      "epoch : 3 Validation accuracy, macro_f1: 0.7038834951456311 0.47501529347371135\n",
      "train loss per epoch 0.8603656987558647\n",
      "Validation loss per epoch: 0.7313224663144177\n",
      "*************** Starting with epoch:  4 ***********************\n",
      "epoch : 4 Train accuracy and macro_f1: 0.7108433734939759 0.4882241657649437\n",
      "epoch : 4 Validation accuracy, macro_f1: 0.7135922330097088 0.4815458937198067\n",
      "train loss per epoch 0.843614584085453\n",
      "Validation loss per epoch: 0.7125542934947801\n",
      "*************** Starting with epoch:  5 ***********************\n",
      "epoch : 5 Train accuracy and macro_f1: 0.7156626506024096 0.4915323036450144\n",
      "epoch : 5 Validation accuracy, macro_f1: 0.7087378640776699 0.47828817079650293\n",
      "train loss per epoch 0.8281790288935703\n",
      "Validation loss per epoch: 0.6948694818228194\n",
      "*************** Starting with epoch:  6 ***********************\n",
      "epoch : 6 Train accuracy and macro_f1: 0.7216867469879518 0.4956336567623594\n",
      "epoch : 6 Validation accuracy, macro_f1: 0.7038834951456311 0.4749818709209572\n",
      "train loss per epoch 0.8135013586836715\n",
      "Validation loss per epoch: 0.6790766561204947\n",
      "*************** Starting with epoch:  7 ***********************\n",
      "epoch : 7 Train accuracy and macro_f1: 0.7228915662650602 0.49641941236631254\n",
      "epoch : 7 Validation accuracy, macro_f1: 0.7087378640776699 0.4782216204017152\n",
      "train loss per epoch 0.7993424808157675\n",
      "Validation loss per epoch: 0.6654516373155186\n",
      "*************** Starting with epoch:  8 ***********************\n",
      "epoch : 8 Train accuracy and macro_f1: 0.727710843373494 0.4997594997594998\n",
      "epoch : 8 Validation accuracy, macro_f1: 0.7087378640776699 0.4783099500080632\n",
      "train loss per epoch 0.7856976436383752\n",
      "Validation loss per epoch: 0.6538912276330503\n",
      "*************** Starting with epoch:  9 ***********************\n",
      "epoch : 9 Train accuracy and macro_f1: 0.7337349397590361 0.5039149784671045\n",
      "epoch : 9 Validation accuracy, macro_f1: 0.7087378640776699 0.4783099500080632\n",
      "train loss per epoch 0.7726343661044017\n",
      "Validation loss per epoch: 0.6457405745693781\n",
      "*************** Starting with epoch:  10 ***********************\n",
      "epoch : 10 Train accuracy and macro_f1: 0.736144578313253 0.5056141324454311\n",
      "epoch : 10 Validation accuracy, macro_f1: 0.7038834951456311 0.47498507655324856\n",
      "train loss per epoch 0.7601802338710358\n",
      "Validation loss per epoch: 0.63982870010207\n",
      "*************** Starting with epoch:  11 ***********************\n",
      "epoch : 11 Train accuracy and macro_f1: 0.7409638554216867 0.5089445180914799\n",
      "epoch : 11 Validation accuracy, macro_f1: 0.7087378640776699 0.4783099500080632\n",
      "train loss per epoch 0.7483350305260724\n",
      "Validation loss per epoch: 0.6356129555881602\n",
      "*************** Starting with epoch:  12 ***********************\n",
      "epoch : 12 Train accuracy and macro_f1: 0.7530120481927711 0.5541829990765348\n",
      "epoch : 12 Validation accuracy, macro_f1: 0.7135922330097088 0.48162298094593287\n",
      "train loss per epoch 0.7370738342887722\n",
      "Validation loss per epoch: 0.6334055712790165\n",
      "*************** Starting with epoch:  13 ***********************\n",
      "epoch : 13 Train accuracy and macro_f1: 0.7602409638554217 0.55919409159433\n",
      "epoch : 13 Validation accuracy, macro_f1: 0.7087378640776699 0.47836596567561035\n",
      "train loss per epoch 0.7263763177742389\n",
      "Validation loss per epoch: 0.6322095726878898\n",
      "*************** Starting with epoch:  14 ***********************\n",
      "epoch : 14 Train accuracy and macro_f1: 0.7686746987951807 0.56500042900921\n",
      "epoch : 14 Validation accuracy, macro_f1: 0.7135922330097088 0.48165869218500806\n",
      "train loss per epoch 0.7162095597751409\n",
      "Validation loss per epoch: 0.6322115562783862\n",
      "*************** Starting with epoch:  15 ***********************\n",
      "epoch : 15 Train accuracy and macro_f1: 0.7759036144578313 0.5922404869826625\n",
      "epoch : 15 Validation accuracy, macro_f1: 0.7087378640776699 0.47838551733024093\n",
      "train loss per epoch 0.7065434286901054\n",
      "Validation loss per epoch: 0.6328077559513086\n",
      "*************** Starting with epoch:  16 ***********************\n",
      "epoch : 16 Train accuracy and macro_f1: 0.7831325301204819 0.6178348661416506\n",
      "epoch : 16 Validation accuracy, macro_f1: 0.7135922330097088 0.481658615136876\n",
      "train loss per epoch 0.6973343832356278\n",
      "Validation loss per epoch: 0.6343768783709378\n",
      "*************** Starting with epoch:  17 ***********************\n",
      "epoch : 17 Train accuracy and macro_f1: 0.791566265060241 0.6502441009162315\n",
      "epoch : 17 Validation accuracy, macro_f1: 0.7135922330097088 0.4828155339805826\n",
      "train loss per epoch 0.6885591151983224\n",
      "Validation loss per epoch: 0.6365039674836455\n",
      "*************** Starting with epoch:  18 ***********************\n",
      "epoch : 18 Train accuracy and macro_f1: 0.7939759036144578 0.6504412111612946\n",
      "epoch : 18 Validation accuracy, macro_f1: 0.7184466019417476 0.4860426929392447\n",
      "train loss per epoch 0.6801903353483146\n",
      "Validation loss per epoch: 0.6387013016648374\n",
      "*************** Starting with epoch:  19 ***********************\n",
      "epoch : 19 Train accuracy and macro_f1: 0.7939759036144578 0.6559081335545156\n",
      "epoch : 19 Validation accuracy, macro_f1: 0.7233009708737864 0.4892901054811363\n",
      "train loss per epoch 0.672197381440797\n",
      "Validation loss per epoch: 0.642217753391749\n",
      "*************** Starting with epoch:  20 ***********************\n",
      "epoch : 20 Train accuracy and macro_f1: 0.8012048192771084 0.675041621232126\n",
      "epoch : 20 Validation accuracy, macro_f1: 0.7330097087378641 0.4956293706293707\n",
      "train loss per epoch 0.6645593417793453\n",
      "Validation loss per epoch: 0.6463419361383591\n",
      "*************** Starting with epoch:  21 ***********************\n",
      "epoch : 21 Train accuracy and macro_f1: 0.8048192771084337 0.6904143921882141\n",
      "epoch : 21 Validation accuracy, macro_f1: 0.7378640776699029 0.4988382354131753\n",
      "train loss per epoch 0.6572478218167829\n",
      "Validation loss per epoch: 0.6504271023192452\n",
      "*************** Starting with epoch:  22 ***********************\n",
      "epoch : 22 Train accuracy and macro_f1: 0.8120481927710843 0.7005797369801655\n",
      "epoch : 22 Validation accuracy, macro_f1: 0.7378640776699029 0.4988382354131753\n",
      "train loss per epoch 0.6502448532512756\n",
      "Validation loss per epoch: 0.6555917499749695\n",
      "*************** Starting with epoch:  23 ***********************\n",
      "epoch : 23 Train accuracy and macro_f1: 0.8156626506024096 0.7048498240173342\n",
      "epoch : 23 Validation accuracy, macro_f1: 0.7330097087378641 0.4968599505401536\n",
      "train loss per epoch 0.6435321964707832\n",
      "Validation loss per epoch: 0.661027838552859\n",
      "*************** Starting with epoch:  24 ***********************\n",
      "epoch : 24 Train accuracy and macro_f1: 0.8204819277108434 0.7081861330951869\n",
      "epoch : 24 Validation accuracy, macro_f1: 0.7330097087378641 0.4968599505401536\n",
      "train loss per epoch 0.6370898635259056\n",
      "Validation loss per epoch: 0.666919253422942\n",
      "*************** Starting with epoch:  25 ***********************\n",
      "epoch : 25 Train accuracy and macro_f1: 0.8240963855421687 0.7155751474747444\n",
      "epoch : 25 Validation accuracy, macro_f1: 0.7281553398058253 0.4948625518036245\n",
      "train loss per epoch 0.6308920545577296\n",
      "Validation loss per epoch: 0.6724735723211638\n",
      "*************** Starting with epoch:  26 ***********************\n",
      "epoch : 26 Train accuracy and macro_f1: 0.8253012048192772 0.7146403424375786\n",
      "epoch : 26 Validation accuracy, macro_f1: 0.7281553398058253 0.4948625518036245\n",
      "train loss per epoch 0.6249229150348822\n",
      "Validation loss per epoch: 0.6778709383554828\n",
      "*************** Starting with epoch:  27 ***********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 27 Train accuracy and macro_f1: 0.8325301204819278 0.7196392837338422\n",
      "epoch : 27 Validation accuracy, macro_f1: 0.7233009708737864 0.4914965986394558\n",
      "train loss per epoch 0.6191634467988886\n",
      "Validation loss per epoch: 0.6843601262594745\n",
      "*************** Starting with epoch:  28 ***********************\n",
      "epoch : 28 Train accuracy and macro_f1: 0.8397590361445784 0.7264249392318423\n",
      "epoch : 28 Validation accuracy, macro_f1: 0.7281553398058253 0.4947286631497158\n",
      "train loss per epoch 0.6136019221217982\n",
      "Validation loss per epoch: 0.69054797201336\n",
      "*************** Starting with epoch:  29 ***********************\n",
      "epoch : 29 Train accuracy and macro_f1: 0.8385542168674699 0.7255922336327338\n",
      "epoch : 29 Validation accuracy, macro_f1: 0.7330097087378641 0.4979545082637866\n",
      "train loss per epoch 0.6082233889984999\n",
      "Validation loss per epoch: 0.6974477611111423\n"
     ]
    }
   ],
   "source": [
    "###########.........Start Training...........\n",
    "model = JointDL()\n",
    "##### Hyperparameter\n",
    "#learning_rate=0.05\n",
    "learning_rate=0.01\n",
    "epochs = 30\n",
    "#opt=\"ADAM\"\n",
    "#opt=\"SGD\" \n",
    "opt=\"ADA\"\n",
    "if(opt==\"SGD\"):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "elif(opt==\"ADA\"):\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-06, weight_decay=0.0001)\n",
    "elif(opt==\"ADAM\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "check_val_acc = 0\n",
    "losses = []\n",
    "per_epoch_train_loss =[]\n",
    "per_epoch_val_loss =[]\n",
    "per_epoch_train_f1 =[]\n",
    "per_epoch_val_f1 = []\n",
    "for epoch in range(epochs): \n",
    "    print('*************** Starting with epoch: ', epoch, '***********************')\n",
    "    for i in range (0,len(train_data)):\n",
    "        #model_des.zero_grad()\n",
    "        #model_loc.zero_grad()\n",
    "        model.zero_grad()\n",
    "        #####Run forward pass.\n",
    "      \n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i])\n",
    "        \n",
    "        #print(\"prediction_joint :\", torch.argmax(prediction_joint, dim=1)) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        #Compute the loss, gradients, and update the parameters by\n",
    "        #calling optimizer.step()\n",
    "        loss = loss_function(prediction_joint, train_gt[i])\n",
    "        #if (i%200 == 0):\n",
    "            #print (\"loss per example\", loss.item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward(retain_graph=True)  #backpropagation\n",
    "        optimizer.step()\n",
    "    accuracy, macro_f1 = make_prediction_tr(model, training_data_des, training_data_loc, train_gt)\n",
    "    print('epoch :', epoch, 'Train accuracy and macro_f1:', accuracy, macro_f1)\n",
    "    per_epoch_train_f1.append(macro_f1)\n",
    "    val_accuracy, val_macro_f1, val_loss = make_prediction_val(model, validation_data_des, validation_data_loc, valid_gt)\n",
    "    per_epoch_val_f1.append(val_macro_f1)\n",
    "    print('epoch :', epoch, 'Validation accuracy, macro_f1:', val_accuracy, val_macro_f1)\n",
    "    per_epoch_train_loss.append(np.mean(losses))\n",
    "    print(\"train loss per epoch\", np.mean(losses))\n",
    "    per_epoch_val_loss.append(np.mean(val_loss))\n",
    "    print('Validation loss per epoch:', np.mean(val_loss))\n",
    "    \n",
    "    torch.save(model.state_dict(),\"data/DL_umotivation/joint_DL_\"+str(epoch)+\".pt\")\n",
    "#     if (check_val_acc < val_macro_f1): #early stopping\n",
    "#         check_val_acc = val_macro_f1\n",
    "#         print (\"Model saved at epoch :\", epoch)\n",
    "#         torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "#         best_epoch = epoch\n",
    "        \n",
    "#print(\"Best model found at epoch : \", best_epoch)        \n",
    "#torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_plots(train_losses, val_losses, train_accs, test_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('loss_DL_umotivation_2layer_cls.png')\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, test_accs, '-', linewidth=2, label='validation')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('accuracy_DL_umotivation_2layer_cls.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1)\n",
    "# print(per_epoch_train_loss)\n",
    "# print(per_epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction(training_data_des, training_data_loc, ground_truths):\n",
    "    for epoch in range(0,30):\n",
    "        model = JointDL()\n",
    "        model.load_state_dict(torch.load(\"data/DL_umotivation/joint_DL_\"+str(epoch)+\".pt\")) \n",
    "        predictions =[]\n",
    "        for i in range (0,len(training_data_des)):\n",
    "            prediction_joint = model(training_data_des[i], training_data_loc[i])\n",
    "            pred = torch.argmax(prediction_joint, dim=1)\n",
    "            predictions.append(pred.item())\n",
    "        #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "        accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "        macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "        print('epoch :', epoch, 'Testing accuracy, macro_f1:', accuracy, macro_f1)\n",
    "        #return accuracy, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no location for user:  vaibhavatttc\n"
     ]
    }
   ],
   "source": [
    "#####prepare testing data for neural net #########\n",
    "testing_data_des, testing_data_loc =  nn_input(test_data,df)\n",
    "#testing_data_net =  nn_input_network(test_data,df)\n",
    "test_gt = find_groundtruth(test_data, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "############........Calculate Validation Accuracy.......\n",
    "# test_accuracy, test_macro_f1 = make_prediction(testing_data_des, testing_data_loc, testing_data_net, test_gt)\n",
    "# print('Testing accuracy, macro_f1:', test_accuracy, test_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Testing accuracy, macro_f1: 0.6755725190839694 0.4623456790123457\n",
      "epoch : 1 Testing accuracy, macro_f1: 0.7480916030534351 0.5109493592136369\n",
      "epoch : 2 Testing accuracy, macro_f1: 0.7595419847328244 0.5173348432183619\n",
      "epoch : 3 Testing accuracy, macro_f1: 0.7595419847328244 0.5169885691761924\n",
      "epoch : 4 Testing accuracy, macro_f1: 0.767175572519084 0.521523178807947\n",
      "epoch : 5 Testing accuracy, macro_f1: 0.7824427480916031 0.5319792198943962\n",
      "epoch : 6 Testing accuracy, macro_f1: 0.7862595419847328 0.5345061975209916\n",
      "epoch : 7 Testing accuracy, macro_f1: 0.7900763358778626 0.5370768960954067\n",
      "epoch : 8 Testing accuracy, macro_f1: 0.7824427480916031 0.5316560641571404\n",
      "epoch : 9 Testing accuracy, macro_f1: 0.7862595419847328 0.5345559517744949\n",
      "epoch : 10 Testing accuracy, macro_f1: 0.7824427480916031 0.5317460317460317\n",
      "epoch : 11 Testing accuracy, macro_f1: 0.7748091603053435 0.5267401122789245\n",
      "epoch : 12 Testing accuracy, macro_f1: 0.7748091603053435 0.5276666031383012\n",
      "epoch : 13 Testing accuracy, macro_f1: 0.7748091603053435 0.5276666031383012\n",
      "epoch : 14 Testing accuracy, macro_f1: 0.7786259541984732 0.5308112572263516\n",
      "epoch : 15 Testing accuracy, macro_f1: 0.7824427480916031 0.5734106748257691\n",
      "epoch : 16 Testing accuracy, macro_f1: 0.7786259541984732 0.5714947972958654\n",
      "epoch : 17 Testing accuracy, macro_f1: 0.7786259541984732 0.5708434207943466\n",
      "epoch : 18 Testing accuracy, macro_f1: 0.7748091603053435 0.598315906253088\n",
      "epoch : 19 Testing accuracy, macro_f1: 0.7633587786259542 0.590696438064859\n",
      "epoch : 20 Testing accuracy, macro_f1: 0.767175572519084 0.5935221553980549\n",
      "epoch : 21 Testing accuracy, macro_f1: 0.7748091603053435 0.5986104738808417\n",
      "epoch : 22 Testing accuracy, macro_f1: 0.7709923664122137 0.5960659223817119\n",
      "epoch : 23 Testing accuracy, macro_f1: 0.767175572519084 0.5937895975507895\n",
      "epoch : 24 Testing accuracy, macro_f1: 0.767175572519084 0.5937895975507895\n",
      "epoch : 25 Testing accuracy, macro_f1: 0.7633587786259542 0.5912417832838323\n",
      "epoch : 26 Testing accuracy, macro_f1: 0.7595419847328244 0.5884359947351124\n",
      "epoch : 27 Testing accuracy, macro_f1: 0.7595419847328244 0.5884359947351124\n",
      "epoch : 28 Testing accuracy, macro_f1: 0.7557251908396947 0.5832392647192065\n",
      "epoch : 29 Testing accuracy, macro_f1: 0.7519083969465649 0.580952380952381\n"
     ]
    }
   ],
   "source": [
    "make_prediction(testing_data_des, testing_data_loc, test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
