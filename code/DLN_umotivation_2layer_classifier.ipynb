{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11c79bfd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## load embeddings #######\n",
    "loc_emb = torch.load('data/locationEmbeddings.pt') \n",
    "des_emb = torch.load('data/descriptionEmbeddings.pt') \n",
    "#twt_emb = torch.load('data/tweetsEmbeddings.pt') \n",
    "\n",
    "#load network embedding\n",
    "net_emb = gsm.KeyedVectors.load_word2vec_format('data/userNetworkEmd.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "#user = net_emb ['000mrs000']\n",
    "#print(user)\n",
    "print(type(net_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 1300 user location, description, yoga tweets, utype, umotivation\n",
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "#print (df) #[1308 rows x 7 columns] name, location, description, text, utype, umotivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create BiLSTMAttention Model for Description\n",
    "class BiLSTMDesAtt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMDesAtt, self).__init__() \n",
    "        self.lstm = nn.LSTM(300, 150//2 , num_layers=1, bidirectional=True ) #BiLSTM with attention \n",
    "        #self.lstm = nn.LSTM(300, 150 , num_layers=1, bidirectional=False) #LSTM with attention\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "        self.attn_fc = torch.nn.Linear(300, 1) #attention layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)\n",
    "        return (torch.zeros(2 * 1, 1, 150//2), torch.zeros(2 * 1, 1, 150//2)) # <- change here: first dim of hidden needs to be doubled\n",
    "        #return (torch.zeros(1 * 1, 1, 150), torch.zeros(1 * 1, 1, 150))#LSTM with attention\n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size()) #torch.Size([13, 1, 150])\n",
    "        #rnn_out = rnn_out.squeeze(0).unsqueeze(1) \n",
    "        #rnn_out = rnn_out.permute(2,0,1) \n",
    "        rnn_out = rnn_out.permute(1,0,2) \n",
    "        #print(\"permute rnn_out\", rnn_out.size()) #torch.Size([150, 13, 1])\n",
    "        #print(\"state\", state.size()) #torch.Size([2, 1, 75])\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        #print(\"merged_state\", merged_state.size()) #torch.Size([1, 150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).size()) #torch.Size([150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).size()) #torch.Size([150, 1])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).unsqueeze(2).size()) # torch.Size([150, 1, 1])\n",
    "        #merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"merged_state2 :\", merged_state.size()) #torch.Size([150, 1, 1])\n",
    "        merged_state = merged_state.permute(1,0,2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([13, 300])\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        lstm_out, hidden = self.lstm(X.view(len(X),1, -1))\n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('hidden[0] = h_n', hidden[0], hidden[0].size()) # torch.Size([2, 1, 75])\n",
    "        #print('hidden[1] = c_n', hidden[1], hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        h_n, c_n = hidden\n",
    "        #print('h_n', h_n, h_n.size()) # torch.Size([2, 1, 75])\n",
    "        #print('c_n', c_n, hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        attn_out = self.attention(lstm_out, h_n)\n",
    "        #print(\"attn_out\", attn_out.size()) #torch.Size([150, 1])\n",
    "        #logits = self.fc2(attn_out)\n",
    "        #logits = self.fc2(attn_out.permute(1,0))\n",
    "        #print(\"logits\", logits, logits.size())\n",
    "        #return logits \n",
    "        return attn_out\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create LSTM Model for Location #############\n",
    "class LSTMLoc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMLoc, self).__init__()\n",
    "        self.lstm = nn.LSTM(300, 150, num_layers=1)\n",
    "        self.fc2 = nn.Linear(150, 50) \n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "\n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)# <- change here: first dim of hidden needs to be doubled\n",
    "        return (torch.zeros(1, 1, 150), torch.zeros(1, 1, 150)) \n",
    "    def forward(self, x):\n",
    "        #x=embeds.permute(1,0,2)\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        #lstm_out, self.hidden = self.lstm(x.view(len(x),1, -1), self.hidden)\n",
    "        lstm_out, _ = self.lstm(x.view(len(x),1, -1))\n",
    "        #lstm_out, _ = self.lstm(x.view(len(x),1,-1)) \n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('self.hidden[0]', self.hidden[0], self.hidden[0].size()) # torch.Size([1, 1, 150])\n",
    "        #print(\"lstm_out[-1]\", lstm_out[-1], lstm_out[-1].size())  # torch.Size([1, 150])\n",
    "        #x = self.fc2(lstm_out[-1])  \n",
    "        #out = F.log_softmax(x, dim = 1)\n",
    "        #return out\n",
    "        #return x\n",
    "        return lstm_out[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create Model \n",
    "class NetworkMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkMLP, self).__init__() \n",
    "        self.fc1 = nn.Linear(300, 150)\n",
    "        \n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        \n",
    "        #self.fc1 = nn.Linear(300, 50)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([300])\n",
    "        #print('resize', X.view(1,len(X)).size()) #resize torch.Size([1, 300])\n",
    "        z1 = self.fc1(X.view(1,len(X)))\n",
    "        #print('z1', z1, z1.size()) # torch.Size([1, 150])\n",
    "        h1 = F.relu(z1) \n",
    "        #logits = self.fc2(h1) #without attention\n",
    "        #print(\"logits\", logits, logits.size()) #torch.Size([1, 3])\n",
    "        #return logits \n",
    "        return h1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointDLN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JointDLN, self).__init__()\n",
    "        self.model_des = BiLSTMDesAtt()\n",
    "        self.model_loc = LSTMLoc()\n",
    "        self.model_net = NetworkMLP()\n",
    "        #self.fc = nn.Linear(150, 3)  #3*50 = 150\n",
    "        self.fc1 = nn.Linear(450, 200) #3*150 = 450\n",
    "        self.fc2 = nn.Linear(200, 3) \n",
    "#        self.fc1 = nn.Linear(450, 3) \n",
    "    def forward(self, x_d, x_l, x_n): \n",
    "        prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size()) #torch.Size([1, 3])\n",
    "        prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size()) #torch.Size([1, 3])\n",
    "        prediction_net = self.model_net(x_n)\n",
    "        #print(prediction_net, prediction_net.size()) #torch.Size([1, 3])\n",
    "        concat_pred = torch.cat((prediction_des, prediction_loc, prediction_net), 1) #concat with dim= 1\n",
    "        #print(concat_pred, concat_pred.size()) #torch.Size([1, 6])\n",
    "        out = self.fc1(concat_pred)\n",
    "        out = self.fc2(F.relu(out))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net input #########\n",
    "def nn_input(train_data,df):\n",
    "    #ground_truths = []\n",
    "    training_data_des =[]\n",
    "    training_data_loc=[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                if (not des_emb[train_data[i]]) and (not loc_emb[train_data[i]]):\n",
    "                    print ('no description and location for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    training_data_loc.append(torch.zeros(1, 300))\n",
    "                    break\n",
    "                \n",
    "                elif (des_emb[train_data[i]]) and (not loc_emb[train_data[i]]): \n",
    "                    print ('no location for user: ', train_data[i])\n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    training_data_loc.append(torch.zeros(1, 300))\n",
    "                    break\n",
    "                    \n",
    "                elif (not des_emb[train_data[i]]) and (loc_emb[train_data[i]]): \n",
    "                    print ('no description for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    sent_tensor_loc = torch.stack(loc_emb[train_data[i]],dim = 1)\n",
    "                    training_data_loc.append(sent_tensor_loc[-1])\n",
    "                    break    \n",
    "               \n",
    "                else:\n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    sent_tensor_loc = torch.stack(loc_emb[train_data[i]],dim = 1)\n",
    "                    training_data_loc.append(sent_tensor_loc[-1])\n",
    "                    break\n",
    "    return training_data_des, training_data_loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net #########\n",
    "def nn_input_network(train_data,df):\n",
    "    training_data =[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                #print(train_data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #print (\"net_emb[train_data[i]] : \", net_emb[train_data[i]], type(net_emb[train_data[i]]), torch.Tensor(net_emb[train_data[i]]), type(torch.Tensor(net_emb[train_data[i]])))\n",
    "                #count = 0\n",
    "                if(train_data[i] not in net_emb ):\n",
    "                    net_emb[train_data[i]] = np.zeros(300) #For users not appearing in the mention network, we set their network embedding vectors as 0.\n",
    "                    #count = count + 1\n",
    "                #print(count)\n",
    "                #print(net_emb[train_data[i]]) #ok\n",
    "                ####.....convert ndarray to torch.tensor........\n",
    "                net_emb_tensor = torch.Tensor(net_emb[train_data[i]])\n",
    "                #print(net_emb_tensor) #ok\n",
    "                training_data.append(net_emb_tensor)\n",
    "                break\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Ground Truth #########\n",
    "def find_groundtruth(data, df):\n",
    "    ground_truths = []\n",
    "    for i in range (0, len(data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (data[i] == df.name[j]):\n",
    "                #print(data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #target_type = torch.tensor(utype, dtype=torch.long) #for user type\n",
    "                target_type = torch.tensor(umotivation, dtype=torch.long) #for user motivation\n",
    "                ground_truths.append(target_type)\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_tr(model, training_data_des, training_data_loc, training_data_net, ground_truths):\n",
    "    predictions =[]\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i], training_data_net[i])\n",
    "        \n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_val(model, training_data_des, training_data_loc, training_data_net, ground_truths):\n",
    "    predictions =[]\n",
    "    val_losses = []\n",
    "    loss_function = nn.NLLLoss()\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i], training_data_net[i])\n",
    "        val_loss = loss_function(prediction_joint, ground_truths[i])\n",
    "        val_losses.append(val_loss.item())\n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    #print(type(predictions), type(ground_truths))\n",
    "    #print(\"predictions\", predictions)\n",
    "    #print(\"ground_truths\", ground_truths)\n",
    "    \n",
    "    return accuracy, macro_f1, val_losses\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no description for user:  bchi49\n",
      "no location for user:  Christoph_Tran\n",
      "no description for user:  viecestlavie\n",
      "no description for user:  crystalization_\n",
      "no location for user:  yogitimesonline\n",
      "no description for user:  mimmosamami\n",
      "no description for user:  wenmarbyoga\n",
      "no location for user:  cipherEquality\n",
      "no description for user:  YogaLifeLine\n",
      "no location for user:  thewaywecame\n"
     ]
    }
   ],
   "source": [
    "##########......prepare training and validation data\n",
    "# ground truth training\n",
    "train_gt = find_groundtruth(train_data, df)\n",
    "#####prepare training data for neural net #########\n",
    "training_data_net =  nn_input_network(train_data,df)\n",
    "#print(training_data_net, len(training_data_net)) #ok\n",
    "training_data_des, training_data_loc =  nn_input(train_data,df)\n",
    "\n",
    "# ground truth validation\n",
    "valid_gt = find_groundtruth(valid_data, df)\n",
    "#####prepare validation data for neural net #########\n",
    "validation_data_net =  nn_input_network(valid_data,df)\n",
    "validation_data_des, validation_data_loc =  nn_input(valid_data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Starting with epoch:  0 ***********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunaz/miniconda2/envs/py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Train accuracy and macro_f1: 0.7108433734939759 0.4840941982718257\n",
      "epoch : 0 Validation accuracy, macro_f1: 0.7281553398058253 0.48678355582121274\n",
      "train loss per epoch 0.9369022923779775\n",
      "Validation loss per epoch: 0.7988951347985314\n",
      "*************** Starting with epoch:  1 ***********************\n",
      "epoch : 1 Train accuracy and macro_f1: 0.7 0.48093803667173346\n",
      "epoch : 1 Validation accuracy, macro_f1: 0.7135922330097088 0.48152620140062785\n",
      "train loss per epoch 0.8892158362520747\n",
      "Validation loss per epoch: 0.7369133008336558\n",
      "*************** Starting with epoch:  2 ***********************\n",
      "epoch : 2 Train accuracy and macro_f1: 0.7156626506024096 0.4913485808876072\n",
      "epoch : 2 Validation accuracy, macro_f1: 0.7135922330097088 0.48165869218500806\n",
      "train loss per epoch 0.8523027764266753\n",
      "Validation loss per epoch: 0.6767270883310188\n",
      "*************** Starting with epoch:  3 ***********************\n",
      "epoch : 3 Train accuracy and macro_f1: 0.7301204819277108 0.5015028716408891\n",
      "epoch : 3 Validation accuracy, macro_f1: 0.7281553398058253 0.49149594124468504\n",
      "train loss per epoch 0.818854556636638\n",
      "Validation loss per epoch: 0.621205291701752\n",
      "*************** Starting with epoch:  4 ***********************\n",
      "epoch : 4 Train accuracy and macro_f1: 0.7433734939759036 0.510656617809493\n",
      "epoch : 4 Validation accuracy, macro_f1: 0.7330097087378641 0.4947722842459685\n",
      "train loss per epoch 0.7887940396601896\n",
      "Validation loss per epoch: 0.5785454812559109\n",
      "*************** Starting with epoch:  5 ***********************\n",
      "epoch : 5 Train accuracy and macro_f1: 0.763855421686747 0.5246865409231812\n",
      "epoch : 5 Validation accuracy, macro_f1: 0.7621359223300971 0.5143114717151228\n",
      "train loss per epoch 0.7622250560297066\n",
      "Validation loss per epoch: 0.5473879781162855\n",
      "*************** Starting with epoch:  6 ***********************\n",
      "epoch : 6 Train accuracy and macro_f1: 0.7819277108433735 0.5369026499440589\n",
      "epoch : 6 Validation accuracy, macro_f1: 0.7621359223300971 0.5139150753363951\n",
      "train loss per epoch 0.7387302894703082\n",
      "Validation loss per epoch: 0.524954252451369\n",
      "*************** Starting with epoch:  7 ***********************\n",
      "epoch : 7 Train accuracy and macro_f1: 0.791566265060241 0.5433900050244154\n",
      "epoch : 7 Validation accuracy, macro_f1: 0.7961165048543689 0.5363856589147286\n",
      "train loss per epoch 0.7178742488195379\n",
      "Validation loss per epoch: 0.5094041951651712\n",
      "*************** Starting with epoch:  8 ***********************\n",
      "epoch : 8 Train accuracy and macro_f1: 0.7975903614457831 0.5474211661460657\n",
      "epoch : 8 Validation accuracy, macro_f1: 0.8155339805825242 0.5490624696395608\n",
      "train loss per epoch 0.6992558741904645\n",
      "Validation loss per epoch: 0.49892930093320825\n",
      "*************** Starting with epoch:  9 ***********************\n",
      "epoch : 9 Train accuracy and macro_f1: 0.8060240963855422 0.5660480567910939\n",
      "epoch : 9 Validation accuracy, macro_f1: 0.8203883495145631 0.5522281639928699\n",
      "train loss per epoch 0.6825212056880974\n",
      "Validation loss per epoch: 0.4920813915798965\n",
      "*************** Starting with epoch:  10 ***********************\n",
      "epoch : 10 Train accuracy and macro_f1: 0.8120481927710843 0.5701983209434541\n",
      "epoch : 10 Validation accuracy, macro_f1: 0.8203883495145631 0.5522281639928699\n",
      "train loss per epoch 0.6673482821256673\n",
      "Validation loss per epoch: 0.4882728694712074\n",
      "*************** Starting with epoch:  11 ***********************\n",
      "epoch : 11 Train accuracy and macro_f1: 0.8180722891566266 0.586553201138516\n",
      "epoch : 11 Validation accuracy, macro_f1: 0.8300970873786407 0.5585585585585585\n",
      "train loss per epoch 0.6534877099665293\n",
      "Validation loss per epoch: 0.4858160406640432\n",
      "*************** Starting with epoch:  12 ***********************\n",
      "epoch : 12 Train accuracy and macro_f1: 0.8289156626506025 0.5940148886590426\n",
      "epoch : 12 Validation accuracy, macro_f1: 0.8349514563106796 0.5617241827516736\n",
      "train loss per epoch 0.6407284629632192\n",
      "Validation loss per epoch: 0.48483376682383345\n",
      "*************** Starting with epoch:  13 ***********************\n",
      "epoch : 13 Train accuracy and macro_f1: 0.8337349397590361 0.5973207562101716\n",
      "epoch : 13 Validation accuracy, macro_f1: 0.8398058252427184 0.5651651651651651\n",
      "train loss per epoch 0.6289005625011999\n",
      "Validation loss per epoch: 0.4855786900497177\n",
      "*************** Starting with epoch:  14 ***********************\n",
      "epoch : 14 Train accuracy and macro_f1: 0.8373493975903614 0.6115601311114358\n",
      "epoch : 14 Validation accuracy, macro_f1: 0.8398058252427184 0.5651651651651651\n",
      "train loss per epoch 0.6178660773656455\n",
      "Validation loss per epoch: 0.487311344412924\n",
      "*************** Starting with epoch:  15 ***********************\n",
      "epoch : 15 Train accuracy and macro_f1: 0.8385542168674699 0.6346665942824848\n",
      "epoch : 15 Validation accuracy, macro_f1: 0.8300970873786407 0.5617526617526618\n",
      "train loss per epoch 0.6075216618557292\n",
      "Validation loss per epoch: 0.4899732783002761\n",
      "*************** Starting with epoch:  16 ***********************\n",
      "epoch : 16 Train accuracy and macro_f1: 0.8409638554216867 0.6547795758793832\n",
      "epoch : 16 Validation accuracy, macro_f1: 0.8300970873786407 0.5617526617526618\n",
      "train loss per epoch 0.5977911957479554\n",
      "Validation loss per epoch: 0.492918470530834\n",
      "*************** Starting with epoch:  17 ***********************\n",
      "epoch : 17 Train accuracy and macro_f1: 0.8481927710843373 0.6770611992846649\n",
      "epoch : 17 Validation accuracy, macro_f1: 0.8300970873786407 0.5617526617526618\n",
      "train loss per epoch 0.5885865979006332\n",
      "Validation loss per epoch: 0.496374240488682\n",
      "*************** Starting with epoch:  18 ***********************\n",
      "epoch : 18 Train accuracy and macro_f1: 0.8506024096385543 0.691653953724892\n",
      "epoch : 18 Validation accuracy, macro_f1: 0.8300970873786407 0.5617526617526618\n",
      "train loss per epoch 0.5798475069264937\n",
      "Validation loss per epoch: 0.5002961474136242\n",
      "*************** Starting with epoch:  19 ***********************\n",
      "epoch : 19 Train accuracy and macro_f1: 0.8554216867469879 0.6951101797167715\n",
      "epoch : 19 Validation accuracy, macro_f1: 0.8300970873786407 0.5617526617526618\n",
      "train loss per epoch 0.5715269764467894\n",
      "Validation loss per epoch: 0.5040616178975522\n",
      "*************** Starting with epoch:  20 ***********************\n",
      "epoch : 20 Train accuracy and macro_f1: 0.8590361445783132 0.6976459869480465\n",
      "epoch : 20 Validation accuracy, macro_f1: 0.8349514563106796 0.5649550790215752\n",
      "train loss per epoch 0.5635800680140278\n",
      "Validation loss per epoch: 0.5085139636275837\n",
      "*************** Starting with epoch:  21 ***********************\n",
      "epoch : 21 Train accuracy and macro_f1: 0.863855421686747 0.714978065055288\n",
      "epoch : 21 Validation accuracy, macro_f1: 0.8349514563106796 0.5649550790215752\n",
      "train loss per epoch 0.5559700188801296\n",
      "Validation loss per epoch: 0.5129839821347912\n",
      "*************** Starting with epoch:  22 ***********************\n",
      "epoch : 22 Train accuracy and macro_f1: 0.8662650602409638 0.7238778095699115\n",
      "epoch : 22 Validation accuracy, macro_f1: 0.8300970873786407 0.5632411067193676\n",
      "train loss per epoch 0.5486635045520668\n",
      "Validation loss per epoch: 0.5178404913365262\n",
      "*************** Starting with epoch:  23 ***********************\n",
      "epoch : 23 Train accuracy and macro_f1: 0.8686746987951808 0.7325970454946781\n",
      "epoch : 23 Validation accuracy, macro_f1: 0.8300970873786407 0.5632411067193676\n",
      "train loss per epoch 0.5416356316771374\n",
      "Validation loss per epoch: 0.5228099200910735\n",
      "*************** Starting with epoch:  24 ***********************\n",
      "epoch : 24 Train accuracy and macro_f1: 0.8710843373493976 0.7343252306748473\n",
      "epoch : 24 Validation accuracy, macro_f1: 0.8300970873786407 0.5632411067193676\n",
      "train loss per epoch 0.5348603437406471\n",
      "Validation loss per epoch: 0.527410536425785\n",
      "*************** Starting with epoch:  25 ***********************\n",
      "epoch : 25 Train accuracy and macro_f1: 0.8734939759036144 0.742899812254651\n",
      "epoch : 25 Validation accuracy, macro_f1: 0.8300970873786407 0.5632411067193676\n",
      "train loss per epoch 0.5283183050884817\n",
      "Validation loss per epoch: 0.5323451864487917\n",
      "*************** Starting with epoch:  26 ***********************\n",
      "epoch : 26 Train accuracy and macro_f1: 0.8759036144578313 0.7529271206690561\n",
      "epoch : 26 Validation accuracy, macro_f1: 0.8300970873786407 0.5632411067193676\n",
      "train loss per epoch 0.5219950088695031\n",
      "Validation loss per epoch: 0.5372631642424944\n",
      "*************** Starting with epoch:  27 ***********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 27 Train accuracy and macro_f1: 0.8783132530120482 0.7611148571351931\n",
      "epoch : 27 Validation accuracy, macro_f1: 0.8300970873786407 0.5632411067193676\n",
      "train loss per epoch 0.5158771853695228\n",
      "Validation loss per epoch: 0.541889991430403\n",
      "*************** Starting with epoch:  28 ***********************\n",
      "epoch : 28 Train accuracy and macro_f1: 0.8831325301204819 0.7645037864193217\n",
      "epoch : 28 Validation accuracy, macro_f1: 0.8300970873786407 0.5632411067193676\n",
      "train loss per epoch 0.5099441914004709\n",
      "Validation loss per epoch: 0.5467710738043183\n",
      "*************** Starting with epoch:  29 ***********************\n",
      "epoch : 29 Train accuracy and macro_f1: 0.8831325301204819 0.768939615760004\n",
      "epoch : 29 Validation accuracy, macro_f1: 0.8300970873786407 0.5634431998945016\n",
      "train loss per epoch 0.5041834083067844\n",
      "Validation loss per epoch: 0.5519451546726875\n"
     ]
    }
   ],
   "source": [
    "###########.........Start Training...........\n",
    "model = JointDLN()\n",
    "##### Hyperparameter\n",
    "#learning_rate=0.005\n",
    "learning_rate=0.01\n",
    "epochs = 30\n",
    "#opt=\"ADAM\"\n",
    "#opt=\"SGD\" \n",
    "opt=\"ADA\"\n",
    "if(opt==\"SGD\"):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "elif(opt==\"ADA\"):\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-06, weight_decay=0.0001)\n",
    "elif(opt==\"ADAM\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "check_val_acc = 0\n",
    "losses = []\n",
    "per_epoch_train_loss =[]\n",
    "per_epoch_val_loss =[]\n",
    "per_epoch_train_f1 =[]\n",
    "per_epoch_val_f1 = []\n",
    "for epoch in range(epochs): \n",
    "    print('*************** Starting with epoch: ', epoch, '***********************')\n",
    "    for i in range (0,len(train_data)):\n",
    "        #model_des.zero_grad()\n",
    "        #model_loc.zero_grad()\n",
    "        model.zero_grad()\n",
    "        #####Run forward pass.\n",
    "      \n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i], training_data_net[i])\n",
    "        \n",
    "        #print(\"prediction_joint :\", torch.argmax(prediction_joint, dim=1)) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        #Compute the loss, gradients, and update the parameters by\n",
    "        #calling optimizer.step()\n",
    "        loss = loss_function(prediction_joint, train_gt[i])\n",
    "        #if (i%200 == 0):\n",
    "            #print (\"loss per example\", loss.item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward(retain_graph=True)  #backpropagation\n",
    "        optimizer.step()\n",
    "    accuracy, macro_f1 = make_prediction_tr(model, training_data_des, training_data_loc, training_data_net, train_gt)\n",
    "    print('epoch :', epoch, 'Train accuracy and macro_f1:', accuracy, macro_f1)\n",
    "    per_epoch_train_f1.append(macro_f1)\n",
    "    val_accuracy, val_macro_f1, val_loss = make_prediction_val(model, validation_data_des, validation_data_loc, validation_data_net, valid_gt)\n",
    "    per_epoch_val_f1.append(val_macro_f1)\n",
    "    print('epoch :', epoch, 'Validation accuracy, macro_f1:', val_accuracy, val_macro_f1)\n",
    "    per_epoch_train_loss.append(np.mean(losses))\n",
    "    print(\"train loss per epoch\", np.mean(losses))\n",
    "    per_epoch_val_loss.append(np.mean(val_loss))\n",
    "    print('Validation loss per epoch:', np.mean(val_loss))\n",
    "    \n",
    "    torch.save(model.state_dict(),\"data/DLN_umotivation_2layer/joint_DLN_\"+str(epoch)+\".pt\")\n",
    "#     if (check_val_acc < val_macro_f1): #early stopping\n",
    "#         check_val_acc = val_macro_f1\n",
    "#         print (\"Model saved at epoch :\", epoch)\n",
    "#         torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "#         best_epoch = epoch\n",
    "        \n",
    "#print(\"Best model found at epoch : \", best_epoch)        \n",
    "#torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU1fn48c+TnYQAgRCWhFV2BFkCaFEElxapiigqLlVwoVotarXVtt9WbW1/WK1V60LR4tKKlqoIVUFc2JQtCSKGfQskBEJIgCSQkGWe3x93CAGSYQKZTGbyvF+veeVu5+a5mSTPnHPPPUdUFWOMMaYmIf4OwBhjTMNmicIYY4xHliiMMcZ4ZInCGGOMR5YojDHGeBTm7wDqUnx8vHbu3NnfYRhjTMBIS0vbr6qtPR0TVImic+fOpKam+jsMY4wJGCKy83THWNOTMcYYjyxRGGOM8cgShTHGGI8sURhjjPHIEoUxxhiPLFEYY4zxyBKFMcYYjyxRGGNMACo6Ws789L38fk465RUun36voHrgzhhjgpWqsi23iIUbc1m4aR8pGfmUVTjzCY0d0J7BnVr67HtbojDGmAbK5VJCQgSA/UWlXPbcksp9IQJDOscxsmcC7Vs08WkcliiMMaaBKDpaztrMg6zedYCUjAPsPljM5w+NQERoHRvJiB6tiY+JYGSvBEZ0j6dFdES9xGWJwhhj/GjT3kLeXJbBt7sOsDmnENdJs1Nn5hfTsVU0AG/fMdQPEVqiMMaYerM5p5DP0vfSOT6Gq85rD8Ch4jLeXbULgLAQoV9iMwZ2jGNgxxZccE4rEmKj/BmyE5e/AzDGmGCXmX+Ev32+mdlrdqMKI3u2rkwU/RKb8+srejGoUxz9EpsTFR7q52hPZYnCGGN8JP9wKS99tZV/r9hJaYWL8FDh2oFJXNo7ofKYJhGh/PTic/wY5elZojDGGB9YtSOfO95MoehoOSJwzYD2/OLynpX3GwKJJQpjjPGBcxObERUeSnLnOH71o170ad/M3yGdMUsUxhhzllwu5X9rs3lrWQZv3zmMppFhREeEMf/Bi4hvGunv8M6aTxOFiIwGXgBCgddVdepJ+38J3FIllt5Aa1XNF5EMoBCoAMpVNdmXsRpjzJlYk3mQ387+nnXZBQD8JyWTOy/sAhAUSQJ8mChEJBR4GbgcyAJSRGSuqq4/doyqPgM84z7+KuAhVc2vcppRqrrfVzEaY8yZKimr4G+fb+a1pdtxKbRtFsVDl3fnukFJ/g6tzvmyRjEU2Kqq2wFE5D1gLLC+huNvAt71YTzGGFMn0nbm88h/17Jj/2FCBH46oisPXtaDJhENr2trXfBlokgEMqusZwHDqjtQRKKB0cD9VTYrsEBEFPiHqk6voexkYDJAx44d6yBsY4zxLP9wGTv2H6ZHm6b8Zfx5DOjQwt8h+ZQvE4VUs02r2QZwFfDNSc1Ow1U1W0QSgM9FZKOqLjm5oDuBTAdITk6u6fzGGHNWMvOP0KGl07X18j5teOnmgVzepw2RYcFZi6jKl/NRZAEdqqwnAdk1HDuBk5qdVDXb/XUfMBunKcsYY+pVQUkZj32wllHPLuL7rEOV26/s375RJAnwbaJIAbqLSBcRicBJBnNPPkhEmgMXA3OqbIsRkdhjy8APgXQfxmqMMaf4ckMOP3xuCe+lZBIiwsa9Bf4OyS981vSkquUicj/wGU732Bmquk5E7nHvn+Y+dBywQFUPVyneBpgtIsdinKmq830VqzHGVJWZf4Sp8zfyydo9AAzs2IJnxvenW0KsnyPzD1ENnmb95ORkTU1N9XcYxpgANj99D1PeW0NpuYuo8BAe+WFPJg3vQmhIdbddA5+IpJ3uOTV7MtsYY6oY1DGOiNAQxpzbll+O7kWij2ePCwSWKIwxjZaqsnDTPv6TksnLNw8iLDSEhGZRLPrlyKB5qrouWKIwxjRKG/YU8KdPNvD1Vmfwh9nf7ub6ZKejpiWJE1miMMY0KvsKSvjrgs3MSstEFZpFhTHl0u5cPaC9v0NrsCxRGGOChqqSf7iUPYdKyD5YzJ5DJVzaO4GkOOdBub99vpmXFm6lwqWEhQg/+UEnplzSnbiYCD9H3rBZojDGBLRDR8p4/svNLNy4jz2HSjha7jphf+vYyMpEERMZikuVy/u04ddX9KJr66b+CDngWKIwxgS0oxUVzErJ5HBpBeA0JbVv0YR2zaNo5/56zK3nd+K2Czo3yHmpGzJLFMaYgKKqfL11Pxd0beX0UoqN4unx/Wnfogk928QSE1nzv7XoCPuXdyZ8OYSHMcbUqU17C7ltxip+8s9VzFy1q3L7lf3bM6hjnMckYc6c/VSNMQ1eXtFR/vbFZmau3IXL3VMpPNQ+59YXSxTGmAartNzF28szeOHLLRSWlBMaItx2fkcevKwHLa2nUr2xRGGMabA++T6bpz7ZAMBF3eP53ZV96NGmcQ7M50+WKIwxDUphSRmxUeEAXH1eIl+s38f4wUmM7Nka94jSpp5ZI58xpkEoKCnjqY/X84OpX7HnUDEAoSHCy7cMYlSvBEsSfmQ1CmOMX7lcyn/TMnnms03sLypFBJZszuXGIR39HZpxs0RhjPGbtJ35PDF3Pd/vdqYYTe4Ux+NX9aVfUnM/R2aqskRhjPGLaYu3MXXeRgDaNovi12N6cfV57a2JqQGyexTGGL8Y0b01TcJD+fkl3fjqkYsZOyDRkkQDZTUKY0y92X2wuHLGuD7tm7Hi15fSPDrcz1GZ07EahTGmXqzYnseoZxfx8sKtqCqAJYkAYYnCGONzm3MKmfx2KqXlLnILj/o7HFNLliiMMT6VU1DCxBmrKCgp50d92/C7K/vYvYgAY4nCGOMzhSVlTHwjhexDJQzq2IIXJgwkNMSSRKCxRGGM8YmyChc/e2c1G/YU0DU+htdvH2ITBgUoSxTGGJ/IKShh274i4ptG8OakoTbaawDzaaIQkdEisklEtorIY9Xs/6WIrHG/0kWkQkRaelPWGNOwJcVF8+HPhvPWHUPp2Cra3+GYs+CzRCEiocDLwBVAH+AmEelT9RhVfUZVB6jqAODXwGJVzfemrDGmYdqWW1S53LZ5FH3b23Acgc6XNYqhwFZV3a6qpcB7wFgPx98EvHuGZY0xDcBXG3P44d+W8NcFmyqflTCBz5eJIhHIrLKe5d52ChGJBkYDH9S2rDGmYVibdZD73vmWCpeTIKwLbPDw5RAe1f2W1PQR4yrgG1XNr21ZEZkMTAbo2NGGJTamPn21MYcNewrZklPIos25FJdVcO2gRH5xeQ9/h2bqkC8TRRbQocp6EpBdw7ETON7sVKuyqjodmA6QnJxsdV1j6lCFS9mZd5gt+4rYklPIrvwjPH1d/8rawtR5G9mcc/yexIgerZl6bX+rTQQZXyaKFKC7iHQBduMkg5tPPkhEmgMXA7fWtqwxpu7tzDvMh6t3s3rXAb7ddZCio+Un7H/kRz1JiI0CYOyARPIPl9I9oSnd28QysEMLQuyBuqDjs0ShquUicj/wGRAKzFDVdSJyj3v/NPeh44AFqnr4dGV9FasxjZGqkpF3hLSdB4hvGsHIngkA7DlUwgtfbqk8rl3zKHq0iaV7QlN6tIk94aG5+0Z1q/e4Tf2TYOqZkJycrKmpqf4Ow5gGbV9hCX/433qWbcsj/3ApAJf2SuCfE4cAUFxawbMLNjG4UxyDO8XRplmUP8M1PiYiaaqa7OkYm4/CmEYk+2Axt7y+kh37nQp8fNMIBneK46LurSuPaRIRyu+utMeWzHGWKIxpJHYfLOaGacvZfbCYPu2a8dLNA+kSH2M3ns1pWaIwppFoGR1BYlwTEppF8uakoTRvYpMGGe9YojCmkWgSEcoM932IppH2p2+8Z6PHGhPE0nYe4OFZ31FW4QKcBGFJwtSW/cYYE6SWbdvPXW+lcqS0gn6JzZg4vIu/QzIByhKFMUFo4cZ93PPvNI6Wuxg3MJFbz+/k75BMALNEYUyQmff9Hqa89y1lFcrNwzry1Nhz7Wlpc1YsURgTRD5cncUj//0Ol8JdF3bhtz/ubd1fzVmzRGFMkFBVZn+7G5fCA5d258HLuluSMHXCEoUxQUJE+MdPBrNgXQ7XDLTpW0zdse6xxgS4CpdWziYXHRFmScLUOUsUxgS4F77YzN1vp5FbeNTfoZggZU1PxgSwNZkHeXnRNlyqbM/tQuvYSH+HZIKQ1SiMCVDFpRX8YtYaKlzKXRd2YVjXVv4OyQSpM0oUIjK9rgMxxtTO0/M3sj33MN0TmvLwD3v6OxwTxGpsehKRljXtAsb4JhxjjDeWbd3Pm8syCAsRnrthwAmzzhlT1zzdo8gFduIkhmPUvZ7gy6CMMTUrKCnjkf9+B8CUS7vTL6m5nyMywc5TotgOXKqqu07eISKZvgvJGONJRGgIV53XnpU78vnZyHP8HY5pBDwliueBOOCURAH8xTfhGGNOJyo8lF+P6U1puYuwUOuPYnyvxkShqi972Pd334RjjKlJ/uFSQkVoHu3MTBcRZknC1I8af9NE5M9Vli+vn3CMMdVRVR79YC0/en4J3+464O9wTCPj6SPJ6CrLT/s6EGNMzT5YvZvP1+dw+Gg5Cc2i/B2OaWSs7mpMA5d14AhPzl0HwBNX9yWxRRM/R2QaG083sxNE5Be4u8O6lyup6nM+jcwYg8ul/PK/ayk8Ws6P+rbh2kE24J+pf54SxWtAbDXLxph68o8l21m+PY9WMRH8eVw/m1/C+IWnXk9Pnu3JRWQ08AIQCryuqlOrOWYkTlfccGC/ql7s3p4BFAIVQLmqJp9tPMYEkt0Hi/nrgk0APHN9f1o1tQH/jH/4bPRYEQkFXgYuB7KAFBGZq6rrqxzTAngFGK2qu0Tk5Ce+R6nqfl/FaExDltiiCa/dnsz67AIu6dXG3+GYRsyXw4wPBbaq6nYAEXkPGAusr3LMzcCHx57+VtV9PozHmIAzqmcCo3raiDnGv3zZ6ykRqDrUR5Z7W1U9gDgRWSQiaSJyW5V9Cixwb59c0zcRkckikioiqbm5uXUWvDH+8vrS7azcnufvMIypdNoaxcm9ndwOAWmqusZT0Wq2aTXffzBwKdAEWC4iK1R1MzBcVbPdzVGfi8hGVV1yyglVpwPTAZKTk08+vzEBZfHmXP706QbCQ0JY/KuRtGtuXWGN/3lTo0gG7sGpDSQCk4GRwGsi8isP5bKADlXWk4Dsao6Zr6qH3fcilgDnAahqtvvrPmA2TlOWMUEr+2AxD773Larw80u6WZIwDYY3iaIVMEhVH1bVh3ESR2tgBDDRQ7kUoLuIdBGRCGACMPekY+YAF4lImIhEA8OADSISIyKxACISA/wQSK/FdRkTUMoqXPz83W85cKSMET1ac9+obv4OyZhK3tzM7giUVlkvAzqparGI1Dibu6qWi8j9wGc43WNnqOo6EbnHvX+aqm4QkfnAWsCF04U2XUS6ArPdfcbDgJmqOv9MLtCYQPD0vI2k7TxA22ZRPH/jAEJC7HkJ03B4kyhmAitEZI57/SrgXfcn/fU1FwNV/RT49KRt005afwZ45qRt23E3QRkT7Oan7+X1r3cQFiK8fMtAWsZE+DskY05w2kShqn8UkU+BC3FuUN+jqqnu3bf4MjhjGoPoiFDiosO5b1Q3BneqaQZiY/zHm15PLwD/UdUX6iEeYxqdET1as+Chi4lvajUJ0zB5czN7NfB/IrJVRJ4RERtKw5g6sGP/4crl1rGRNo6TabBOmyhU9S1VHYPTPXUz8LSIbPF5ZMYEsTlrdnPpXxfx2pLt/g7FmNOqzZPZ3YBeQGdgo0+iMaYR2HOomP/7KB2XQpOIUH+HY8xpnTZRiMixGsQfgHXAYFW9yueRGROEVJXffPg9hSXlXNY7gVuGdfR3SMacljfdY3cAF9gorsacvQ9X72bhplyaRYXxJ5tfwgQIb7rHThOROBEZCkRV2X7KuEvGmJrtKyjhyf85U5r+/qq+tLG5r02A8KZ77F3AAzhjNa0BzgeWA5f4NjRjgstTn2ygoKScUT1bc51NaWoCiDdNTw8AQ4AVqjpKRHoBZz37nTGNzaNX9KLc5eJ3V/axJicTULxJFCWqWiIiiEikqm4UkZ4+j8yYIJPYogmv3DLY32EYU2vedI/Nck9Z+hHOvBBzOHW4cGNMNVSVL9bn4HLZVCkmcHlzM3uce/EJEVkINAdsJFdjvPDJ93u4f+a3jOzZmjcmDrEmJxOQajsVak9Vnauqpac/1JjGLa/oKL+f4/Ry+mGftpYkTMCqbaK4xydRGBOEfj93HfmHSxnerRU3De1w+gLGNFC1TRT2kcgYL8z7fg+frN1DdEQoU6/tb7UJE9Bqmyiu9EkUxgSR/MOl/G6OM3Pvr6/oRYeW0X6OyJiz481YT81F5G8ikgrMEZG/ikjzeojNmID05rIM9heVcn7XltwyrJO/wzHmrHnzHMUMIB24wb3+E+AN4FpfBWVMIJtySTeaRobyo75tbe5rExS8SRTnqOp1VdafFJE1vgrImEAXFhrC5BHn+DsMY+qMN/coikXkwmMrIjIcKPZdSMYEnl15R7jl9RXsPVTi71CMqXPeJIp7gJdFJENEMoCXgJ/6NCpjAsimvYWMn7aMb7bm8fR8m9PLBB+PTU8iEoLzkN15ItIMQFUL6iUyYwLA6l0HmPRGCoeKy7igayv+eM25/g7JmDrnsUahqi7gfvdygSUJY477est+bn19JYeKy7i8TxvemDSEppHe3PYzJrB40/T0uYg8IiIdRKTlsZfPIzOmAZufvoc73kzhSGkF1w5K5NVbBhEVbvNfm+DkTaK4A7gPWAKkuV+p3pxcREaLyCYR2Soij9VwzEgRWSMi60RkcW3KGuMvW3KKKK1wMfEHnXl2/HmEhdb22VVjAoc3o8d2OZMTi0go8DJwOZAFpIjIXFVdX+WYFsArwGhV3SUiCd6WNcaf7r+kG/07tGBE93gbnsMEPW+ezL7P/Q/92HqciPzMi3MPBbaq6nb3aLPvAWNPOuZm4ENV3QWgqvtqUdaYeqOqTFu8jawDRwAQES7u0dqShGkUvKkv362qB4+tqOoB4G4vyiUCmVXWs9zbquoBxInIIhFJE5HbalEWABGZLCKpIpKam5vrRVjG1I7Lpfx+zjqmztvIxDdSKKtw+TskY+qVN100QkREVFWhslkowoty1X3UOnmarzBgMHAp0ARYLiIrvCzrbFSdDkwHSE5OtmnEzFmrcCmb9hayJvMgazIPkJpxgO37DxMRFsKvftSTcLsfYRoZbxLFZ8AsEZmG88/6Hryb4S4LqDoIfxKnTqGaBexX1cPAYRFZApznZVlj6kROQQl5RaX0ad8MgF35Rxjz4tITjmkRHc4rtwziB+fE+yNEY/zKm0TxKM6T2PfifNJfALzuRbkUoLuIdAF2AxNw7klUNQd4SUTCcGopw4C/ARu9KGvMGVuTeZA3vtnBqh357DlUQr/E5vzv585INZ1bRdMvsTld4mMY0KEFAzu2oE/7ZkSGWfdX0zh50+vJBbzqfnlNVctF5H6cGkkoMENV14nIPe7901R1g4jMB9YCLuB1VU0HqK5sbb6/MdVJzcjnxa+2smTz8ftZsZFhtGoagaoiIohIZdIwxoC4bz3UfIBId+D/AX2AqGPbVbWrb0OrveTkZE1N9eoRD9MILVi3l8n/SgMgJiKUWy/oxPhBSZzTuqkNB24aLRFJU9VkT8d40/T0BvA4TpPQKGASNiWqCQCqys68I3SOjwHg4p6t6dkmlh/1bcOk4V2Ii/GmT4YxxptE0URVv3T3fNoJPCEiS3GShzENjqryxYZ9/P2rLWzbV8TXj15CXEwEkWGhzHvgIqs9GFNL3iSKEvcoslvc9w12Awm+DcuYM7Nyex5P/G89G/Y441fGN41ga24RQ2Kc4cksSRhTe94kigeBaGAK8EfgEuB2XwZlzJlYtSOf22as4mi5i4TYSH568TncPLQjTSKst5IxZ8ObXk8p7sUinPsTxjQ46bsPceebKRwtd3FjcgeeHNvXRnM1po7UmChEZK6ngqp6dd2HY8yZOVpeAQI/7t+OP1/bj1BrYjKmzniqUVyAM97Su8BKrKeTacAGd2rJR/cNp0NctCUJY+qYp0Fr2gK/Ac4FXsAZ8nu/qi5W1cUeyhlTL3ILj7Jw477K9XNaNyUizMZhMqau1fhXpaoVqjpfVW8Hzge2AotE5Of1Fp0xNTh0pIzbZqzizrdS+GzdXn+HY0xQ83gzW0QigR8DNwGdgReBD30fljE1O1JazqQ3V7FhTwFd42MY3CnO3yEZE9Q83cx+C6fZaR7w5LExmIzxp6PlFfz0X2ms3nWQ9s2j+Nddw4hvGunvsIwJap5qFD8BDuNMLjSlykxeAqiqNvNxbMacoLzCxYPvrWHplv20iong33cNI7FFE3+HZUzQqzFRqKrdFTQNyh8/Xs+89L3ERoXx1h1D6dq6qb9DMqZRsGRgAsa4QUm0bx7FGxOHcG5ic3+HY0yj4c0QHsbUq7IKF2syD7J0cy5fb93PjIlDaBEdwYAOLVj4y5E2gZAx9cwShfE7VWXH/sMs3bKfpVv2s2J7HkVHyyv3L9uWx5h+7QAsSRjjB5YojN8dPFLGpc8tpuocWue0juGi7q25qHs853dt5b/gjDGWKIx/lFe4CAt1bpHFxURwUffWNIsKY0T31lzYPZ72/u7NVJQLWasg0/06sMPLggItu0KHIZA0FDoMhZh4n4bqVwV7YHcaZK+GPWuhrNi7cmGR0KwdNEuC5onQrP3x5chY38Zsas0Shal3h4+Wc+P05dyY3IGfXNAZgLfvGOq/gFwVsG8DZK50kkLWKsjffubnK8yGnV8fX2/Z1Z003MkjoQ+EBuCfXvFByP7WSQq7VzsJonBP3X+fyGbQLNFJGrHtILyRdoFWBa1wfj+PfXVVgKv8xHV1b7v6Jedn5gMB+NtqApnLpTz0nzWk7y7gSGkG4wd3qP/5Igpzjv+zy1oFWWlQWnjiMeHRkDjYqRF0GAYJvSHEiz+XijLYt96ddFKc75O/3Xmtfc997hhIGuwkDG/O6W9F+5zryNt66r7I5pA40PlZtR8IUS28O2fZESjYDYd2O1+rLh8tgNwCyN1Qt9cR7EqLfHbqAPgtNcHk+S82s2B9Ds2iwnj9tmTfJ4mTPwVnf+v8MzpZi45OQugwDJKGQJtzz/xTf1wn6HmFs1xRDjnpkJVyvMZycCfsWOK8AkloJLTr704Kg5yvLbtCSB32sleF4gNwKMudQLKdT8uNlYRASKjzgULcX0NC3dvDTtzXzDe1CbBEYerRx2uzefGrrYQIvHTzIN88MJe3DTZ/djwx5G879ZiIWGg/wPkEnDTEqTXEtq37WMBJNu0HOK+hdzvbCnOcxOH1fQ8/i2jq/KwS+kBYhG+/lwhEt3Re7fr79nsZr1miMPUiffchHvnvdwD89sd9GNGjdd2dvKIctnwGKa/Dtq9O3BcaCW37QeIg96fgQdCqe91+Cq6t2DbQ+0r/fX9jaskShfE5VeXRD9ZSUubi+sFJ3DG8c92cuGgfrH4LUt+EgixnW1gU9L4aOl3gJIb6+BRsTJCzRGF8TkR49ZbB/P2rLTw17lyqDDBZe6qwa7lTe1g/F1xlzvaWXWHIXXDeTU6zhTGmzvg0UYjIaJzZ8UKB11V16kn7RwJzgGONtR+q6h/c+zKAQqACKFfVZF/GanyrY6tonrn+vFN3VHh5o7LsMHz/X0j5p9OrCJwber2uhCF3QpeR/m1OMiaI+SxRiEgo8DLOFKpZQIqIzFXV9ScdulRVa2qwHaWq+30Vo/GtN7/ZQblLufPCLk4touSQ0+to92r3zeZvjzcZ1UZMAgy+HQZPhOZJdR63MeZEvqxRDAW2qup2ABF5DxgLnJwoTBD6ZkMmH3/yEf1kGwcyCml5ML36fviIUzM4HRGn6+qQO6HXVXbfwZh65MtEkQhkVlnPAoZVc9wFIvIdkA08oqrr3NsVWCAiCvxDVaf7MFZTV/ZtpOTD+xi2dzXvR7icbcd6qDbEHkjGmNPyZaKo7o6lnrS+GuikqkUiMgb4COju3jdcVbNFJAH4XEQ2quopTyiJyGRgMkDHjh3rLnpTK6rKntS5xM+/l6iKw5RrCJmRXUnqOxw5lhisB5IxAcmXH+WygA5V1pNwag2VVLVAVYvcy58C4SIS717Pdn/dB8zGaco6hapOV9VkVU1u3boO++ab0yoscfc4UqVk8Qu0/fh2IioO83HFMG5o8S4tH05Bxr4EyXc4D5xZkjAmIPmyRpECdBeRLsBuYAJwc9UDRKQtkKOqKiJDcRJXnojEACGqWuhe/iHwBx/GarxwqLiMFdvz+Gbrfr7Zup+io+Ws+OWFyCcP02TNv0Hg01YTOTzsF7zVvz0xkdb72phg4LO/ZFUtF5H7gc9wusfOUNV1InKPe/80YDxwr4iUA8XABHfSaAPMdve3DwNmqup8X8VqauZyKYs35/LW8gyWbM7FVaXxsENEEWVvXEVE9ioIawLjXmVM33F+i9UY4xuievJtg8CVnJysqamp/g4jqCzZnMttM1YBEB4qDOwQx/Bu8VzWch99Fk9GDmVBbHu46V2neckYE1BEJO10z6lZ24A5waa9hXy76wAThjodA4Z3i+cH57RiRI/W3JjcgbiYCNj4CXxwt/MQXGIyTHjHd4PqGWP8zhKFobzCxefrc3hreQYrtucTFiKM6pVAm2ZRhIYIM+8+3zlQFZb+Fb78I6DQ7wa4+u8QHuXP8I0xPmaJohErq3Ax4+sdvLUsg+xDJQBER4Ry3aBqnnYuK4G5P4fvZwEClz4OFz7kPAhnjAlqligaqSOl5Ux6I4WVO/IB6BIfw20XdOK6wUk0iwo/8WBV+O9E2DzPmZ3tuteg14/rP2hjjF9YomikmoSH0rZ5FAmxkUy9rh8jeyQQElJD7eDbfzlJIqo5TPwU2p5bv8EaY/zKEkUjc/hoOTGRYYgIU6/tT2FJGQnNPNxjOLAT5v/aWR7zV0sSxjRCNshOI1FW4eLxOelc9+oyjpQ6Q3s3iQj1nCRcLphznzNpe++rod/4eorWGNOQWI2iEcgtPMp9771QTRwAABL/SURBVKxmVUY+EaEhfLvrIMO7xZ++4KrpkLEUYlrDlX+zG9fGNFKWKILcd5kHueffaew5VEKbZpG8eutgBnWMO33B/Vvgi8ed5SufhxgvEosxJihZoghis1Iz+b+P0iktd5HcKY5Xbh1EQqwXzzxUlMNH90J5iTO1aO+a5pUyxjQGliiC1Irtefzq/bUA3Hp+R35/ZV8iwry8JbXsRchKcYbmGD319McbY4KaJYogNaxLS24e1pHzkppz45BazNORsw4W/tlZHvsSNGnhmwCNMQHDEkUQ2V90lNJyF+1bNEFE+PO4frU7QXkpfPhTcJVB8p3Q7VLfBGqMCSjWPTZIZOYf4fppy/nJP1dy4HDpmZ1kyV8g53uI6wyX2/QfxhiH1SiCwKa9hdw2YyU5BUfp3a4ZZS5X7U+SlQZLnwMErnkVIpvWeZzG1EZZWRlZWVmUlJT4O5SgEBUVRVJSEuHh4ac/+CSWKAJc2s587ngzlUPFZQzt0pLXb08+daym0ykrho/uAa2AC+6HTj/wTbDG1EJWVhaxsbF07twZsWd4zoqqkpeXR1ZWFl26dKl1eWt6CmALN+3jltdXcqi4jMt6t+HtO4bWPkkAfPUU7N8M8T3hkt/VfaDGnIGSkhJatWplSaIOiAitWrU649qZ1SgC1JacQu5+K5Vyl3L94CT+37X9CAs9g7yf8TUsfxkkFMZNs7klTINiSaLunM3P0hJFgOqW0JRJwzsTEiI8NrrXib8EXzwJK14BBMIiIDQCQiOrLEdAWKTzNXcToDDiEUgc5K/LMcY0YJYoAoiqcvBIGXExEYgIvxnT+9RPCWlvwdfPHV8vLz79idudBxc9UrfBGhPgDh48yMyZM/nZz35Wq3Jjxoxh5syZtGgRPM8gWaIIECVlFfzh4/V8vWU/7997AQmxUacmid2r4dNfOstjX4a+46Ci1Hk+ouIoVJRB+VFnW0Wps+wqg/aDnNqGMabSwYMHeeWVV05JFBUVFYSGhtZY7tNPP/V1aPXOEkUAWLhxH0/8bx07844QERbC5r1Fp47ZdDgPZt3mJITkO2Dgre4dMfUerzG+0PmxT2rc9+dx/bh5mDMCwcyVu/jN7O9rPDZjqnezMz722GNs27aNAQMGEB4eTtOmTWnXrh1r1qxh/fr1XHPNNWRmZlJSUsIDDzzA5MmTnTg7dyY1NZWioiKuuOIKLrzwQpYtW0ZiYiJz5syhSZMmtbjqhsESRQOWmX+EJ/+3ni825ADQPaEpU6/rz+BOJ43+6qqAD+6EQ5mQmGzjMxlTB6ZOnUp6ejpr1qxh0aJF/PjHPyY9Pb2ye+mMGTNo2bIlxcXFDBkyhOuuu45WrVqdcI4tW7bw7rvv8tprr3HDDTfwwQcfcOutt1b37Ro0SxQN1HurdvH43HUcLXfRNDKMBy/rzu0/6Ex4dT2bFv4Zti+E6Hi44W3nRrUxQcbbmsDNwzpW1i7q0tChQ094BuHFF19k9uzZAGRmZrJly5ZTEkWXLl0YMGAAAIMHDyYjI6PO46oPligaqI4tozla7uKaAe35zZjeNc9Et/ETWPosSAiMnwHNE+s3UGMaiZiY4824ixYt4osvvmD58uVER0czcuTIap9RiIw8/qEtNDSU4mIvOpc0QJYoGohdeUdYtHkft13QGYAfdIvn84dG0L1NbM2F8rbB7Huc5Usfh64X+z5QYxqJ2NhYCgsLq9136NAh4uLiiI6OZuPGjaxYsaKeo6tfPk0UIjIaeAEIBV5X1akn7R8JzAF2uDd9qKp/8KZssCgpq+DVRdt4dfE2Sstd9G3fvPIehMckUXoY/nMrHC2A3lfB8AfqKWJjGodWrVoxfPhwzj33XJo0aUKbNm0q940ePZpp06bRv39/evbsyfnnn+/HSH3PZ4lCREKBl4HLgSwgRUTmqur6kw5dqqpXnmHZgJaakc+v3l/L9v2HARg3MJEOcV70iFCFuVNg33po1R3GvmLzWRvjAzNnzqx2e2RkJPPmzat237H7EPHx8aSnp1duf+SRwH1WyZc1iqHAVlXdDiAi7wFjAW/+2Z9N2QavuLSCZz7bxBvLdqDq9GZ66ppzGda11ekLA6z8B6S/D+ExcOO/IaqZbwM2xjRqvhwUMBHIrLKe5d52sgtE5DsRmScifWtZFhGZLCKpIpKam5tbF3H73LMLNjHjmx2EiHD/qG58POVC75PEzuWw4LfO8jUvQ0Iv3wVqjDH4tkZRXVuInrS+GuikqkUiMgb4COjuZVlno+p0YDpAcnJytcc0NPeN6sbGvQU8Nro3/ZKae1+wcC/893ZwlTvDgfcd57sgjTHGzZc1iiygQ5X1JCC76gGqWqCqRe7lT4FwEYn3pmwgWbk9j5/+K5XScmdCoZYxEbxz1/neJQlVOJgJ62bDe7dAUQ50uhAue9LHURtjjMOXNYoUoLuIdAF2AxOAm6seICJtgRxVVREZipO48oCDpysbCA4fLecv8zfy1vKdAMxcuZOJw08zaUhJAWSvhqxU2J3mvIpyju+PbQfXvwGh1rPZGFM/fPbfRlXLReR+4DOcLq4zVHWdiNzj3j8NGA/cKyLlQDEwQVUVqLasr2L1heXb8vjVB9+RmV9MWIhw36hu3Dys06kHFuXCxv85U5HuTj0+7HdVUS0gKRkSBztjODVNqJdrMMYY8PFzFO7mpE9P2jatyvJLwEvelg0ExaUVPD1/I28uywCgd7tmPHt9f/q2P6mZSRXWvAOf/RZKDh7fHhIObfu5E0Oy87VlV+v+akwD17RpU4qKisjOzmbKlCm8//77pxwzcuRInn32WZKTk2s8z/PPP8/kyZOJjo4GGsaw5dZ+Ucfmpe/hzWUZhIUI91/SjftGdTt1fKb8HfC/B2DHYme980XQ60onKbTtZ2M1GRPA2rdvX22S8Nbzzz/PrbfeWpkoGsKw5ZYowLkf0H4ghNQ8xrwnqlo5N8S4gYmszTrE+MFJnJt4Ui2iohxWvgpf/cmZUKhJS2ek1/43WI3BGE+eqEXvwFqd91CNux599FE6depUOR/FE088gYiwZMkSDhw4QFlZGU899RRjx449oVxGRgZXXnkl6enpFBcXM2nSJNavX0/v3r1PGOvp3nvvJSUlheLiYsaPH8+TTz7Jiy++SHZ2NqNGjSI+Pp6FCxdWDlseHx/Pc889x4wZMwC46667ePDBB8nIyPD5cOa+7PUUGPJ3wOuXwnO9Yd6jkLnKaRby0rrsQ1z36jJ25jlPV4sIT1zd99QksTcd/nkZLPg/J0n0ux7uT4HzbrQkYUwDNGHCBP7zn/9Urs+aNYtJkyYxe/ZsVq9ezcKFC3n44YdRD/8vXn31VaKjo1m7di2//e1vSUtLq9z3pz/9idTUVNauXcvixYtZu3YtU6ZMoX379ixcuJCFCxeecK60tDTeeOMNVq5cyYoVK3jttdf49ttvAWc48/vuu49169bRokULPvjggzr9WViNomA3GtcFObADVk5zXs07wrnj4NzroG3/av+Rl1e4eHXRNl74cgvlLuX5L7bwtxsHnHr+shJY8hf45gXn+YdmSXDlc9DjR/VwccYECQ+f/H1l4MCB7Nu3j+zsbHJzc4mLi6Ndu3Y89NBDLFmyhJCQEHbv3k1OTg5t27at9hxLlixhypQpAPTv35/+/ftX7ps1axbTp0+nvLycPXv2sH79+hP2n+zrr79m3LhxlaPYXnvttSxdupSrr77a58OZW6LofCHbJizhoeff4vrIlfw4dAWtDu1y/rF/8wK06uYkjL7XVj4FvXVfEQ/PWsN3Wc4v7+0XdOLRK6p5QnrnMmdMprwtgMDQyXDp7yHSw2B/xpgGY/z48bz//vvs3buXCRMm8M4775Cbm0taWhrh4eF07ty52uHFqzplymJgx44dPPvss6SkpBAXF8fEiRNPex5PNRdfD2duiQLYW1DK7uhe/P5wVx7nRgbLZq4KXc6Y0JW0ztsKi5+GxU+jbfryXdRQlu4o4BKXi3ExoVzaqzUdolfCIpfTZKUu51WU4zwkBxDfE67+O3Qc5t8LNcbUyoQJE7j77rvZv38/ixcvZtasWSQkJBAeHs7ChQvZuXOnx/IjRozgnXfeYdSoUaSnp7N27VoACgoKiImJoXnz5uTk5DBv3jxGjhwJHB/ePD4+/pRzTZw4kcceewxVZfbs2fzrX//yyXWfzBIFcGH3eNL+7zKyDhSzNusQa7O6MT9rOM/tzmdgRTozBu8iZONcJGcdA1jHgBCcuzsVgKenO0LC4aJfwEUPW08mYwJQ3759KSwsJDExkXbt2nHLLbdw1VVXkZyczIABA+jVy/NYa/feey+TJk2if//+DBgwgKFDhwJw3nnnMXDgQPr27UvXrl0ZPnx4ZZnJkydzxRVX0K5duxPuUwwaNIiJEydWnuOuu+5i4MCB9TJrnniqzgSa5ORkTU1NrbPzuVzK7oPFdGgZDeWl6NYveP/jjxnUMY5zEpo5s8qFhDhfT3iFOj2oulwMrXvUWTzGNCYbNmygd+/e/g4jqFT3MxWRNFWt+cEOrEbhUUiIOEkCICwC6TWG63uN8W9QxhhTz6x7rDHGGI8sURhjGqxgahr3t7P5WVqiMMY0SFFRUeTl5VmyqAOqSl5eHlFRUWdU3u5RGGMapKSkJLKysgiUmSsbuqioKJKSks6orCUKY0yDFB4eTpcup5m/xdQLa3oyxhjjkSUKY4wxHlmiMMYY41FQPZktIrmA58FXahYP7K/DcPwt2K4Hgu+agu16IPiuKdiuB069pk6q2tpTgaBKFGdDRFJP9xh7IAm264Hgu6Zgux4IvmsKtuuBM7sma3oyxhjjkSUKY4wxHlmiOG66vwOoY8F2PRB81xRs1wPBd03Bdj1wBtdk9yiMMcZ4ZDUKY4wxHlmiMMYY41GjTxQiMlpENonIVhF5zN/x1AURyRCR70VkjYjU3ZR/9UREZojIPhFJr7KtpYh8LiJb3F/j/BljbdVwTU+IyG73+7RGRAJmViwR6SAiC0Vkg4isE5EH3NsD9n3ycE0B+T6JSJSIrBKR79zX86R7e63fo0Z9j0JEQoHNwOVAFpAC3KSq6/0a2FkSkQwgWVUD8kEhERkBFAFvq+q57m1/AfJVdao7ocep6qP+jLM2arimJ4AiVX3Wn7GdCRFpB7RT1dUiEgukAdcAEwnQ98nDNd1AAL5PIiJAjKoWiUg48DXwAHAttXyPGnuNYiiwVVW3q2op8B4w1s8xNXqqugTIP2nzWOAt9/JbOH/AAaOGawpYqrpHVVe7lwuBDUAiAfw+ebimgKSOIvdquPulnMF71NgTRSKQWWU9iwD+xahCgQUikiYik/0dTB1po6p7wPmDBhL8HE9duV9E1rqbpgKmmaYqEekMDARWEiTv00nXBAH6PolIqIisAfYBn6vqGb1HjT1RSDXbgqEtbriqDgKuAO5zN3uYhudV4BxgALAH+Kt/w6k9EWkKfAA8qKoF/o6nLlRzTQH7PqlqhaoOAJKAoSJy7pmcp7EniiygQ5X1JCDbT7HUGVXNdn/dB8zGaWILdDnuNuRjbcn7/BzPWVPVHPcfsgt4jQB7n9zt3h8A76jqh+7NAf0+VXdNgf4+AajqQWARMJozeI8ae6JIAbqLSBcRiQAmAHP9HNNZEZEY9404RCQG+CGQ7rlUQJgL3O5evh2Y48dY6sSxP1a3cQTQ++S+UfpPYIOqPldlV8C+TzVdU6C+TyLSWkRauJebAJcBGzmD96hR93oCcHd1ex4IBWao6p/8HNJZEZGuOLUIcKa6nRlo1yQi7wIjcYZDzgEeBz4CZgEdgV3A9aoaMDeHa7imkTjNGQpkAD891nbc0InIhcBS4HvA5d78G5w2/YB8nzxc000E4PskIv1xblaH4lQKZqnqH0SkFbV8jxp9ojDGGONZY296MsYYcxqWKIwxxnhkicIYY4xHliiMMcZ4ZInCGGOMR5YojKkFEamoMoromroccVhEOlcdXdaYhiLM3wEYE2CK3UMiGNNoWI3CmDrgngPkaff4/6tEpJt7eycR+dI9oNyXItLRvb2NiMx2zxXwnYj8wH2qUBF5zT1/wAL3E7XG+JUlCmNqp8lJTU83VtlXoKpDgZdwnvbHvfy2qvYH3gFedG9/EVisqucBg4B17u3dgZdVtS9wELjOx9djzGnZk9nG1IKIFKlq02q2ZwCXqOp298Bye1W1lYjsx5kMp8y9fY+qxotILpCkqkernKMzzlDQ3d3rjwLhqvqU76/MmJpZjcKYuqM1LNd0THWOVlmuwO4jmgbAEoUxdefGKl+Xu5eX4YxKDHALznSUAF8C90Ll5DLN6itIY2rLPq0YUztN3DOGHTNfVY91kY0UkZU4H8Bucm+bAswQkV8CucAk9/YHgOkicidOzeFenElxjGlw7B6FMXXAfY8iWVX3+zsWY+qaNT0ZY4zxyGoUxhhjPLIahTHGGI8sURhjjPHIEoUxxhiPLFEYY4zxyBKFMcYYj/4/cMx7t9dKo/oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_plots(train_losses, val_losses, train_accs, test_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('loss_DLN_umotivation_2layer_cls.png')\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, test_accs, '-', linewidth=2, label='validation')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('accuracy_DLN_umotivation_2layer_cls.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1)\n",
    "# print(per_epoch_train_loss)\n",
    "# print(per_epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction(training_data_des, training_data_loc, training_data_net, ground_truths):\n",
    "    for epoch in range(0,30):\n",
    "        model = JointDLN()\n",
    "        model.load_state_dict(torch.load(\"data/DLN_umotivation_2layer/joint_DLN_\"+str(epoch)+\".pt\")) \n",
    "        predictions =[]\n",
    "        for i in range (0,len(training_data_des)):\n",
    "            prediction_joint = model(training_data_des[i], training_data_loc[i], training_data_net[i])\n",
    "            pred = torch.argmax(prediction_joint, dim=1)\n",
    "            predictions.append(pred.item())\n",
    "        #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "        accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "        macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "        print('epoch :', epoch, 'Testing accuracy, macro_f1:', accuracy, macro_f1)\n",
    "        #return accuracy, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no location for user:  vaibhavatttc\n"
     ]
    }
   ],
   "source": [
    "#####prepare testing data for neural net #########\n",
    "testing_data_des, testing_data_loc =  nn_input(test_data,df)\n",
    "testing_data_net =  nn_input_network(test_data,df)\n",
    "test_gt = find_groundtruth(test_data, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "############........Calculate Validation Accuracy.......\n",
    "# test_accuracy, test_macro_f1 = make_prediction(testing_data_des, testing_data_loc, testing_data_net, test_gt)\n",
    "# print('Testing accuracy, macro_f1:', test_accuracy, test_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Testing accuracy, macro_f1: 0.7213740458015268 0.4941982392181596\n",
      "epoch : 1 Testing accuracy, macro_f1: 0.7938931297709924 0.5403174603174603\n",
      "epoch : 2 Testing accuracy, macro_f1: 0.7977099236641222 0.541720430107527\n",
      "epoch : 3 Testing accuracy, macro_f1: 0.8015267175572519 0.5450276257869203\n",
      "epoch : 4 Testing accuracy, macro_f1: 0.816793893129771 0.55684666210982\n",
      "epoch : 5 Testing accuracy, macro_f1: 0.8358778625954199 0.5712999036487788\n",
      "epoch : 6 Testing accuracy, macro_f1: 0.8358778625954199 0.5714733542319749\n",
      "epoch : 7 Testing accuracy, macro_f1: 0.8244274809160306 0.5635702244698784\n",
      "epoch : 8 Testing accuracy, macro_f1: 0.8320610687022901 0.5688936207967349\n",
      "epoch : 9 Testing accuracy, macro_f1: 0.8206106870229007 0.5611679986679986\n",
      "epoch : 10 Testing accuracy, macro_f1: 0.8206106870229007 0.5611679986679986\n",
      "epoch : 11 Testing accuracy, macro_f1: 0.8244274809160306 0.5638986354775829\n",
      "epoch : 12 Testing accuracy, macro_f1: 0.8320610687022901 0.6123109024517475\n",
      "epoch : 13 Testing accuracy, macro_f1: 0.8282442748091603 0.6470323106589118\n",
      "epoch : 14 Testing accuracy, macro_f1: 0.8282442748091603 0.6470323106589118\n",
      "epoch : 15 Testing accuracy, macro_f1: 0.8244274809160306 0.6443074122889599\n",
      "epoch : 16 Testing accuracy, macro_f1: 0.816793893129771 0.6350926853908508\n",
      "epoch : 17 Testing accuracy, macro_f1: 0.816793893129771 0.6319546592856201\n",
      "epoch : 18 Testing accuracy, macro_f1: 0.8091603053435115 0.6266192170818505\n",
      "epoch : 19 Testing accuracy, macro_f1: 0.8129770992366412 0.6293627382122957\n",
      "epoch : 20 Testing accuracy, macro_f1: 0.8091603053435115 0.6267717198511571\n",
      "epoch : 21 Testing accuracy, macro_f1: 0.8129770992366412 0.6293627382122957\n",
      "epoch : 22 Testing accuracy, macro_f1: 0.8129770992366412 0.6291612061116112\n",
      "epoch : 23 Testing accuracy, macro_f1: 0.8091603053435115 0.6238787314688014\n",
      "epoch : 24 Testing accuracy, macro_f1: 0.8091603053435115 0.6238787314688014\n",
      "epoch : 25 Testing accuracy, macro_f1: 0.8091603053435115 0.6237204121414647\n",
      "epoch : 26 Testing accuracy, macro_f1: 0.8053435114503816 0.6209613223450084\n",
      "epoch : 27 Testing accuracy, macro_f1: 0.8053435114503816 0.6209613223450084\n",
      "epoch : 28 Testing accuracy, macro_f1: 0.8053435114503816 0.6209613223450084\n",
      "epoch : 29 Testing accuracy, macro_f1: 0.8053435114503816 0.6209613223450084\n"
     ]
    }
   ],
   "source": [
    "make_prediction(testing_data_des, testing_data_loc, testing_data_net, test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
