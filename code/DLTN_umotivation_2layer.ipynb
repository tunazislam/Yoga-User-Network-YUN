{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1307c5750>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## load embeddings #######\n",
    "loc_emb = torch.load('data/locationEmbeddings.pt') \n",
    "des_emb = torch.load('data/descriptionEmbeddings.pt') \n",
    "twt_emb = torch.load('data/tweetsEmbeddings.pt') \n",
    "\n",
    "#load network embedding\n",
    "net_emb = gsm.KeyedVectors.load_word2vec_format('data/userNetworkEmd.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "#user = net_emb ['000mrs000']\n",
    "#print(user)\n",
    "print(type(net_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 1300 user location, description, yoga tweets, utype, umotivation\n",
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "#print (df) #[1308 rows x 7 columns] name, location, description, text, utype, umotivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create BiLSTMAttention Model for Description\n",
    "class BiLSTMDesAtt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMDesAtt, self).__init__() \n",
    "        self.lstm = nn.LSTM(300, 150//2 , num_layers=1, bidirectional=True ) #BiLSTM with attention \n",
    "        #self.lstm = nn.LSTM(300, 150 , num_layers=1, bidirectional=False) #LSTM with attention\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "        self.attn_fc = torch.nn.Linear(300, 1) #attention layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)\n",
    "        return (torch.zeros(2 * 1, 1, 150//2), torch.zeros(2 * 1, 1, 150//2)) # <- change here: first dim of hidden needs to be doubled\n",
    "        #return (torch.zeros(1 * 1, 1, 150), torch.zeros(1 * 1, 1, 150))#LSTM with attention\n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size()) #torch.Size([13, 1, 150])\n",
    "        #rnn_out = rnn_out.squeeze(0).unsqueeze(1) \n",
    "        #rnn_out = rnn_out.permute(2,0,1) \n",
    "        rnn_out = rnn_out.permute(1,0,2) \n",
    "        #print(\"permute rnn_out\", rnn_out.size()) #torch.Size([150, 13, 1])\n",
    "        #print(\"state\", state.size()) #torch.Size([2, 1, 75])\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        #print(\"merged_state\", merged_state.size()) #torch.Size([1, 150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).size()) #torch.Size([150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).size()) #torch.Size([150, 1])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).unsqueeze(2).size()) # torch.Size([150, 1, 1])\n",
    "        #merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"merged_state2 :\", merged_state.size()) #torch.Size([150, 1, 1])\n",
    "        merged_state = merged_state.permute(1,0,2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([13, 300])\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        lstm_out, hidden = self.lstm(X.view(len(X),1, -1))\n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('hidden[0] = h_n', hidden[0], hidden[0].size()) # torch.Size([2, 1, 75])\n",
    "        #print('hidden[1] = c_n', hidden[1], hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        h_n, c_n = hidden\n",
    "        #print('h_n', h_n, h_n.size()) # torch.Size([2, 1, 75])\n",
    "        #print('c_n', c_n, hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        attn_out = self.attention(lstm_out, h_n)\n",
    "        #print(\"attn_out\", attn_out.size()) #torch.Size([150, 1])\n",
    "        #logits = self.fc2(attn_out)\n",
    "        #logits = self.fc2(attn_out.permute(1,0))\n",
    "        #print(\"logits\", logits, logits.size())\n",
    "        #return logits \n",
    "        return attn_out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create BiLSTMAttention Model for Description\n",
    "class BiLSTMTwtAtt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMTwtAtt, self).__init__() \n",
    "        self.lstm = nn.LSTM(300, 150//2 , num_layers=1, bidirectional=True ) #BiLSTM with attention \n",
    "        #self.lstm = nn.LSTM(300, 150 , num_layers=1, bidirectional=False) #LSTM with attention\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "        self.attn_fc = torch.nn.Linear(300, 1) #attention layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)\n",
    "        return (torch.zeros(2 * 1, 1, 150//2), torch.zeros(2 * 1, 1, 150//2)) # <- change here: first dim of hidden needs to be doubled\n",
    "        #return (torch.zeros(1 * 1, 1, 150), torch.zeros(1 * 1, 1, 150))#LSTM with attention\n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size()) #torch.Size([13, 1, 150])\n",
    "        #rnn_out = rnn_out.squeeze(0).unsqueeze(1) \n",
    "        #rnn_out = rnn_out.permute(2,0,1) \n",
    "        rnn_out = rnn_out.permute(1,0,2) \n",
    "        #print(\"permute rnn_out\", rnn_out.size()) #torch.Size([150, 13, 1])\n",
    "        #print(\"state\", state.size()) #torch.Size([2, 1, 75])\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        #print(\"merged_state\", merged_state.size()) #torch.Size([1, 150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).size()) #torch.Size([150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).size()) #torch.Size([150, 1])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).unsqueeze(2).size()) # torch.Size([150, 1, 1])\n",
    "        #merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"merged_state2 :\", merged_state.size()) #torch.Size([150, 1, 1])\n",
    "        merged_state = merged_state.permute(1,0,2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([13, 300])\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        lstm_out, hidden = self.lstm(X.view(len(X),1, -1))\n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('hidden[0] = h_n', hidden[0], hidden[0].size()) # torch.Size([2, 1, 75])\n",
    "        #print('hidden[1] = c_n', hidden[1], hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        h_n, c_n = hidden\n",
    "        #print('h_n', h_n, h_n.size()) # torch.Size([2, 1, 75])\n",
    "        #print('c_n', c_n, hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        attn_out = self.attention(lstm_out, h_n)\n",
    "        #print(\"attn_out\", attn_out.size()) #torch.Size([150, 1])\n",
    "        #logits = self.fc2(attn_out)\n",
    "        #logits = self.fc2(attn_out.permute(1,0))\n",
    "        #print(\"logits\", logits, logits.size())\n",
    "        #return logits \n",
    "        return attn_out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create LSTM Model for Location #############\n",
    "class LSTMLoc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMLoc, self).__init__()\n",
    "        self.lstm = nn.LSTM(300, 150, num_layers=1)\n",
    "        self.fc2 = nn.Linear(150, 50) \n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "\n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)# <- change here: first dim of hidden needs to be doubled\n",
    "        return (torch.zeros(1, 1, 150), torch.zeros(1, 1, 150)) \n",
    "    def forward(self, x):\n",
    "        #x=embeds.permute(1,0,2)\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        #lstm_out, self.hidden = self.lstm(x.view(len(x),1, -1), self.hidden)\n",
    "        lstm_out, _ = self.lstm(x.view(len(x),1, -1))\n",
    "        #lstm_out, _ = self.lstm(x.view(len(x),1,-1)) \n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('self.hidden[0]', self.hidden[0], self.hidden[0].size()) # torch.Size([1, 1, 150])\n",
    "        #print(\"lstm_out[-1]\", lstm_out[-1], lstm_out[-1].size())  # torch.Size([1, 150])\n",
    "        #x = self.fc2(lstm_out[-1])  \n",
    "        #out = F.log_softmax(x, dim = 1)\n",
    "        #return out\n",
    "        #return x\n",
    "        return lstm_out[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create Model \n",
    "class NetworkMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkMLP, self).__init__() \n",
    "        self.fc1 = nn.Linear(300, 150)\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([300])\n",
    "        #print('resize', X.view(1,len(X)).size()) #resize torch.Size([1, 300])\n",
    "        z1 = self.fc1(X.view(1,len(X)))\n",
    "        #print('z1', z1, z1.size()) # torch.Size([1, 150])\n",
    "        h1 = F.relu(z1)\n",
    "        return h1\n",
    "        #logits = self.fc2(h1) #without attention\n",
    "        #print(\"logits\", logits, logits.size()) #torch.Size([1, 3])\n",
    "        #return logits \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointDLTN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JointDLTN, self).__init__()\n",
    "        self.model_des = BiLSTMDesAtt()\n",
    "        self.model_loc = LSTMLoc()\n",
    "        #self.model_twt = BiLSTMDesAtt()\n",
    "        self.model_twt = BiLSTMTwtAtt()\n",
    "        self.model_net = NetworkMLP()\n",
    "        #self.fc = nn.Linear(200, 3) #4*50 = 150\n",
    "        self.fc1 = nn.Linear(600, 200) #4*150 = 600\n",
    "        self.fc2 = nn.Linear(200, 3)\n",
    "    def forward(self, x_d, x_l, x_t, x_n): \n",
    "        prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size())\n",
    "        prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size())\n",
    "        prediction_twt = self.model_twt(x_t)\n",
    "        prediction_net = self.model_net(x_n)\n",
    "        concat_pred = torch.cat((prediction_des, prediction_loc, prediction_twt, prediction_net), 1) #concat with dim= 1\n",
    "        #print(concat_pred, concat_pred.size()) \n",
    "#         out = F.log_softmax(self.fc(concat_pred), dim = 1)\n",
    "#         return out\n",
    "        out = self.fc1(concat_pred)\n",
    "        out = self.fc2(F.relu(out))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net input #########\n",
    "def nn_input(train_data,df):\n",
    "    #ground_truths = []\n",
    "    training_data_des =[]\n",
    "    training_data_loc=[]\n",
    "    training_data_twt =[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                if (not des_emb[train_data[i]]) and (not loc_emb[train_data[i]]) and (twt_emb[train_data[i]]):\n",
    "                    print ('no description and location for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    training_data_loc.append(torch.zeros(1, 300))\n",
    "                    sent_tensor_twt = torch.stack(twt_emb[train_data[i]],dim = 1)\n",
    "                    training_data_twt.append(sent_tensor_twt[-1])\n",
    "                    break\n",
    "                \n",
    "                elif (des_emb[train_data[i]]) and (not loc_emb[train_data[i]]) and (twt_emb[train_data[i]]): \n",
    "                    print ('no location for user: ', train_data[i])\n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    training_data_loc.append(torch.zeros(1, 300))\n",
    "                    sent_tensor_twt = torch.stack(twt_emb[train_data[i]],dim = 1)\n",
    "                    training_data_twt.append(sent_tensor_twt[-1])\n",
    "                    break\n",
    "                    \n",
    "                elif (not des_emb[train_data[i]]) and (loc_emb[train_data[i]]) and (twt_emb[train_data[i]]): \n",
    "                    print ('no description for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    sent_tensor_loc = torch.stack(loc_emb[train_data[i]],dim = 1)\n",
    "                    training_data_loc.append(sent_tensor_loc[-1])\n",
    "                    sent_tensor_twt = torch.stack(twt_emb[train_data[i]],dim = 1)\n",
    "                    training_data_twt.append(sent_tensor_twt[-1])\n",
    "                    break    \n",
    "               \n",
    "                elif (des_emb[train_data[i]]) and (loc_emb[train_data[i]]) and (twt_emb[train_data[i]]): \n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    sent_tensor_loc = torch.stack(loc_emb[train_data[i]],dim = 1)\n",
    "                    training_data_loc.append(sent_tensor_loc[-1])\n",
    "                    sent_tensor_twt = torch.stack(twt_emb[train_data[i]],dim = 1)\n",
    "                    training_data_twt.append(sent_tensor_twt[-1])\n",
    "                    break\n",
    "    return training_data_des, training_data_loc, training_data_twt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net #########\n",
    "def nn_input_network(train_data,df):\n",
    "    training_data =[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                #print(train_data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #print (\"net_emb[train_data[i]] : \", net_emb[train_data[i]], type(net_emb[train_data[i]]), torch.Tensor(net_emb[train_data[i]]), type(torch.Tensor(net_emb[train_data[i]])))\n",
    "                #count = 0\n",
    "                if(train_data[i] not in net_emb ):\n",
    "                    net_emb[train_data[i]] = np.zeros(300) #For users not appearing in the mention network, we set their network embedding vectors as 0.\n",
    "                    #count = count + 1\n",
    "                #print(count)\n",
    "                #print(net_emb[train_data[i]]) #ok\n",
    "                ####.....convert ndarray to torch.tensor........\n",
    "                net_emb_tensor = torch.Tensor(net_emb[train_data[i]])\n",
    "                #print(net_emb_tensor) #ok\n",
    "                training_data.append(net_emb_tensor)\n",
    "                break\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Ground Truth #########\n",
    "def find_groundtruth(data, df):\n",
    "    ground_truths = []\n",
    "    for i in range (0, len(data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (data[i] == df.name[j]):\n",
    "                #print(data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #target_type = torch.tensor(utype, dtype=torch.long) #for user type\n",
    "                target_type = torch.tensor(umotivation, dtype=torch.long) #for user motivation\n",
    "                ground_truths.append(target_type)\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_tr(model, training_data_des, training_data_loc, training_data_twt, training_data_net, ground_truths):\n",
    "    predictions =[]\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i], training_data_twt[i], training_data_net[i])\n",
    "        \n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_val(model, training_data_des, training_data_loc, training_data_twt, training_data_net, ground_truths):\n",
    "    predictions =[]\n",
    "    val_losses = []\n",
    "    loss_function = nn.NLLLoss()\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i], training_data_twt[i], training_data_net[i])\n",
    "        val_loss = loss_function(prediction_joint, ground_truths[i])\n",
    "        val_losses.append(val_loss.item())\n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    return accuracy, macro_f1, val_losses\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no description for user:  bchi49\n",
      "no location for user:  Christoph_Tran\n",
      "no description for user:  viecestlavie\n",
      "no description for user:  crystalization_\n",
      "no location for user:  yogitimesonline\n",
      "no description for user:  mimmosamami\n",
      "no description for user:  wenmarbyoga\n",
      "no location for user:  cipherEquality\n",
      "no description for user:  YogaLifeLine\n",
      "no location for user:  thewaywecame\n"
     ]
    }
   ],
   "source": [
    "##########......prepare training and validation data\n",
    "# ground truth training\n",
    "train_gt = find_groundtruth(train_data, df)\n",
    "#####prepare training data for neural net #########\n",
    "training_data_net =  nn_input_network(train_data,df)\n",
    "#print(training_data_net, len(training_data_net)) #ok\n",
    "training_data_des, training_data_loc, training_data_twt =  nn_input(train_data,df)\n",
    "\n",
    "# ground truth validation\n",
    "valid_gt = find_groundtruth(valid_data, df)\n",
    "#####prepare validation data for neural net #########\n",
    "validation_data_net =  nn_input_network(valid_data,df)\n",
    "validation_data_des, validation_data_loc, validation_data_twt =  nn_input(valid_data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Starting with epoch:  0 ***********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunaz/miniconda2/envs/py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Train accuracy and macro_f1: 0.6060240963855422 0.4110014776681444\n",
      "epoch : 0 Validation accuracy, macro_f1: 0.6601941747572816 0.44431469858414835\n",
      "train loss per epoch 0.8230578087180493\n",
      "Validation loss per epoch: 0.7241739179324178\n",
      "*************** Starting with epoch:  1 ***********************\n",
      "epoch : 1 Train accuracy and macro_f1: 0.6506024096385542 0.44443469669530417\n",
      "epoch : 1 Validation accuracy, macro_f1: 0.6941747572815534 0.46819363065549696\n",
      "train loss per epoch 0.770523130785988\n",
      "Validation loss per epoch: 0.6665857729402561\n",
      "*************** Starting with epoch:  2 ***********************\n",
      "epoch : 2 Train accuracy and macro_f1: 0.6819277108433734 0.46727988669437054\n",
      "epoch : 2 Validation accuracy, macro_f1: 0.7184466019417476 0.4847825386078603\n",
      "train loss per epoch 0.7357930981729883\n",
      "Validation loss per epoch: 0.6339096612143285\n",
      "*************** Starting with epoch:  3 ***********************\n",
      "epoch : 3 Train accuracy and macro_f1: 0.7036144578313253 0.48288524468243343\n",
      "epoch : 3 Validation accuracy, macro_f1: 0.7378640776699029 0.49798711755233493\n",
      "train loss per epoch 0.7088981254452682\n",
      "Validation loss per epoch: 0.6086993344779154\n",
      "*************** Starting with epoch:  4 ***********************\n",
      "epoch : 4 Train accuracy and macro_f1: 0.7192771084337349 0.4938951467111668\n",
      "epoch : 4 Validation accuracy, macro_f1: 0.7524271844660194 0.5077356516139419\n",
      "train loss per epoch 0.686602882089385\n",
      "Validation loss per epoch: 0.5888883425194082\n",
      "*************** Starting with epoch:  5 ***********************\n",
      "epoch : 5 Train accuracy and macro_f1: 0.7337349397590361 0.5167703967459168\n",
      "epoch : 5 Validation accuracy, macro_f1: 0.7669902912621359 0.5174268779796418\n",
      "train loss per epoch 0.667146396074429\n",
      "Validation loss per epoch: 0.5742829326287057\n",
      "*************** Starting with epoch:  6 ***********************\n",
      "epoch : 6 Train accuracy and macro_f1: 0.7469879518072289 0.5259608477815819\n",
      "epoch : 6 Validation accuracy, macro_f1: 0.7718446601941747 0.5206450469608365\n",
      "train loss per epoch 0.6497593577470303\n",
      "Validation loss per epoch: 0.5641170517333502\n",
      "*************** Starting with epoch:  7 ***********************\n",
      "epoch : 7 Train accuracy and macro_f1: 0.7578313253012048 0.5334256046818862\n",
      "epoch : 7 Validation accuracy, macro_f1: 0.7766990291262136 0.5237058538945332\n",
      "train loss per epoch 0.6339747123510004\n",
      "Validation loss per epoch: 0.5538055480105205\n",
      "*************** Starting with epoch:  8 ***********************\n",
      "epoch : 8 Train accuracy and macro_f1: 0.7686746987951807 0.5763482422503451\n",
      "epoch : 8 Validation accuracy, macro_f1: 0.7718446601941747 0.5203362212219479\n",
      "train loss per epoch 0.6195096896674579\n",
      "Validation loss per epoch: 0.548893807293142\n",
      "*************** Starting with epoch:  9 ***********************\n",
      "epoch : 9 Train accuracy and macro_f1: 0.7795180722891566 0.5837905180931061\n",
      "epoch : 9 Validation accuracy, macro_f1: 0.7669902912621359 0.5184464760422745\n",
      "train loss per epoch 0.6061693748364966\n",
      "Validation loss per epoch: 0.5448634491383451\n",
      "*************** Starting with epoch:  10 ***********************\n",
      "epoch : 10 Train accuracy and macro_f1: 0.7927710843373494 0.6244399174778922\n",
      "epoch : 10 Validation accuracy, macro_f1: 0.7669902912621359 0.5197640694756737\n",
      "train loss per epoch 0.5937933692665769\n",
      "Validation loss per epoch: 0.5428159260055394\n",
      "*************** Starting with epoch:  11 ***********************\n",
      "epoch : 11 Train accuracy and macro_f1: 0.8 0.6293885847021735\n",
      "epoch : 11 Validation accuracy, macro_f1: 0.7718446601941747 0.5229851330203443\n",
      "train loss per epoch 0.582255148540539\n",
      "Validation loss per epoch: 0.5435652909348312\n",
      "*************** Starting with epoch:  12 ***********************\n",
      "epoch : 12 Train accuracy and macro_f1: 0.8072289156626506 0.6343456974524936\n",
      "epoch : 12 Validation accuracy, macro_f1: 0.7766990291262136 0.5262024759015511\n",
      "train loss per epoch 0.5713713224890942\n",
      "Validation loss per epoch: 0.5420322788571849\n",
      "*************** Starting with epoch:  13 ***********************\n",
      "epoch : 13 Train accuracy and macro_f1: 0.8120481927710843 0.6642923075212136\n",
      "epoch : 13 Validation accuracy, macro_f1: 0.7766990291262136 0.5262024759015511\n",
      "train loss per epoch 0.5610848439232208\n",
      "Validation loss per epoch: 0.5451130444563709\n",
      "*************** Starting with epoch:  14 ***********************\n",
      "epoch : 14 Train accuracy and macro_f1: 0.8216867469879519 0.6709218793021451\n",
      "epoch : 14 Validation accuracy, macro_f1: 0.7815533980582524 0.5294165646674828\n",
      "train loss per epoch 0.5513150203658874\n",
      "Validation loss per epoch: 0.5466785911217477\n",
      "*************** Starting with epoch:  15 ***********************\n",
      "epoch : 15 Train accuracy and macro_f1: 0.8289156626506025 0.6829553164809652\n",
      "epoch : 15 Validation accuracy, macro_f1: 0.7864077669902912 0.5328081420952194\n",
      "train loss per epoch 0.5419925709647766\n",
      "Validation loss per epoch: 0.5492866204201596\n",
      "*************** Starting with epoch:  16 ***********************\n",
      "epoch : 16 Train accuracy and macro_f1: 0.8361445783132531 0.7099677671252197\n",
      "epoch : 16 Validation accuracy, macro_f1: 0.7864077669902912 0.5341859321200197\n",
      "train loss per epoch 0.5330819645855631\n",
      "Validation loss per epoch: 0.5525747262737126\n",
      "*************** Starting with epoch:  17 ***********************\n",
      "epoch : 17 Train accuracy and macro_f1: 0.8457831325301205 0.7165918939489556\n",
      "epoch : 17 Validation accuracy, macro_f1: 0.7912621359223301 0.5374184816045281\n",
      "train loss per epoch 0.5245475120393627\n",
      "Validation loss per epoch: 0.5551452827685087\n",
      "*************** Starting with epoch:  18 ***********************\n",
      "epoch : 18 Train accuracy and macro_f1: 0.8530120481927711 0.7289320777389864\n",
      "epoch : 18 Validation accuracy, macro_f1: 0.7961165048543689 0.540809968847352\n",
      "train loss per epoch 0.5163564844776273\n",
      "Validation loss per epoch: 0.558375335723451\n",
      "*************** Starting with epoch:  19 ***********************\n",
      "epoch : 19 Train accuracy and macro_f1: 0.8542168674698796 0.7298128510110732\n",
      "epoch : 19 Validation accuracy, macro_f1: 0.8009708737864077 0.5440465936589968\n",
      "train loss per epoch 0.5084485641732274\n",
      "Validation loss per epoch: 0.5629379804273253\n",
      "*************** Starting with epoch:  20 ***********************\n",
      "epoch : 20 Train accuracy and macro_f1: 0.8590361445783132 0.7403077724083375\n",
      "epoch : 20 Validation accuracy, macro_f1: 0.8009708737864077 0.5440465936589968\n",
      "train loss per epoch 0.5008310870396222\n",
      "Validation loss per epoch: 0.5687311575250719\n",
      "*************** Starting with epoch:  21 ***********************\n",
      "epoch : 21 Train accuracy and macro_f1: 0.8626506024096385 0.7408945100099521\n",
      "epoch : 21 Validation accuracy, macro_f1: 0.8058252427184466 0.5472813238770685\n",
      "train loss per epoch 0.49346591669572404\n",
      "Validation loss per epoch: 0.5747118565498046\n",
      "*************** Starting with epoch:  22 ***********************\n",
      "epoch : 22 Train accuracy and macro_f1: 0.8662650602409638 0.7502187144090359\n",
      "epoch : 22 Validation accuracy, macro_f1: 0.8009708737864077 0.5440465936589968\n",
      "train loss per epoch 0.4863090987756825\n",
      "Validation loss per epoch: 0.5813088280338685\n",
      "*************** Starting with epoch:  23 ***********************\n",
      "epoch : 23 Train accuracy and macro_f1: 0.8686746987951808 0.7519335779397452\n",
      "epoch : 23 Validation accuracy, macro_f1: 0.7961165048543689 0.5422209035916201\n",
      "train loss per epoch 0.4793418411889214\n",
      "Validation loss per epoch: 0.5883363525482636\n",
      "*************** Starting with epoch:  24 ***********************\n",
      "epoch : 24 Train accuracy and macro_f1: 0.8710843373493976 0.7602530585531543\n",
      "epoch : 24 Validation accuracy, macro_f1: 0.7961165048543689 0.5436468482799762\n",
      "train loss per epoch 0.4725565687132977\n",
      "Validation loss per epoch: 0.5956592453842603\n",
      "*************** Starting with epoch:  25 ***********************\n",
      "epoch : 25 Train accuracy and macro_f1: 0.8759036144578313 0.7700351476141464\n",
      "epoch : 25 Validation accuracy, macro_f1: 0.7961165048543689 0.5436468482799762\n",
      "train loss per epoch 0.4659369007518137\n",
      "Validation loss per epoch: 0.6007219837996566\n",
      "*************** Starting with epoch:  26 ***********************\n",
      "epoch : 26 Train accuracy and macro_f1: 0.8783132530120482 0.7779726469532519\n",
      "epoch : 26 Validation accuracy, macro_f1: 0.7912621359223301 0.5402478961986487\n",
      "train loss per epoch 0.4594800499720031\n",
      "Validation loss per epoch: 0.6093333091912339\n",
      "*************** Starting with epoch:  27 ***********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 27 Train accuracy and macro_f1: 0.8843373493975903 0.796425271231927\n",
      "epoch : 27 Validation accuracy, macro_f1: 0.7912621359223301 0.5402478961986487\n",
      "train loss per epoch 0.45317046752407114\n",
      "Validation loss per epoch: 0.617004171863777\n",
      "*************** Starting with epoch:  28 ***********************\n",
      "epoch : 28 Train accuracy and macro_f1: 0.8867469879518072 0.7958626996898049\n",
      "epoch : 28 Validation accuracy, macro_f1: 0.7912621359223301 0.5402478961986487\n",
      "train loss per epoch 0.446997820132667\n",
      "Validation loss per epoch: 0.6252937974968871\n",
      "*************** Starting with epoch:  29 ***********************\n",
      "epoch : 29 Train accuracy and macro_f1: 0.8927710843373494 0.8114046630925807\n",
      "epoch : 29 Validation accuracy, macro_f1: 0.7961165048543689 0.5436957695022211\n",
      "train loss per epoch 0.4409603205790564\n",
      "Validation loss per epoch: 0.6343268440059812\n"
     ]
    }
   ],
   "source": [
    "###########.........Start Training...........\n",
    "model = JointDLTN()\n",
    "##### Hyperparameter\n",
    "learning_rate=0.01\n",
    "epochs = 30\n",
    "#opt=\"ADAM\"\n",
    "#opt=\"SGD\" \n",
    "opt=\"ADA\"\n",
    "if(opt==\"SGD\"):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "elif(opt==\"ADA\"):\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-06, weight_decay=0.0001)\n",
    "elif(opt==\"ADAM\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "check_val_acc = 0\n",
    "losses = []\n",
    "per_epoch_train_loss =[]\n",
    "per_epoch_val_loss =[]\n",
    "per_epoch_train_f1 =[]\n",
    "per_epoch_val_f1 = []\n",
    "for epoch in range(epochs): \n",
    "    print('*************** Starting with epoch: ', epoch, '***********************')\n",
    "    for i in range (0,len(train_data)):\n",
    "        #model_des.zero_grad()\n",
    "        #model_loc.zero_grad()\n",
    "        model.zero_grad()\n",
    "        #####Run forward pass.\n",
    "      \n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i], training_data_twt[i], training_data_net[i])\n",
    "        \n",
    "        #print(\"prediction_joint :\", torch.argmax(prediction_joint, dim=1)) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        #Compute the loss, gradients, and update the parameters by\n",
    "        #calling optimizer.step()\n",
    "        loss = loss_function(prediction_joint, train_gt[i])\n",
    "        #if (i%200 == 0):\n",
    "            #print (\"loss per example\", loss.item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward(retain_graph=True)  #backpropagation\n",
    "        optimizer.step()\n",
    "    accuracy, macro_f1 = make_prediction_tr(model, training_data_des, training_data_loc, training_data_twt, training_data_net, train_gt)\n",
    "    print('epoch :', epoch, 'Train accuracy and macro_f1:', accuracy, macro_f1)\n",
    "    per_epoch_train_f1.append(macro_f1)\n",
    "    val_accuracy, val_macro_f1, val_loss = make_prediction_val(model, validation_data_des, validation_data_loc, validation_data_twt, validation_data_net, valid_gt)\n",
    "    per_epoch_val_f1.append(val_macro_f1)\n",
    "    print('epoch :', epoch, 'Validation accuracy, macro_f1:', val_accuracy, val_macro_f1)\n",
    "    per_epoch_train_loss.append(np.mean(losses))\n",
    "    print(\"train loss per epoch\", np.mean(losses))\n",
    "    per_epoch_val_loss.append(np.mean(val_loss))\n",
    "    print('Validation loss per epoch:', np.mean(val_loss))\n",
    "    torch.save(model.state_dict(),\"data/DLTN_umotivation_2layer/joint_DLTN_\"+str(epoch)+\".pt\")\n",
    "#     if (check_val_acc < val_macro_f1): #early stopping\n",
    "#         check_val_acc = val_macro_f1\n",
    "#         print (\"Model saved at epoch :\", epoch)\n",
    "#         torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "#         best_epoch = epoch\n",
    "        \n",
    "#print(\"Best model found at epoch : \", best_epoch)        \n",
    "#torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_plots(train_losses, val_losses, train_accs, test_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('loss_DLTN_umotivation_2layer.png')\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, test_accs, '-', linewidth=2, label='validation')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('accuracy_DLTN_umotivation_2layer.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1)\n",
    "# print(per_epoch_train_loss)\n",
    "# print(per_epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "# def make_prediction(training_data_des, training_data_loc, training_data_twt, training_data_net, ground_truths):\n",
    "#     model = JointDLTN()\n",
    "#     #model.load_state_dict(torch.load(\"data/YUN_utype/joint_DLTN_14.pt\")) #Testing accuracy, macro_f1: 0.7633587786259542 0.6908145811582292\n",
    "#     #model.load_state_dict(torch.load(\"data/YUN_utype/joint_DLTN_19.pt\")) #Testing accuracy, macro_f1: 0.767175572519084 0.714798206278027\n",
    "#     #model.load_state_dict(torch.load(\"data/YUN_utype/joint_DLTN_11.pt\")) #best #Testing accuracy, macro_f1: 0.7862595419847328 0.7560951140518181\n",
    "#     #model.load_state_dict(torch.load(\"data/YUN_utype_overfit/joint_DLTN_16.pt\")) #Testing accuracy, macro_f1: 0.7442748091603053 0.6970324361628709\n",
    "#     model.load_state_dict(torch.load(\"data/YUN_utype_overfit/joint_DLTN_23.pt\")) #Testing accuracy, macro_f1: 0.7213740458015268 0.6876262717476371\n",
    "#     predictions =[]\n",
    "#     for i in range (0,len(training_data_des)):\n",
    "#         prediction_joint = model(training_data_des[i], training_data_loc[i], training_data_twt[i], training_data_net[i])\n",
    "#         pred = torch.argmax(prediction_joint, dim=1)\n",
    "#         predictions.append(pred.item())\n",
    "#     #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "#     accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "#     macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "#     return accuracy, macro_f1\n",
    "\n",
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction(training_data_des, training_data_loc, training_data_twt, training_data_net, ground_truths):\n",
    "    for epoch in range(0,30):\n",
    "        model = JointDLTN()\n",
    "        model.load_state_dict(torch.load(\"data/DLTN_umotivation_2layer/joint_DLTN_\"+str(epoch)+\".pt\")) #best #Testing accuracy, macro_f1: 0.7862595419847328 0.7560951140518181\n",
    "        predictions =[]\n",
    "        for i in range (0,len(training_data_des)):\n",
    "            prediction_joint = model(training_data_des[i], training_data_loc[i], training_data_twt[i], training_data_net[i])\n",
    "            pred = torch.argmax(prediction_joint, dim=1)\n",
    "            predictions.append(pred.item())\n",
    "        #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "        accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "        macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "        print('epoch :', epoch, 'Testing accuracy, macro_f1:', accuracy, macro_f1)\n",
    "        #return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no location for user:  vaibhavatttc\n"
     ]
    }
   ],
   "source": [
    "#####prepare testing data for neural net #########\n",
    "testing_data_des, testing_data_loc, testing_data_twt =  nn_input(test_data,df)\n",
    "testing_data_net =  nn_input_network(test_data,df)\n",
    "test_gt = find_groundtruth(test_data, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########........Calculate Validation Accuracy.......\n",
    "# test_accuracy, test_macro_f1 = make_prediction(testing_data_des, testing_data_loc, testing_data_twt, testing_data_net, test_gt)\n",
    "# print('Testing accuracy, macro_f1:', test_accuracy, test_macro_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Testing accuracy, macro_f1: 0.7022900763358778 0.46729024276926\n",
      "epoch : 1 Testing accuracy, macro_f1: 0.7404580152671756 0.49798292522750726\n",
      "epoch : 2 Testing accuracy, macro_f1: 0.7480916030534351 0.5042443422505728\n",
      "epoch : 3 Testing accuracy, macro_f1: 0.767175572519084 0.519064465408805\n",
      "epoch : 4 Testing accuracy, macro_f1: 0.7938931297709924 0.5395955304000908\n",
      "epoch : 5 Testing accuracy, macro_f1: 0.8015267175572519 0.5454711930499228\n",
      "epoch : 6 Testing accuracy, macro_f1: 0.8053435114503816 0.5483870967741935\n",
      "epoch : 7 Testing accuracy, macro_f1: 0.8091603053435115 0.5512888631277271\n",
      "epoch : 8 Testing accuracy, macro_f1: 0.8129770992366412 0.5541768462560542\n",
      "epoch : 9 Testing accuracy, macro_f1: 0.8129770992366412 0.5541768462560542\n",
      "epoch : 10 Testing accuracy, macro_f1: 0.816793893129771 0.5566448801742919\n",
      "epoch : 11 Testing accuracy, macro_f1: 0.8129770992366412 0.5540717046514728\n",
      "epoch : 12 Testing accuracy, macro_f1: 0.816793893129771 0.5566448801742919\n",
      "epoch : 13 Testing accuracy, macro_f1: 0.8206106870229007 0.5595095295215248\n",
      "epoch : 14 Testing accuracy, macro_f1: 0.8244274809160306 0.5623616079032532\n",
      "epoch : 15 Testing accuracy, macro_f1: 0.8244274809160306 0.5626167430803192\n",
      "epoch : 16 Testing accuracy, macro_f1: 0.8320610687022901 0.6108452950558213\n",
      "epoch : 17 Testing accuracy, macro_f1: 0.8396946564885496 0.6164728168481469\n",
      "epoch : 18 Testing accuracy, macro_f1: 0.8435114503816794 0.6192702708425979\n",
      "epoch : 19 Testing accuracy, macro_f1: 0.8435114503816794 0.6192702708425979\n",
      "epoch : 20 Testing accuracy, macro_f1: 0.8396946564885496 0.6164728168481469\n",
      "epoch : 21 Testing accuracy, macro_f1: 0.8358778625954199 0.6138809478432119\n",
      "epoch : 22 Testing accuracy, macro_f1: 0.8396946564885496 0.6166751258300555\n",
      "epoch : 23 Testing accuracy, macro_f1: 0.8396946564885496 0.6166751258300555\n",
      "epoch : 24 Testing accuracy, macro_f1: 0.8396946564885496 0.6166751258300555\n",
      "epoch : 25 Testing accuracy, macro_f1: 0.8396946564885496 0.6166751258300555\n",
      "epoch : 26 Testing accuracy, macro_f1: 0.8358778625954199 0.6138809478432119\n",
      "epoch : 27 Testing accuracy, macro_f1: 0.8358778625954199 0.6138809478432119\n",
      "epoch : 28 Testing accuracy, macro_f1: 0.8358778625954199 0.6118594865239161\n",
      "epoch : 29 Testing accuracy, macro_f1: 0.8396946564885496 0.6146742570518242\n"
     ]
    }
   ],
   "source": [
    "make_prediction(testing_data_des, testing_data_loc, testing_data_twt, testing_data_net, test_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
