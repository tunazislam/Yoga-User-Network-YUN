{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11024a6f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## load embeddings #######\n",
    "#loc_emb = torch.load('data/locationEmbeddings.pt') \n",
    "des_emb = torch.load('data/descriptionEmbeddings.pt') \n",
    "#twt_emb = torch.load('data/tweetsEmbeddings.pt') \n",
    "\n",
    "#load network embedding\n",
    "#net_emb = gsm.KeyedVectors.load_word2vec_format('data/userNetworkEmd.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user = net_emb ['000mrs000']\n",
    "#print(user)\n",
    "#print(type(net_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 1300 user location, description, yoga tweets, utype, umotivation\n",
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "#print (df) #[1308 rows x 7 columns] name, location, description, text, utype, umotivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create BiLSTMAttention Model for Description\n",
    "class BiLSTMDesAtt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMDesAtt, self).__init__() \n",
    "        self.lstm = nn.LSTM(300, 150//2 , num_layers=1, bidirectional=True ) #BiLSTM with attention \n",
    "        #self.lstm = nn.LSTM(300, 150 , num_layers=1, bidirectional=False) #LSTM with attention\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "        self.attn_fc = torch.nn.Linear(300, 1) #attention layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)\n",
    "        return (torch.zeros(2 * 1, 1, 150//2), torch.zeros(2 * 1, 1, 150//2)) # <- change here: first dim of hidden needs to be doubled\n",
    "        #return (torch.zeros(1 * 1, 1, 150), torch.zeros(1 * 1, 1, 150))#LSTM with attention\n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size()) #torch.Size([13, 1, 150])\n",
    "        #rnn_out = rnn_out.squeeze(0).unsqueeze(1) \n",
    "        #rnn_out = rnn_out.permute(2,0,1) \n",
    "        rnn_out = rnn_out.permute(1,0,2) \n",
    "        #print(\"permute rnn_out\", rnn_out.size()) #torch.Size([150, 13, 1])\n",
    "        #print(\"state\", state.size()) #torch.Size([2, 1, 75])\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        #print(\"merged_state\", merged_state.size()) #torch.Size([1, 150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).size()) #torch.Size([150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).size()) #torch.Size([150, 1])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).unsqueeze(2).size()) # torch.Size([150, 1, 1])\n",
    "        #merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"merged_state2 :\", merged_state.size()) #torch.Size([150, 1, 1])\n",
    "        merged_state = merged_state.permute(1,0,2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([13, 300])\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        lstm_out, hidden = self.lstm(X.view(len(X),1, -1))\n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('hidden[0] = h_n', hidden[0], hidden[0].size()) # torch.Size([2, 1, 75])\n",
    "        #print('hidden[1] = c_n', hidden[1], hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        h_n, c_n = hidden\n",
    "        #print('h_n', h_n, h_n.size()) # torch.Size([2, 1, 75])\n",
    "        #print('c_n', c_n, hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        attn_out = self.attention(lstm_out, h_n)\n",
    "        #print(\"attn_out\", attn_out.size()) #torch.Size([150, 1])\n",
    "        #logits = self.fc2(attn_out)\n",
    "        #logits = self.fc2(attn_out.permute(1,0))\n",
    "        #print(\"logits\", logits, logits.size())\n",
    "        #return logits \n",
    "        return attn_out\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Des(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Des, self).__init__()\n",
    "        self.model_des = BiLSTMDesAtt()\n",
    "        #self.model_loc = LSTMLoc()\n",
    "        #self.model_net = NetworkMLP()\n",
    "        self.fc1 = nn.Linear(150, 200) \n",
    "        self.fc2 = nn.Linear(200, 3) #if we use two-layer classifier\n",
    "    def forward(self, x_d): \n",
    "        prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size()) #torch.Size([1, 3])\n",
    "        #prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size()) #torch.Size([1, 3])\n",
    "        #prediction_net = self.model_net(x_n)\n",
    "        #print(prediction_net, prediction_net.size()) #torch.Size([1, 3])\n",
    "        #concat_pred = torch.cat((prediction_des, prediction_loc, prediction_net), 1) #concat with dim= 1\n",
    "        #concat_pred = torch.cat((prediction_des, prediction_loc), 1) #concat with dim= 1\n",
    "        #print(concat_pred, concat_pred.size()) #torch.Size([1, 6])\n",
    "        out = self.fc1(prediction_des)\n",
    "        out = self.fc2(F.relu(out))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net input #########\n",
    "def nn_input(train_data,df):\n",
    "    #ground_truths = []\n",
    "    training_data_des =[]\n",
    "    #training_data_loc=[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                if (not des_emb[train_data[i]]):\n",
    "                    print ('no description for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    break\n",
    "                else:\n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    break\n",
    "    return training_data_des\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Ground Truth #########\n",
    "def find_groundtruth(data, df):\n",
    "    ground_truths = []\n",
    "    for i in range (0, len(data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (data[i] == df.name[j]):\n",
    "                #print(data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #target_type = torch.tensor(utype, dtype=torch.long) #for user type\n",
    "                target_type = torch.tensor(umotivation, dtype=torch.long) #for user motivation\n",
    "                ground_truths.append(target_type)\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_tr(model, training_data_des, ground_truths):\n",
    "    predictions =[]\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i])\n",
    "        \n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_val(model, training_data_des, ground_truths):\n",
    "    predictions =[]\n",
    "    val_losses = []\n",
    "    loss_function = nn.NLLLoss()\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i])\n",
    "        val_loss = loss_function(prediction_joint, ground_truths[i])\n",
    "        val_losses.append(val_loss.item())\n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    #print(type(predictions), type(ground_truths))\n",
    "    #print(\"predictions\", predictions)\n",
    "    #print(\"ground_truths\", ground_truths)\n",
    "    \n",
    "    return accuracy, macro_f1, val_losses\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no description for user:  bchi49\n",
      "no description for user:  viecestlavie\n",
      "no description for user:  crystalization_\n",
      "no description for user:  mimmosamami\n",
      "no description for user:  wenmarbyoga\n",
      "no description for user:  YogaLifeLine\n"
     ]
    }
   ],
   "source": [
    "##########......prepare training and validation data\n",
    "# ground truth training\n",
    "train_gt = find_groundtruth(train_data, df)\n",
    "#####prepare training data for neural net #########\n",
    "#training_data_net =  nn_input_network(train_data,df)\n",
    "#print(training_data_net, len(training_data_net)) #ok\n",
    "training_data_des =  nn_input(train_data,df)\n",
    "\n",
    "# ground truth validation\n",
    "valid_gt = find_groundtruth(valid_data, df)\n",
    "#####prepare validation data for neural net #########\n",
    "#validation_data_net =  nn_input_network(valid_data,df)\n",
    "validation_data_des =  nn_input(valid_data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Starting with epoch:  0 ***********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunaz/miniconda2/envs/py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Train accuracy and macro_f1: 0.6807228915662651 0.46674277705167916\n",
      "epoch : 0 Validation accuracy, macro_f1: 0.6650485436893204 0.44744744744744747\n",
      "train loss per epoch 0.9129786212401217\n",
      "Validation loss per epoch: 0.8234054493961982\n",
      "*************** Starting with epoch:  1 ***********************\n",
      "epoch : 1 Train accuracy and macro_f1: 0.6867469879518072 0.47186855022073865\n",
      "epoch : 1 Validation accuracy, macro_f1: 0.6990291262135923 0.4717312212241218\n",
      "train loss per epoch 0.8841650631413402\n",
      "Validation loss per epoch: 0.7978438089773493\n",
      "*************** Starting with epoch:  2 ***********************\n",
      "epoch : 2 Train accuracy and macro_f1: 0.7024096385542169 0.4825808533244553\n",
      "epoch : 2 Validation accuracy, macro_f1: 0.7038834951456311 0.47492468545100125\n",
      "train loss per epoch 0.8648296865713165\n",
      "Validation loss per epoch: 0.7805995799384071\n",
      "*************** Starting with epoch:  3 ***********************\n",
      "epoch : 3 Train accuracy and macro_f1: 0.7144578313253012 0.49077413428520344\n",
      "epoch : 3 Validation accuracy, macro_f1: 0.7038834951456311 0.47492468545100125\n",
      "train loss per epoch 0.8491105166185333\n",
      "Validation loss per epoch: 0.7665656527269233\n",
      "*************** Starting with epoch:  4 ***********************\n",
      "epoch : 4 Train accuracy and macro_f1: 0.7228915662650602 0.49657999784226997\n",
      "epoch : 4 Validation accuracy, macro_f1: 0.7038834951456311 0.4749818709209572\n",
      "train loss per epoch 0.8354197946154928\n",
      "Validation loss per epoch: 0.7566097335329334\n",
      "*************** Starting with epoch:  5 ***********************\n",
      "epoch : 5 Train accuracy and macro_f1: 0.7265060240963855 0.4990012145724026\n",
      "epoch : 5 Validation accuracy, macro_f1: 0.7135922330097088 0.4815906455353692\n",
      "train loss per epoch 0.8230914682269576\n",
      "Validation loss per epoch: 0.7487152224605523\n",
      "*************** Starting with epoch:  6 ***********************\n",
      "epoch : 6 Train accuracy and macro_f1: 0.7253012048192771 0.4982203699822037\n",
      "epoch : 6 Validation accuracy, macro_f1: 0.7135922330097088 0.4815906455353692\n",
      "train loss per epoch 0.8117038475615638\n",
      "Validation loss per epoch: 0.7435728783167682\n",
      "*************** Starting with epoch:  7 ***********************\n",
      "epoch : 7 Train accuracy and macro_f1: 0.7313253012048193 0.5023673454289889\n",
      "epoch : 7 Validation accuracy, macro_f1: 0.6990291262135923 0.47166070219557016\n",
      "train loss per epoch 0.8010065460644932\n",
      "Validation loss per epoch: 0.7400974378134441\n",
      "*************** Starting with epoch:  8 ***********************\n",
      "epoch : 8 Train accuracy and macro_f1: 0.7385542168674699 0.5201581834417656\n",
      "epoch : 8 Validation accuracy, macro_f1: 0.7087378640776699 0.4782916888180046\n",
      "train loss per epoch 0.7908762560232256\n",
      "Validation loss per epoch: 0.7380160408691295\n",
      "*************** Starting with epoch:  9 ***********************\n",
      "epoch : 9 Train accuracy and macro_f1: 0.7385542168674699 0.5201581834417656\n",
      "epoch : 9 Validation accuracy, macro_f1: 0.7184466019417476 0.48484848484848486\n",
      "train loss per epoch 0.7812109348931945\n",
      "Validation loss per epoch: 0.7371949050032977\n",
      "*************** Starting with epoch:  10 ***********************\n",
      "epoch : 10 Train accuracy and macro_f1: 0.7409638554216867 0.5341107097754962\n",
      "epoch : 10 Validation accuracy, macro_f1: 0.7184466019417476 0.48484848484848486\n",
      "train loss per epoch 0.7719586567836941\n",
      "Validation loss per epoch: 0.7366216628296861\n",
      "*************** Starting with epoch:  11 ***********************\n",
      "epoch : 11 Train accuracy and macro_f1: 0.7469879518072289 0.5500701620104606\n",
      "epoch : 11 Validation accuracy, macro_f1: 0.7184466019417476 0.48484848484848486\n",
      "train loss per epoch 0.763091206547606\n",
      "Validation loss per epoch: 0.73672437798051\n",
      "*************** Starting with epoch:  12 ***********************\n",
      "epoch : 12 Train accuracy and macro_f1: 0.755421686746988 0.5672041804026092\n",
      "epoch : 12 Validation accuracy, macro_f1: 0.7184466019417476 0.48484848484848486\n",
      "train loss per epoch 0.7545828972940538\n",
      "Validation loss per epoch: 0.7380249116895268\n",
      "*************** Starting with epoch:  13 ***********************\n",
      "epoch : 13 Train accuracy and macro_f1: 0.7590361445783133 0.5805998261514994\n",
      "epoch : 13 Validation accuracy, macro_f1: 0.7184466019417476 0.48484848484848486\n",
      "train loss per epoch 0.7464033949613674\n",
      "Validation loss per epoch: 0.7395144540418699\n",
      "*************** Starting with epoch:  14 ***********************\n",
      "epoch : 14 Train accuracy and macro_f1: 0.7674698795180723 0.5864241184398503\n",
      "epoch : 14 Validation accuracy, macro_f1: 0.7184466019417476 0.48484848484848486\n",
      "train loss per epoch 0.7385305922134334\n",
      "Validation loss per epoch: 0.7417712811272121\n",
      "*************** Starting with epoch:  15 ***********************\n",
      "epoch : 15 Train accuracy and macro_f1: 0.7771084337349398 0.6327853498437814\n",
      "epoch : 15 Validation accuracy, macro_f1: 0.7135922330097088 0.4815906455353692\n",
      "train loss per epoch 0.7309462852742389\n",
      "Validation loss per epoch: 0.7441424899598927\n",
      "*************** Starting with epoch:  16 ***********************\n",
      "epoch : 16 Train accuracy and macro_f1: 0.7807228915662651 0.6530536453724436\n",
      "epoch : 16 Validation accuracy, macro_f1: 0.7038834951456311 0.4750620627397879\n",
      "train loss per epoch 0.7236263337317381\n",
      "Validation loss per epoch: 0.7472277722023066\n",
      "*************** Starting with epoch:  17 ***********************\n",
      "epoch : 17 Train accuracy and macro_f1: 0.7867469879518072 0.6719042963620199\n",
      "epoch : 17 Validation accuracy, macro_f1: 0.7038834951456311 0.4750620627397879\n",
      "train loss per epoch 0.7165528915214092\n",
      "Validation loss per epoch: 0.7507361844182014\n",
      "*************** Starting with epoch:  18 ***********************\n",
      "epoch : 18 Train accuracy and macro_f1: 0.791566265060241 0.683014058684506\n",
      "epoch : 18 Validation accuracy, macro_f1: 0.6990291262135923 0.4728642605419857\n",
      "train loss per epoch 0.7097066353956921\n",
      "Validation loss per epoch: 0.7551008601790493\n",
      "*************** Starting with epoch:  19 ***********************\n",
      "epoch : 19 Train accuracy and macro_f1: 0.7939759036144578 0.6846849260751106\n",
      "epoch : 19 Validation accuracy, macro_f1: 0.6941747572815534 0.470643800589567\n",
      "train loss per epoch 0.7030792409394789\n",
      "Validation loss per epoch: 0.7596941188122462\n",
      "*************** Starting with epoch:  20 ***********************\n",
      "epoch : 20 Train accuracy and macro_f1: 0.7951807228915663 0.6982341910369779\n",
      "epoch : 20 Validation accuracy, macro_f1: 0.6990291262135923 0.47399267399267403\n",
      "train loss per epoch 0.6966602963808447\n",
      "Validation loss per epoch: 0.7649772588199782\n",
      "*************** Starting with epoch:  21 ***********************\n",
      "epoch : 21 Train accuracy and macro_f1: 0.7987951807228916 0.6987433324863309\n",
      "epoch : 21 Validation accuracy, macro_f1: 0.6893203883495146 0.46739926739926735\n",
      "train loss per epoch 0.6904358278569304\n",
      "Validation loss per epoch: 0.7701566180630216\n",
      "*************** Starting with epoch:  22 ***********************\n",
      "epoch : 22 Train accuracy and macro_f1: 0.8048192771084337 0.7165053964074882\n",
      "epoch : 22 Validation accuracy, macro_f1: 0.6893203883495146 0.46739926739926735\n",
      "train loss per epoch 0.6843923771967589\n",
      "Validation loss per epoch: 0.7758738563658254\n",
      "*************** Starting with epoch:  23 ***********************\n",
      "epoch : 23 Train accuracy and macro_f1: 0.8048192771084337 0.7144384810364593\n",
      "epoch : 23 Validation accuracy, macro_f1: 0.6844660194174758 0.46516118474881357\n",
      "train loss per epoch 0.6785272921227951\n",
      "Validation loss per epoch: 0.78151185852159\n",
      "*************** Starting with epoch:  24 ***********************\n",
      "epoch : 24 Train accuracy and macro_f1: 0.8036144578313253 0.7136000583135483\n",
      "epoch : 24 Validation accuracy, macro_f1: 0.6844660194174758 0.46516118474881357\n",
      "train loss per epoch 0.672833634254085\n",
      "Validation loss per epoch: 0.7878905102899931\n",
      "*************** Starting with epoch:  25 ***********************\n",
      "epoch : 25 Train accuracy and macro_f1: 0.8036144578313253 0.7115989847715736\n",
      "epoch : 25 Validation accuracy, macro_f1: 0.6844660194174758 0.4662852069254673\n",
      "train loss per epoch 0.6673073360627384\n",
      "Validation loss per epoch: 0.7948467900741447\n",
      "*************** Starting with epoch:  26 ***********************\n",
      "epoch : 26 Train accuracy and macro_f1: 0.8048192771084337 0.7124376002322294\n",
      "epoch : 26 Validation accuracy, macro_f1: 0.6893203883495146 0.46952874414014306\n",
      "train loss per epoch 0.6619366469316348\n",
      "Validation loss per epoch: 0.8022807872338781\n",
      "*************** Starting with epoch:  27 ***********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 27 Train accuracy and macro_f1: 0.8096385542168675 0.7157885386528603\n",
      "epoch : 27 Validation accuracy, macro_f1: 0.6893203883495146 0.46952874414014306\n",
      "train loss per epoch 0.6567129406028888\n",
      "Validation loss per epoch: 0.8094533110236369\n",
      "*************** Starting with epoch:  28 ***********************\n",
      "epoch : 28 Train accuracy and macro_f1: 0.810843373493976 0.7166266149563395\n",
      "epoch : 28 Validation accuracy, macro_f1: 0.6844660194174758 0.4662852069254673\n",
      "train loss per epoch 0.6516260987454131\n",
      "Validation loss per epoch: 0.8167521754749915\n",
      "*************** Starting with epoch:  29 ***********************\n",
      "epoch : 29 Train accuracy and macro_f1: 0.8144578313253013 0.7275651380043585\n",
      "epoch : 29 Validation accuracy, macro_f1: 0.6796116504854369 0.463034188034188\n",
      "train loss per epoch 0.6466728918115986\n",
      "Validation loss per epoch: 0.8245470447520029\n"
     ]
    }
   ],
   "source": [
    "###########.........Start Training...........\n",
    "model = Des()\n",
    "##### Hyperparameter\n",
    "#learning_rate=0.05\n",
    "learning_rate=0.01\n",
    "epochs = 30\n",
    "#opt=\"ADAM\"\n",
    "#opt=\"SGD\" \n",
    "opt=\"ADA\"\n",
    "if(opt==\"SGD\"):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "elif(opt==\"ADA\"):\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-06, weight_decay=0.0001)\n",
    "elif(opt==\"ADAM\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "check_val_acc = 0\n",
    "losses = []\n",
    "per_epoch_train_loss =[]\n",
    "per_epoch_val_loss =[]\n",
    "per_epoch_train_f1 =[]\n",
    "per_epoch_val_f1 = []\n",
    "for epoch in range(epochs): \n",
    "    print('*************** Starting with epoch: ', epoch, '***********************')\n",
    "    for i in range (0,len(train_data)):\n",
    "        #model_des.zero_grad()\n",
    "        #model_loc.zero_grad()\n",
    "        model.zero_grad()\n",
    "        #####Run forward pass.\n",
    "      \n",
    "        prediction_joint = model(training_data_des[i])\n",
    "        \n",
    "        #print(\"prediction_joint :\", torch.argmax(prediction_joint, dim=1)) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        #Compute the loss, gradients, and update the parameters by\n",
    "        #calling optimizer.step()\n",
    "        loss = loss_function(prediction_joint, train_gt[i])\n",
    "        #if (i%200 == 0):\n",
    "            #print (\"loss per example\", loss.item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward(retain_graph=True)  #backpropagation\n",
    "        optimizer.step()\n",
    "    accuracy, macro_f1 = make_prediction_tr(model, training_data_des, train_gt)\n",
    "    print('epoch :', epoch, 'Train accuracy and macro_f1:', accuracy, macro_f1)\n",
    "    per_epoch_train_f1.append(macro_f1)\n",
    "    val_accuracy, val_macro_f1, val_loss = make_prediction_val(model, validation_data_des, valid_gt)\n",
    "    per_epoch_val_f1.append(val_macro_f1)\n",
    "    print('epoch :', epoch, 'Validation accuracy, macro_f1:', val_accuracy, val_macro_f1)\n",
    "    per_epoch_train_loss.append(np.mean(losses))\n",
    "    print(\"train loss per epoch\", np.mean(losses))\n",
    "    per_epoch_val_loss.append(np.mean(val_loss))\n",
    "    print('Validation loss per epoch:', np.mean(val_loss))\n",
    "    \n",
    "    torch.save(model.state_dict(),\"data/Des_umotivation_2layer/Des_\"+str(epoch)+\".pt\")\n",
    "#     if (check_val_acc < val_macro_f1): #early stopping\n",
    "#         check_val_acc = val_macro_f1\n",
    "#         print (\"Model saved at epoch :\", epoch)\n",
    "#         torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "#         best_epoch = epoch\n",
    "        \n",
    "#print(\"Best model found at epoch : \", best_epoch)        \n",
    "#torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_plots(train_losses, val_losses, train_accs, test_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('loss_Des_umotivation.png')\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, test_accs, '-', linewidth=2, label='validation')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('accuracy_Des_umotivation.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1)\n",
    "# print(per_epoch_train_loss)\n",
    "# print(per_epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction(training_data_des, ground_truths):\n",
    "    for epoch in range(0,30):\n",
    "        model = Des()\n",
    "        model.load_state_dict(torch.load(\"data/Des_umotivation_2layer/Des_\"+str(epoch)+\".pt\")) \n",
    "        predictions =[]\n",
    "        for i in range (0,len(training_data_des)):\n",
    "            prediction_joint = model(training_data_des[i])\n",
    "            pred = torch.argmax(prediction_joint, dim=1)\n",
    "            predictions.append(pred.item())\n",
    "        #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "        accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "        macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "        print('epoch :', epoch, 'Testing accuracy, macro_f1:', accuracy, macro_f1)\n",
    "        #return accuracy, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare testing data for neural net #########\n",
    "testing_data_des =  nn_input(test_data,df)\n",
    "#testing_data_net =  nn_input_network(test_data,df)\n",
    "test_gt = find_groundtruth(test_data, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Testing accuracy, macro_f1: 0.7099236641221374 0.4843924191750279\n",
      "epoch : 1 Testing accuracy, macro_f1: 0.7366412213740458 0.5014629049111807\n",
      "epoch : 2 Testing accuracy, macro_f1: 0.7557251908396947 0.5137393947068508\n",
      "epoch : 3 Testing accuracy, macro_f1: 0.7557251908396947 0.5133333333333333\n",
      "epoch : 4 Testing accuracy, macro_f1: 0.7557251908396947 0.5133333333333333\n",
      "epoch : 5 Testing accuracy, macro_f1: 0.7595419847328244 0.5158244448330127\n",
      "epoch : 6 Testing accuracy, macro_f1: 0.7595419847328244 0.5167993980723479\n",
      "epoch : 7 Testing accuracy, macro_f1: 0.7595419847328244 0.5167993980723479\n",
      "epoch : 8 Testing accuracy, macro_f1: 0.7595419847328244 0.5165079365079365\n",
      "epoch : 9 Testing accuracy, macro_f1: 0.7595419847328244 0.5164139602366034\n",
      "epoch : 10 Testing accuracy, macro_f1: 0.7595419847328244 0.5164139602366034\n",
      "epoch : 11 Testing accuracy, macro_f1: 0.7633587786259542 0.5617472352859626\n",
      "epoch : 12 Testing accuracy, macro_f1: 0.767175572519084 0.5646548541897379\n",
      "epoch : 13 Testing accuracy, macro_f1: 0.7748091603053435 0.5696989116387368\n",
      "epoch : 14 Testing accuracy, macro_f1: 0.7824427480916031 0.5751353776154314\n",
      "epoch : 15 Testing accuracy, macro_f1: 0.7786259541984732 0.5700320409781607\n",
      "epoch : 16 Testing accuracy, macro_f1: 0.7748091603053435 0.5675144847288426\n",
      "epoch : 17 Testing accuracy, macro_f1: 0.7786259541984732 0.5700320409781607\n",
      "epoch : 18 Testing accuracy, macro_f1: 0.7748091603053435 0.5675144847288426\n",
      "epoch : 19 Testing accuracy, macro_f1: 0.7748091603053435 0.5679077787201358\n",
      "epoch : 20 Testing accuracy, macro_f1: 0.7633587786259542 0.5581837064273641\n",
      "epoch : 21 Testing accuracy, macro_f1: 0.7633587786259542 0.5584396721651624\n",
      "epoch : 22 Testing accuracy, macro_f1: 0.7633587786259542 0.5584396721651624\n",
      "epoch : 23 Testing accuracy, macro_f1: 0.7519083969465649 0.5509208378186955\n",
      "epoch : 24 Testing accuracy, macro_f1: 0.7480916030534351 0.5484179445756805\n",
      "epoch : 25 Testing accuracy, macro_f1: 0.7404580152671756 0.5413368701778406\n",
      "epoch : 26 Testing accuracy, macro_f1: 0.7404580152671756 0.5413368701778406\n",
      "epoch : 27 Testing accuracy, macro_f1: 0.7404580152671756 0.5413368701778406\n",
      "epoch : 28 Testing accuracy, macro_f1: 0.7404580152671756 0.5413368701778406\n",
      "epoch : 29 Testing accuracy, macro_f1: 0.7366412213740458 0.5388450027684754\n"
     ]
    }
   ],
   "source": [
    "make_prediction(testing_data_des, test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
