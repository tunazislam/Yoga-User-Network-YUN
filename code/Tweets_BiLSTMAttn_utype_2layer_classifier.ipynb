{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1141d5b10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## load embeddings #######\n",
    "#loc_emb = torch.load('data/locationEmbeddings.pt') \n",
    "#des_emb = torch.load('data/descriptionEmbeddings.pt') \n",
    "twt_emb = torch.load('data/tweetsEmbeddings.pt') \n",
    "\n",
    "#load network embedding\n",
    "#net_emb = gsm.KeyedVectors.load_word2vec_format('data/userNetworkEmd.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user = net_emb ['000mrs000']\n",
    "#print(user)\n",
    "#print(type(net_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 1300 user location, description, yoga tweets, utype, umotivation\n",
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "#print (df) #[1308 rows x 7 columns] name, location, description, text, utype, umotivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create BiLSTMAttention Model for Description\n",
    "class BiLSTMTwtAtt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMTwtAtt, self).__init__() \n",
    "        self.lstm = nn.LSTM(300, 150//2 , num_layers=1, bidirectional=True ) #BiLSTM with attention \n",
    "        #self.lstm = nn.LSTM(300, 150 , num_layers=1, bidirectional=False) #LSTM with attention\n",
    "        #self.fc2 = nn.Linear(150, 50)\n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "        self.attn_fc = torch.nn.Linear(300, 1) #attention layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)\n",
    "        return (torch.zeros(2 * 1, 1, 150//2), torch.zeros(2 * 1, 1, 150//2)) # <- change here: first dim of hidden needs to be doubled\n",
    "        #return (torch.zeros(1 * 1, 1, 150), torch.zeros(1 * 1, 1, 150))#LSTM with attention\n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size()) #torch.Size([13, 1, 150])\n",
    "        #rnn_out = rnn_out.squeeze(0).unsqueeze(1) \n",
    "        #rnn_out = rnn_out.permute(2,0,1) \n",
    "        rnn_out = rnn_out.permute(1,0,2) \n",
    "        #print(\"permute rnn_out\", rnn_out.size()) #torch.Size([150, 13, 1])\n",
    "        #print(\"state\", state.size()) #torch.Size([2, 1, 75])\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        #print(\"merged_state\", merged_state.size()) #torch.Size([1, 150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).size()) #torch.Size([150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).size()) #torch.Size([150, 1])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).unsqueeze(2).size()) # torch.Size([150, 1, 1])\n",
    "        #merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"merged_state2 :\", merged_state.size()) #torch.Size([150, 1, 1])\n",
    "        merged_state = merged_state.permute(1,0,2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([13, 300])\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        lstm_out, hidden = self.lstm(X.view(len(X),1, -1))\n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('hidden[0] = h_n', hidden[0], hidden[0].size()) # torch.Size([2, 1, 75])\n",
    "        #print('hidden[1] = c_n', hidden[1], hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        h_n, c_n = hidden\n",
    "        #print('h_n', h_n, h_n.size()) # torch.Size([2, 1, 75])\n",
    "        #print('c_n', c_n, hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        attn_out = self.attention(lstm_out, h_n)\n",
    "        #print(\"attn_out\", attn_out.size()) #torch.Size([150, 1])\n",
    "        #logits = self.fc2(attn_out)\n",
    "        #logits = self.fc2(attn_out.permute(1,0))\n",
    "        #print(\"logits\", logits, logits.size())\n",
    "        #return logits \n",
    "        return attn_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Twt, self).__init__()\n",
    "        #self.model_des = BiLSTMDesAtt()\n",
    "        #self.model_loc = LSTMLoc()\n",
    "        #self.model_twt = BiLSTMDesAtt()\n",
    "        self.model_twt = BiLSTMTwtAtt()\n",
    "        #self.model_net = NetworkMLP()\n",
    "\n",
    "        self.fc1 = nn.Linear(150, 200) \n",
    "        self.fc2 = nn.Linear(200, 3)\n",
    "    def forward(self, x_t): \n",
    "        #prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size())\n",
    "        #prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size())\n",
    "        prediction_twt = self.model_twt(x_t)\n",
    "        #prediction_net = self.model_net(x_n)\n",
    "        #concat_pred = torch.cat((prediction_des, prediction_loc, prediction_twt), 1) #concat with dim= 1\n",
    "        #print(concat_pred, concat_pred.size()) \n",
    "#         out = F.log_softmax(self.fc(concat_pred), dim = 1)\n",
    "#         return out\n",
    "        out = self.fc1(prediction_twt)\n",
    "        out = self.fc2(F.relu(out))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net input #########\n",
    "def nn_input(train_data,df):\n",
    "    #ground_truths = []\n",
    "    training_data_twt =[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                sent_tensor_twt = torch.stack(twt_emb[train_data[i]],dim = 1)\n",
    "                training_data_twt.append(sent_tensor_twt[-1])\n",
    "                break\n",
    "    return training_data_twt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Ground Truth #########\n",
    "def find_groundtruth(data, df):\n",
    "    ground_truths = []\n",
    "    for i in range (0, len(data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (data[i] == df.name[j]):\n",
    "                #print(data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                target_type = torch.tensor(utype, dtype=torch.long) #for user type\n",
    "                #target_type = torch.tensor(umotivation, dtype=torch.long) #for user motivation\n",
    "                ground_truths.append(target_type)\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_tr(model, training_data_twt, ground_truths):\n",
    "    predictions =[]\n",
    "    for i in range (0,len(training_data_twt)):\n",
    "        prediction_joint = model(training_data_twt[i])\n",
    "        \n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_twt)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_val(model, training_data_twt, ground_truths):\n",
    "    predictions =[]\n",
    "    val_losses = []\n",
    "    loss_function = nn.NLLLoss()\n",
    "    for i in range (0,len(training_data_twt)):\n",
    "        prediction_joint = model(training_data_twt[i])\n",
    "        val_loss = loss_function(prediction_joint, ground_truths[i])\n",
    "        val_losses.append(val_loss.item())\n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_twt)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    return accuracy, macro_f1, val_losses\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########......prepare training and validation data\n",
    "# ground truth training\n",
    "train_gt = find_groundtruth(train_data, df)\n",
    "#####prepare training data for neural net #########\n",
    "#training_data_net =  nn_input_network(train_data,df)\n",
    "#print(training_data_net, len(training_data_net)) #ok\n",
    "training_data_twt =  nn_input(train_data,df)\n",
    "\n",
    "# ground truth validation\n",
    "valid_gt = find_groundtruth(valid_data, df)\n",
    "#####prepare validation data for neural net #########\n",
    "#validation_data_net =  nn_input_network(valid_data,df)\n",
    "validation_data_twt =  nn_input(valid_data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Starting with epoch:  0 ***********************\n",
      "epoch : 0 Train accuracy and macro_f1: 0.5518072289156627 0.5333753680723858\n",
      "epoch : 0 Validation accuracy, macro_f1: 0.529126213592233 0.5090190040364292\n",
      "train loss per epoch 1.3332337226494249\n",
      "Validation loss per epoch: 1.248619140930546\n",
      "*************** Starting with epoch:  1 ***********************\n",
      "epoch : 1 Train accuracy and macro_f1: 0.6156626506024097 0.6042531201158061\n",
      "epoch : 1 Validation accuracy, macro_f1: 0.5728155339805825 0.5520818419369145\n",
      "train loss per epoch 1.175221289245479\n",
      "Validation loss per epoch: 1.1371174061182634\n",
      "*************** Starting with epoch:  2 ***********************\n",
      "epoch : 2 Train accuracy and macro_f1: 0.6433734939759036 0.6340185907807494\n",
      "epoch : 2 Validation accuracy, macro_f1: 0.587378640776699 0.5723984236816452\n",
      "train loss per epoch 1.088931968389266\n",
      "Validation loss per epoch: 1.0890809757616913\n",
      "*************** Starting with epoch:  3 ***********************\n",
      "epoch : 3 Train accuracy and macro_f1: 0.6518072289156627 0.64282771388382\n",
      "epoch : 3 Validation accuracy, macro_f1: 0.6116504854368932 0.5945550745550746\n",
      "train loss per epoch 1.0306819833186736\n",
      "Validation loss per epoch: 1.067914176913141\n",
      "*************** Starting with epoch:  4 ***********************\n",
      "epoch : 4 Train accuracy and macro_f1: 0.6783132530120481 0.6698901238692234\n",
      "epoch : 4 Validation accuracy, macro_f1: 0.6262135922330098 0.6086399679199027\n",
      "train loss per epoch 0.9878053033782775\n",
      "Validation loss per epoch: 1.0449136345131884\n",
      "*************** Starting with epoch:  5 ***********************\n",
      "epoch : 5 Train accuracy and macro_f1: 0.6975903614457831 0.6902329412066875\n",
      "epoch : 5 Validation accuracy, macro_f1: 0.6262135922330098 0.6069612675256577\n",
      "train loss per epoch 0.9531854132571852\n",
      "Validation loss per epoch: 1.026652775343182\n",
      "*************** Starting with epoch:  6 ***********************\n",
      "epoch : 6 Train accuracy and macro_f1: 0.7108433734939759 0.7026341408441823\n",
      "epoch : 6 Validation accuracy, macro_f1: 0.6359223300970874 0.6156679860485167\n",
      "train loss per epoch 0.9250648058322539\n",
      "Validation loss per epoch: 1.0275560281809093\n",
      "*************** Starting with epoch:  7 ***********************\n",
      "epoch : 7 Train accuracy and macro_f1: 0.7156626506024096 0.7079432501155803\n",
      "epoch : 7 Validation accuracy, macro_f1: 0.6407766990291263 0.621810121857898\n",
      "train loss per epoch 0.9011298691203077\n",
      "Validation loss per epoch: 1.018205400809501\n",
      "*************** Starting with epoch:  8 ***********************\n",
      "epoch : 8 Train accuracy and macro_f1: 0.7301204819277108 0.7238058479111146\n",
      "epoch : 8 Validation accuracy, macro_f1: 0.6359223300970874 0.6176761589132682\n",
      "train loss per epoch 0.880151582506287\n",
      "Validation loss per epoch: 1.0207664549929425\n",
      "*************** Starting with epoch:  9 ***********************\n",
      "epoch : 9 Train accuracy and macro_f1: 0.7409638554216867 0.7349344045955597\n",
      "epoch : 9 Validation accuracy, macro_f1: 0.6407766990291263 0.6225820962663068\n",
      "train loss per epoch 0.86147999288806\n",
      "Validation loss per epoch: 1.0302235912350775\n",
      "*************** Starting with epoch:  10 ***********************\n",
      "epoch : 10 Train accuracy and macro_f1: 0.744578313253012 0.7386599747323045\n",
      "epoch : 10 Validation accuracy, macro_f1: 0.6359223300970874 0.6170777988614801\n",
      "train loss per epoch 0.8448139778168021\n",
      "Validation loss per epoch: 1.0323145059705938\n",
      "*************** Starting with epoch:  11 ***********************\n",
      "epoch : 11 Train accuracy and macro_f1: 0.7518072289156627 0.7462736134716442\n",
      "epoch : 11 Validation accuracy, macro_f1: 0.6359223300970874 0.6170777988614801\n",
      "train loss per epoch 0.8299011257099339\n",
      "Validation loss per epoch: 1.0342671541334356\n",
      "*************** Starting with epoch:  12 ***********************\n",
      "epoch : 12 Train accuracy and macro_f1: 0.7518072289156627 0.7465540882258866\n",
      "epoch : 12 Validation accuracy, macro_f1: 0.6359223300970874 0.6177162093781169\n",
      "train loss per epoch 0.8163291527619507\n",
      "Validation loss per epoch: 1.0490926266873923\n",
      "*************** Starting with epoch:  13 ***********************\n",
      "epoch : 13 Train accuracy and macro_f1: 0.755421686746988 0.7501517911353975\n",
      "epoch : 13 Validation accuracy, macro_f1: 0.6310679611650486 0.612510592028287\n",
      "train loss per epoch 0.8037958534931356\n",
      "Validation loss per epoch: 1.051353070921111\n",
      "*************** Starting with epoch:  14 ***********************\n",
      "epoch : 14 Train accuracy and macro_f1: 0.7542168674698795 0.7488057629963342\n",
      "epoch : 14 Validation accuracy, macro_f1: 0.6407766990291263 0.6239010795936792\n",
      "train loss per epoch 0.7922533623616859\n",
      "Validation loss per epoch: 1.0593470109319223\n",
      "*************** Starting with epoch:  15 ***********************\n",
      "epoch : 15 Train accuracy and macro_f1: 0.7602409638554217 0.7549508083121711\n",
      "epoch : 15 Validation accuracy, macro_f1: 0.6407766990291263 0.6239010795936792\n",
      "train loss per epoch 0.781496008445161\n",
      "Validation loss per epoch: 1.0660644254638154\n",
      "*************** Starting with epoch:  16 ***********************\n",
      "epoch : 16 Train accuracy and macro_f1: 0.7662650602409639 0.7612309955351956\n",
      "epoch : 16 Validation accuracy, macro_f1: 0.6359223300970874 0.6168395963572914\n",
      "train loss per epoch 0.7714766025965641\n",
      "Validation loss per epoch: 1.0769439339637756\n",
      "*************** Starting with epoch:  17 ***********************\n",
      "epoch : 17 Train accuracy and macro_f1: 0.7662650602409639 0.7613993719427904\n",
      "epoch : 17 Validation accuracy, macro_f1: 0.6359223300970874 0.6166807049159991\n",
      "train loss per epoch 0.7620547777677635\n",
      "Validation loss per epoch: 1.0923974855432232\n",
      "*************** Starting with epoch:  18 ***********************\n",
      "epoch : 18 Train accuracy and macro_f1: 0.7662650602409639 0.7612390381494905\n",
      "epoch : 18 Validation accuracy, macro_f1: 0.6407766990291263 0.6208196797841748\n",
      "train loss per epoch 0.7531853668018467\n",
      "Validation loss per epoch: 1.102156666297357\n",
      "*************** Starting with epoch:  19 ***********************\n",
      "epoch : 19 Train accuracy and macro_f1: 0.7686746987951807 0.763887991026996\n",
      "epoch : 19 Validation accuracy, macro_f1: 0.6456310679611651 0.6246743562417796\n",
      "train loss per epoch 0.7447657876237329\n",
      "Validation loss per epoch: 1.1075856083805122\n",
      "*************** Starting with epoch:  20 ***********************\n",
      "epoch : 20 Train accuracy and macro_f1: 0.7698795180722892 0.7657383530245393\n",
      "epoch : 20 Validation accuracy, macro_f1: 0.6456310679611651 0.6244399158730781\n",
      "train loss per epoch 0.7368283514756275\n",
      "Validation loss per epoch: 1.1211588168607174\n",
      "*************** Starting with epoch:  21 ***********************\n",
      "epoch : 21 Train accuracy and macro_f1: 0.7783132530120482 0.7742327779748112\n",
      "epoch : 21 Validation accuracy, macro_f1: 0.6456310679611651 0.6246101153570339\n",
      "train loss per epoch 0.729198307157999\n",
      "Validation loss per epoch: 1.125333380930632\n",
      "*************** Starting with epoch:  22 ***********************\n",
      "epoch : 22 Train accuracy and macro_f1: 0.7783132530120482 0.7744378137728521\n",
      "epoch : 22 Validation accuracy, macro_f1: 0.6456310679611651 0.6242530983616387\n",
      "train loss per epoch 0.7219160143307456\n",
      "Validation loss per epoch: 1.1297475203727056\n",
      "*************** Starting with epoch:  23 ***********************\n",
      "epoch : 23 Train accuracy and macro_f1: 0.7843373493975904 0.7804303522879427\n",
      "epoch : 23 Validation accuracy, macro_f1: 0.6456310679611651 0.6236836213388696\n",
      "train loss per epoch 0.7148960864306034\n",
      "Validation loss per epoch: 1.137527209607143\n",
      "*************** Starting with epoch:  24 ***********************\n",
      "epoch : 24 Train accuracy and macro_f1: 0.7879518072289157 0.7836053100792455\n",
      "epoch : 24 Validation accuracy, macro_f1: 0.6553398058252428 0.6329000079749783\n",
      "train loss per epoch 0.7081744192145675\n",
      "Validation loss per epoch: 1.1368620256052433\n",
      "*************** Starting with epoch:  25 ***********************\n",
      "epoch : 25 Train accuracy and macro_f1: 0.791566265060241 0.787278475029403\n",
      "epoch : 25 Validation accuracy, macro_f1: 0.6504854368932039 0.6282970683379354\n",
      "train loss per epoch 0.7016710364797513\n",
      "Validation loss per epoch: 1.146236710431217\n",
      "*************** Starting with epoch:  26 ***********************\n",
      "epoch : 26 Train accuracy and macro_f1: 0.7963855421686747 0.7922329986318687\n",
      "epoch : 26 Validation accuracy, macro_f1: 0.6504854368932039 0.6281730159770944\n",
      "train loss per epoch 0.6953933213323368\n",
      "Validation loss per epoch: 1.1532502967177085\n",
      "*************** Starting with epoch:  27 ***********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 27 Train accuracy and macro_f1: 0.7903614457831325 0.7859470479575578\n",
      "epoch : 27 Validation accuracy, macro_f1: 0.6504854368932039 0.6281730159770944\n",
      "train loss per epoch 0.6892884625267821\n",
      "Validation loss per epoch: 1.162752134056346\n",
      "*************** Starting with epoch:  28 ***********************\n",
      "epoch : 28 Train accuracy and macro_f1: 0.7927710843373494 0.7886582771463951\n",
      "epoch : 28 Validation accuracy, macro_f1: 0.6504854368932039 0.6278189300411522\n",
      "train loss per epoch 0.6833701400890309\n",
      "Validation loss per epoch: 1.161826895649355\n",
      "*************** Starting with epoch:  29 ***********************\n",
      "epoch : 29 Train accuracy and macro_f1: 0.791566265060241 0.7874405687219094\n",
      "epoch : 29 Validation accuracy, macro_f1: 0.6504854368932039 0.62781929726715\n",
      "train loss per epoch 0.6776173720759037\n",
      "Validation loss per epoch: 1.1734095455590383\n"
     ]
    }
   ],
   "source": [
    "###########.........Start Training...........\n",
    "model = Twt()\n",
    "##### Hyperparameter\n",
    "#learning_rate=0.05\n",
    "learning_rate=0.01\n",
    "epochs = 30\n",
    "#opt=\"ADAM\"\n",
    "#opt=\"SGD\" \n",
    "opt=\"ADA\"\n",
    "if(opt==\"SGD\"):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "elif(opt==\"ADA\"):\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-06, weight_decay=0.0001)\n",
    "elif(opt==\"ADAM\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "check_val_acc = 0\n",
    "losses = []\n",
    "per_epoch_train_loss =[]\n",
    "per_epoch_val_loss =[]\n",
    "per_epoch_train_f1 =[]\n",
    "per_epoch_val_f1 = []\n",
    "for epoch in range(epochs): \n",
    "    print('*************** Starting with epoch: ', epoch, '***********************')\n",
    "    for i in range (0,len(train_data)):\n",
    "        #model_des.zero_grad()\n",
    "        #model_loc.zero_grad()\n",
    "        model.zero_grad()\n",
    "        #####Run forward pass.\n",
    "      \n",
    "        prediction_joint = model(training_data_twt[i])\n",
    "        \n",
    "        #print(\"prediction_joint :\", torch.argmax(prediction_joint, dim=1)) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        #Compute the loss, gradients, and update the parameters by\n",
    "        #calling optimizer.step()\n",
    "        loss = loss_function(prediction_joint, train_gt[i])\n",
    "        #if (i%200 == 0):\n",
    "            #print (\"loss per example\", loss.item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward(retain_graph=True)  #backpropagation\n",
    "        optimizer.step()\n",
    "    accuracy, macro_f1 = make_prediction_tr(model, training_data_twt, train_gt)\n",
    "    print('epoch :', epoch, 'Train accuracy and macro_f1:', accuracy, macro_f1)\n",
    "    per_epoch_train_f1.append(macro_f1)\n",
    "    val_accuracy, val_macro_f1, val_loss = make_prediction_val(model, validation_data_twt, valid_gt)\n",
    "    per_epoch_val_f1.append(val_macro_f1)\n",
    "    print('epoch :', epoch, 'Validation accuracy, macro_f1:', val_accuracy, val_macro_f1)\n",
    "    per_epoch_train_loss.append(np.mean(losses))\n",
    "    print(\"train loss per epoch\", np.mean(losses))\n",
    "    per_epoch_val_loss.append(np.mean(val_loss))\n",
    "    print('Validation loss per epoch:', np.mean(val_loss))\n",
    "    torch.save(model.state_dict(),\"data/Twt_utype_2layer/Twt_\"+str(epoch)+\".pt\")\n",
    "#     if (check_val_acc < val_macro_f1): #early stopping\n",
    "#         check_val_acc = val_macro_f1\n",
    "#         print (\"Model saved at epoch :\", epoch)\n",
    "#         torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "#         best_epoch = epoch\n",
    "        \n",
    "#print(\"Best model found at epoch : \", best_epoch)        \n",
    "#torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_plots(train_losses, val_losses, train_accs, test_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('loss_Twt_utype.png')\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, test_accs, '-', linewidth=2, label='validation')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('accuracy_Twt_utype.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1)\n",
    "# print(per_epoch_train_loss)\n",
    "# print(per_epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction( training_data_twt, ground_truths):\n",
    "    for epoch in range(0,30):\n",
    "        model = Twt()\n",
    "        model.load_state_dict(torch.load(\"data/Twt_utype_2layer/Twt_\"+str(epoch)+\".pt\")) #best #Testing accuracy, macro_f1: 0.7862595419847328 0.7560951140518181\n",
    "        predictions =[]\n",
    "        for i in range (0,len(training_data_twt)):\n",
    "            prediction_joint = model( training_data_twt[i])\n",
    "            pred = torch.argmax(prediction_joint, dim=1)\n",
    "            predictions.append(pred.item())\n",
    "        #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "        accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_twt)\n",
    "        macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "        print('epoch :', epoch, 'Testing accuracy, macro_f1:', accuracy, macro_f1)\n",
    "        #return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare testing data for neural net #########\n",
    "testing_data_twt =  nn_input(test_data,df)\n",
    "#testing_data_net =  nn_input_network(test_data,df)\n",
    "test_gt = find_groundtruth(test_data, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Testing accuracy, macro_f1: 0.5763358778625954 0.5224030501089325\n",
      "epoch : 1 Testing accuracy, macro_f1: 0.6335877862595419 0.5757280406403215\n",
      "epoch : 2 Testing accuracy, macro_f1: 0.6679389312977099 0.6225626504386986\n",
      "epoch : 3 Testing accuracy, macro_f1: 0.6717557251908397 0.6270581366910447\n",
      "epoch : 4 Testing accuracy, macro_f1: 0.6793893129770993 0.6347450980392156\n",
      "epoch : 5 Testing accuracy, macro_f1: 0.6908396946564885 0.6480443123516936\n",
      "epoch : 6 Testing accuracy, macro_f1: 0.6946564885496184 0.6531690418458376\n",
      "epoch : 7 Testing accuracy, macro_f1: 0.6946564885496184 0.6488583428789615\n",
      "epoch : 8 Testing accuracy, macro_f1: 0.7175572519083969 0.6816055887750873\n",
      "epoch : 9 Testing accuracy, macro_f1: 0.7099236641221374 0.6737318007662836\n",
      "epoch : 10 Testing accuracy, macro_f1: 0.7175572519083969 0.6830311407431499\n",
      "epoch : 11 Testing accuracy, macro_f1: 0.7213740458015268 0.6866143020782197\n",
      "epoch : 12 Testing accuracy, macro_f1: 0.7213740458015268 0.6857846586910626\n",
      "epoch : 13 Testing accuracy, macro_f1: 0.7137404580152672 0.6776542385371808\n",
      "epoch : 14 Testing accuracy, macro_f1: 0.7137404580152672 0.6768449607966577\n",
      "epoch : 15 Testing accuracy, macro_f1: 0.7061068702290076 0.6659819483348896\n",
      "epoch : 16 Testing accuracy, macro_f1: 0.7061068702290076 0.6659819483348896\n",
      "epoch : 17 Testing accuracy, macro_f1: 0.6946564885496184 0.6524941989997319\n",
      "epoch : 18 Testing accuracy, macro_f1: 0.6946564885496184 0.6524941989997319\n",
      "epoch : 19 Testing accuracy, macro_f1: 0.6946564885496184 0.6524941989997319\n",
      "epoch : 20 Testing accuracy, macro_f1: 0.6908396946564885 0.6486519535187052\n",
      "epoch : 21 Testing accuracy, macro_f1: 0.6984732824427481 0.6563119061754752\n",
      "epoch : 22 Testing accuracy, macro_f1: 0.6946564885496184 0.6524941989997319\n",
      "epoch : 23 Testing accuracy, macro_f1: 0.6946564885496184 0.6519588274431529\n",
      "epoch : 24 Testing accuracy, macro_f1: 0.6946564885496184 0.6513159751897474\n",
      "epoch : 25 Testing accuracy, macro_f1: 0.6870229007633588 0.6415711867076147\n",
      "epoch : 26 Testing accuracy, macro_f1: 0.6870229007633588 0.6415711867076147\n",
      "epoch : 27 Testing accuracy, macro_f1: 0.6870229007633588 0.6415711867076147\n",
      "epoch : 28 Testing accuracy, macro_f1: 0.6870229007633588 0.6409834273320199\n",
      "epoch : 29 Testing accuracy, macro_f1: 0.683206106870229 0.6350352389951964\n"
     ]
    }
   ],
   "source": [
    "make_prediction(testing_data_twt, test_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
