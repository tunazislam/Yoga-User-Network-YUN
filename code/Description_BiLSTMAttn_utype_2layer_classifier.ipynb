{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11cb695d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## load embeddings #######\n",
    "#loc_emb = torch.load('data/locationEmbeddings.pt') \n",
    "des_emb = torch.load('data/descriptionEmbeddings.pt') \n",
    "#twt_emb = torch.load('data/tweetsEmbeddings.pt') \n",
    "\n",
    "#load network embedding\n",
    "#net_emb = gsm.KeyedVectors.load_word2vec_format('data/userNetworkEmd.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user = net_emb ['000mrs000']\n",
    "#print(user)\n",
    "#print(type(net_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 1300 user location, description, yoga tweets, utype, umotivation\n",
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "#print (df) #[1308 rows x 7 columns] name, location, description, text, utype, umotivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create BiLSTMAttention Model for Description\n",
    "class BiLSTMDesAtt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMDesAtt, self).__init__() \n",
    "        self.lstm = nn.LSTM(300, 150//2 , num_layers=1, bidirectional=True ) #BiLSTM with attention \n",
    "        #self.lstm = nn.LSTM(300, 150 , num_layers=1, bidirectional=False) #LSTM with attention\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "        self.attn_fc = torch.nn.Linear(300, 1) #attention layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)\n",
    "        return (torch.zeros(2 * 1, 1, 150//2), torch.zeros(2 * 1, 1, 150//2)) # <- change here: first dim of hidden needs to be doubled\n",
    "        #return (torch.zeros(1 * 1, 1, 150), torch.zeros(1 * 1, 1, 150))#LSTM with attention\n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size()) #torch.Size([13, 1, 150])\n",
    "        #rnn_out = rnn_out.squeeze(0).unsqueeze(1) \n",
    "        #rnn_out = rnn_out.permute(2,0,1) \n",
    "        rnn_out = rnn_out.permute(1,0,2) \n",
    "        #print(\"permute rnn_out\", rnn_out.size()) #torch.Size([150, 13, 1])\n",
    "        #print(\"state\", state.size()) #torch.Size([2, 1, 75])\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        #print(\"merged_state\", merged_state.size()) #torch.Size([1, 150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).size()) #torch.Size([150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).size()) #torch.Size([150, 1])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).unsqueeze(2).size()) # torch.Size([150, 1, 1])\n",
    "        #merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"merged_state2 :\", merged_state.size()) #torch.Size([150, 1, 1])\n",
    "        merged_state = merged_state.permute(1,0,2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([13, 300])\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        lstm_out, hidden = self.lstm(X.view(len(X),1, -1))\n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('hidden[0] = h_n', hidden[0], hidden[0].size()) # torch.Size([2, 1, 75])\n",
    "        #print('hidden[1] = c_n', hidden[1], hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        h_n, c_n = hidden\n",
    "        #print('h_n', h_n, h_n.size()) # torch.Size([2, 1, 75])\n",
    "        #print('c_n', c_n, hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        attn_out = self.attention(lstm_out, h_n)\n",
    "        #print(\"attn_out\", attn_out.size()) #torch.Size([150, 1])\n",
    "        #logits = self.fc2(attn_out)\n",
    "        #logits = self.fc2(attn_out.permute(1,0))\n",
    "        #print(\"logits\", logits, logits.size())\n",
    "        #return logits \n",
    "        return attn_out\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Des(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Des, self).__init__()\n",
    "        self.model_des = BiLSTMDesAtt()\n",
    "        #self.model_loc = LSTMLoc()\n",
    "        #self.model_net = NetworkMLP()\n",
    "        self.fc1 = nn.Linear(150, 200) \n",
    "        self.fc2 = nn.Linear(200, 3) #if we use two-layer classifier\n",
    "    def forward(self, x_d): \n",
    "        prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size()) #torch.Size([1, 3])\n",
    "        #prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size()) #torch.Size([1, 3])\n",
    "        #prediction_net = self.model_net(x_n)\n",
    "        #print(prediction_net, prediction_net.size()) #torch.Size([1, 3])\n",
    "        #concat_pred = torch.cat((prediction_des, prediction_loc, prediction_net), 1) #concat with dim= 1\n",
    "        #concat_pred = torch.cat((prediction_des, prediction_loc), 1) #concat with dim= 1\n",
    "        #print(concat_pred, concat_pred.size()) #torch.Size([1, 6])\n",
    "        out = self.fc1(prediction_des)\n",
    "        out = self.fc2(F.relu(out))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net input #########\n",
    "def nn_input(train_data,df):\n",
    "    #ground_truths = []\n",
    "    training_data_des =[]\n",
    "    #training_data_loc=[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                if (not des_emb[train_data[i]]):\n",
    "                    print ('no description for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    break\n",
    "                else:\n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    break\n",
    "    return training_data_des\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Ground Truth #########\n",
    "def find_groundtruth(data, df):\n",
    "    ground_truths = []\n",
    "    for i in range (0, len(data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (data[i] == df.name[j]):\n",
    "                #print(data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                target_type = torch.tensor(utype, dtype=torch.long) #for user type\n",
    "                #target_type = torch.tensor(umotivation, dtype=torch.long) #for user motivation\n",
    "                ground_truths.append(target_type)\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_tr(model, training_data_des, ground_truths):\n",
    "    predictions =[]\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i])\n",
    "        \n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_val(model, training_data_des, ground_truths):\n",
    "    predictions =[]\n",
    "    val_losses = []\n",
    "    loss_function = nn.NLLLoss()\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i])\n",
    "        val_loss = loss_function(prediction_joint, ground_truths[i])\n",
    "        val_losses.append(val_loss.item())\n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    #print(type(predictions), type(ground_truths))\n",
    "    #print(\"predictions\", predictions)\n",
    "    #print(\"ground_truths\", ground_truths)\n",
    "    \n",
    "    return accuracy, macro_f1, val_losses\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no description for user:  bchi49\n",
      "no description for user:  viecestlavie\n",
      "no description for user:  crystalization_\n",
      "no description for user:  mimmosamami\n",
      "no description for user:  wenmarbyoga\n",
      "no description for user:  YogaLifeLine\n"
     ]
    }
   ],
   "source": [
    "##########......prepare training and validation data\n",
    "# ground truth training\n",
    "train_gt = find_groundtruth(train_data, df)\n",
    "#####prepare training data for neural net #########\n",
    "#training_data_net =  nn_input_network(train_data,df)\n",
    "#print(training_data_net, len(training_data_net)) #ok\n",
    "training_data_des =  nn_input(train_data,df)\n",
    "\n",
    "# ground truth validation\n",
    "valid_gt = find_groundtruth(valid_data, df)\n",
    "#####prepare validation data for neural net #########\n",
    "#validation_data_net =  nn_input_network(valid_data,df)\n",
    "validation_data_des =  nn_input(valid_data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Starting with epoch:  0 ***********************\n",
      "epoch : 0 Train accuracy and macro_f1: 0.5457831325301205 0.43221690609024604\n",
      "epoch : 0 Validation accuracy, macro_f1: 0.558252427184466 0.43699655030727985\n",
      "train loss per epoch 1.0492881620145706\n",
      "Validation loss per epoch: 0.9669747556586867\n",
      "*************** Starting with epoch:  1 ***********************\n",
      "epoch : 1 Train accuracy and macro_f1: 0.6012048192771084 0.5163722512602137\n",
      "epoch : 1 Validation accuracy, macro_f1: 0.6116504854368932 0.5151810467201552\n",
      "train loss per epoch 1.0054332397967936\n",
      "Validation loss per epoch: 0.9065305671645599\n",
      "*************** Starting with epoch:  2 ***********************\n",
      "epoch : 2 Train accuracy and macro_f1: 0.6397590361445783 0.5906927813360562\n",
      "epoch : 2 Validation accuracy, macro_f1: 0.6310679611650486 0.5656002751977984\n",
      "train loss per epoch 0.9718510137384675\n",
      "Validation loss per epoch: 0.8604444298640038\n",
      "*************** Starting with epoch:  3 ***********************\n",
      "epoch : 3 Train accuracy and macro_f1: 0.6650602409638554 0.6431115543251414\n",
      "epoch : 3 Validation accuracy, macro_f1: 0.6456310679611651 0.5994318017439405\n",
      "train loss per epoch 0.943707778062447\n",
      "Validation loss per epoch: 0.8237923610846973\n",
      "*************** Starting with epoch:  4 ***********************\n",
      "epoch : 4 Train accuracy and macro_f1: 0.6855421686746987 0.6716299067502134\n",
      "epoch : 4 Validation accuracy, macro_f1: 0.6504854368932039 0.6150521230612763\n",
      "train loss per epoch 0.9193413085104472\n",
      "Validation loss per epoch: 0.7957925579501587\n",
      "*************** Starting with epoch:  5 ***********************\n",
      "epoch : 5 Train accuracy and macro_f1: 0.6879518072289157 0.6782901677282753\n",
      "epoch : 5 Validation accuracy, macro_f1: 0.6699029126213593 0.6579584962887787\n",
      "train loss per epoch 0.8980152839817197\n",
      "Validation loss per epoch: 0.7740364343795961\n",
      "*************** Starting with epoch:  6 ***********************\n",
      "epoch : 6 Train accuracy and macro_f1: 0.7036144578313253 0.6984999365905628\n",
      "epoch : 6 Validation accuracy, macro_f1: 0.6844660194174758 0.6788463249019053\n",
      "train loss per epoch 0.8791541145918906\n",
      "Validation loss per epoch: 0.75805857972902\n",
      "*************** Starting with epoch:  7 ***********************\n",
      "epoch : 7 Train accuracy and macro_f1: 0.7132530120481928 0.7091837097976693\n",
      "epoch : 7 Validation accuracy, macro_f1: 0.6796116504854369 0.6730506155950753\n",
      "train loss per epoch 0.8623267203906034\n",
      "Validation loss per epoch: 0.7465558248816184\n",
      "*************** Starting with epoch:  8 ***********************\n",
      "epoch : 8 Train accuracy and macro_f1: 0.7180722891566265 0.7147235156203392\n",
      "epoch : 8 Validation accuracy, macro_f1: 0.6844660194174758 0.6808785529715763\n",
      "train loss per epoch 0.8471901128300462\n",
      "Validation loss per epoch: 0.738714757438713\n",
      "*************** Starting with epoch:  9 ***********************\n",
      "epoch : 9 Train accuracy and macro_f1: 0.7265060240963855 0.7237187588327808\n",
      "epoch : 9 Validation accuracy, macro_f1: 0.6796116504854369 0.6733870485843626\n",
      "train loss per epoch 0.8334910781726421\n",
      "Validation loss per epoch: 0.7331921822454744\n",
      "*************** Starting with epoch:  10 ***********************\n",
      "epoch : 10 Train accuracy and macro_f1: 0.736144578313253 0.7335303624919457\n",
      "epoch : 10 Validation accuracy, macro_f1: 0.6796116504854369 0.6769102990033223\n",
      "train loss per epoch 0.8209961644358533\n",
      "Validation loss per epoch: 0.7299987240106736\n",
      "*************** Starting with epoch:  11 ***********************\n",
      "epoch : 11 Train accuracy and macro_f1: 0.7469879518072289 0.7447262764945286\n",
      "epoch : 11 Validation accuracy, macro_f1: 0.6699029126213593 0.6703238141421489\n",
      "train loss per epoch 0.8095261164196583\n",
      "Validation loss per epoch: 0.7278647749024687\n",
      "*************** Starting with epoch:  12 ***********************\n",
      "epoch : 12 Train accuracy and macro_f1: 0.7481927710843373 0.7454124012099611\n",
      "epoch : 12 Validation accuracy, macro_f1: 0.6747572815533981 0.6775065228122136\n",
      "train loss per epoch 0.7989369122213076\n",
      "Validation loss per epoch: 0.7266819173994573\n",
      "*************** Starting with epoch:  13 ***********************\n",
      "epoch : 13 Train accuracy and macro_f1: 0.7542168674698795 0.7524796925557723\n",
      "epoch : 13 Validation accuracy, macro_f1: 0.6747572815533981 0.6775065228122136\n",
      "train loss per epoch 0.7891091477927366\n",
      "Validation loss per epoch: 0.7266386207735654\n",
      "*************** Starting with epoch:  14 ***********************\n",
      "epoch : 14 Train accuracy and macro_f1: 0.755421686746988 0.7534714649218466\n",
      "epoch : 14 Validation accuracy, macro_f1: 0.6796116504854369 0.6831036470300273\n",
      "train loss per epoch 0.7799433943180435\n",
      "Validation loss per epoch: 0.7273147976369534\n",
      "*************** Starting with epoch:  15 ***********************\n",
      "epoch : 15 Train accuracy and macro_f1: 0.7566265060240964 0.7548248651136307\n",
      "epoch : 15 Validation accuracy, macro_f1: 0.6796116504854369 0.6831036470300273\n",
      "train loss per epoch 0.7713481894234218\n",
      "Validation loss per epoch: 0.7283993727952531\n",
      "*************** Starting with epoch:  16 ***********************\n",
      "epoch : 16 Train accuracy and macro_f1: 0.7602409638554217 0.7584296619411125\n",
      "epoch : 16 Validation accuracy, macro_f1: 0.6796116504854369 0.6831036470300273\n",
      "train loss per epoch 0.7632506768012579\n",
      "Validation loss per epoch: 0.7304912985673229\n",
      "*************** Starting with epoch:  17 ***********************\n",
      "epoch : 17 Train accuracy and macro_f1: 0.7674698795180723 0.7659917635172672\n",
      "epoch : 17 Validation accuracy, macro_f1: 0.6844660194174758 0.686862367417923\n",
      "train loss per epoch 0.7555860465252834\n",
      "Validation loss per epoch: 0.7332893587214854\n",
      "*************** Starting with epoch:  18 ***********************\n",
      "epoch : 18 Train accuracy and macro_f1: 0.7674698795180723 0.7654234823700473\n",
      "epoch : 18 Validation accuracy, macro_f1: 0.6844660194174758 0.686862367417923\n",
      "train loss per epoch 0.7483064920717665\n",
      "Validation loss per epoch: 0.7361318291463319\n",
      "*************** Starting with epoch:  19 ***********************\n",
      "epoch : 19 Train accuracy and macro_f1: 0.7698795180722892 0.7687617185715451\n",
      "epoch : 19 Validation accuracy, macro_f1: 0.6844660194174758 0.686862367417923\n",
      "train loss per epoch 0.741364011634029\n",
      "Validation loss per epoch: 0.7395905007436437\n",
      "*************** Starting with epoch:  20 ***********************\n",
      "epoch : 20 Train accuracy and macro_f1: 0.7710843373493976 0.7698064560784212\n",
      "epoch : 20 Validation accuracy, macro_f1: 0.6844660194174758 0.686862367417923\n",
      "train loss per epoch 0.7347235444573044\n",
      "Validation loss per epoch: 0.7431405766604884\n",
      "*************** Starting with epoch:  21 ***********************\n",
      "epoch : 21 Train accuracy and macro_f1: 0.7759036144578313 0.7741238810430474\n",
      "epoch : 21 Validation accuracy, macro_f1: 0.6893203883495146 0.6910031503896535\n",
      "train loss per epoch 0.7283553871475078\n",
      "Validation loss per epoch: 0.7473565541135455\n",
      "*************** Starting with epoch:  22 ***********************\n",
      "epoch : 22 Train accuracy and macro_f1: 0.7771084337349398 0.775245740016325\n",
      "epoch : 22 Validation accuracy, macro_f1: 0.6844660194174758 0.6887233682618343\n",
      "train loss per epoch 0.722233859960218\n",
      "Validation loss per epoch: 0.7515162718346686\n",
      "*************** Starting with epoch:  23 ***********************\n",
      "epoch : 23 Train accuracy and macro_f1: 0.7807228915662651 0.7798754936686375\n",
      "epoch : 23 Validation accuracy, macro_f1: 0.6844660194174758 0.6886784587981533\n",
      "train loss per epoch 0.7163340101949871\n",
      "Validation loss per epoch: 0.7562974461544196\n",
      "*************** Starting with epoch:  24 ***********************\n",
      "epoch : 24 Train accuracy and macro_f1: 0.7855421686746988 0.7842797669248531\n",
      "epoch : 24 Validation accuracy, macro_f1: 0.6844660194174758 0.6886784587981533\n",
      "train loss per epoch 0.7106351843035544\n",
      "Validation loss per epoch: 0.7611691435997926\n",
      "*************** Starting with epoch:  25 ***********************\n",
      "epoch : 25 Train accuracy and macro_f1: 0.7891566265060241 0.7883943619462942\n",
      "epoch : 25 Validation accuracy, macro_f1: 0.6844660194174758 0.6886784587981533\n",
      "train loss per epoch 0.7051189860338526\n",
      "Validation loss per epoch: 0.7666903341062439\n",
      "*************** Starting with epoch:  26 ***********************\n",
      "epoch : 26 Train accuracy and macro_f1: 0.7903614457831325 0.7902617755784137\n",
      "epoch : 26 Validation accuracy, macro_f1: 0.6941747572815534 0.6968810916179337\n",
      "train loss per epoch 0.6997697559450052\n",
      "Validation loss per epoch: 0.7723696258967941\n",
      "*************** Starting with epoch:  27 ***********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 27 Train accuracy and macro_f1: 0.7939759036144578 0.7937869306686358\n",
      "epoch : 27 Validation accuracy, macro_f1: 0.6990291262135923 0.7027163625261279\n",
      "train loss per epoch 0.6945734126806714\n",
      "Validation loss per epoch: 0.7780740939948744\n",
      "*************** Starting with epoch:  28 ***********************\n",
      "epoch : 28 Train accuracy and macro_f1: 0.7951807228915663 0.7948337362926855\n",
      "epoch : 28 Validation accuracy, macro_f1: 0.6990291262135923 0.7027163625261279\n",
      "train loss per epoch 0.6895230302465658\n",
      "Validation loss per epoch: 0.7844121872438389\n",
      "*************** Starting with epoch:  29 ***********************\n",
      "epoch : 29 Train accuracy and macro_f1: 0.8 0.8001410250360795\n",
      "epoch : 29 Validation accuracy, macro_f1: 0.6990291262135923 0.7027163625261279\n",
      "train loss per epoch 0.6846087519165472\n",
      "Validation loss per epoch: 0.7905753582209638\n"
     ]
    }
   ],
   "source": [
    "###########.........Start Training...........\n",
    "model = Des()\n",
    "##### Hyperparameter\n",
    "#learning_rate=0.05\n",
    "learning_rate=0.01\n",
    "epochs = 30\n",
    "#opt=\"ADAM\"\n",
    "#opt=\"SGD\" \n",
    "opt=\"ADA\"\n",
    "if(opt==\"SGD\"):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "elif(opt==\"ADA\"):\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-06, weight_decay=0.0001)\n",
    "elif(opt==\"ADAM\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "check_val_acc = 0\n",
    "losses = []\n",
    "per_epoch_train_loss =[]\n",
    "per_epoch_val_loss =[]\n",
    "per_epoch_train_f1 =[]\n",
    "per_epoch_val_f1 = []\n",
    "for epoch in range(epochs): \n",
    "    print('*************** Starting with epoch: ', epoch, '***********************')\n",
    "    for i in range (0,len(train_data)):\n",
    "        #model_des.zero_grad()\n",
    "        #model_loc.zero_grad()\n",
    "        model.zero_grad()\n",
    "        #####Run forward pass.\n",
    "      \n",
    "        prediction_joint = model(training_data_des[i])\n",
    "        \n",
    "        #print(\"prediction_joint :\", torch.argmax(prediction_joint, dim=1)) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        #Compute the loss, gradients, and update the parameters by\n",
    "        #calling optimizer.step()\n",
    "        loss = loss_function(prediction_joint, train_gt[i])\n",
    "        #if (i%200 == 0):\n",
    "            #print (\"loss per example\", loss.item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward(retain_graph=True)  #backpropagation\n",
    "        optimizer.step()\n",
    "    accuracy, macro_f1 = make_prediction_tr(model, training_data_des, train_gt)\n",
    "    print('epoch :', epoch, 'Train accuracy and macro_f1:', accuracy, macro_f1)\n",
    "    per_epoch_train_f1.append(macro_f1)\n",
    "    val_accuracy, val_macro_f1, val_loss = make_prediction_val(model, validation_data_des, valid_gt)\n",
    "    per_epoch_val_f1.append(val_macro_f1)\n",
    "    print('epoch :', epoch, 'Validation accuracy, macro_f1:', val_accuracy, val_macro_f1)\n",
    "    per_epoch_train_loss.append(np.mean(losses))\n",
    "    print(\"train loss per epoch\", np.mean(losses))\n",
    "    per_epoch_val_loss.append(np.mean(val_loss))\n",
    "    print('Validation loss per epoch:', np.mean(val_loss))\n",
    "    \n",
    "    torch.save(model.state_dict(),\"data/Des_utype_2layer/Des_\"+str(epoch)+\".pt\")\n",
    "#     if (check_val_acc < val_macro_f1): #early stopping\n",
    "#         check_val_acc = val_macro_f1\n",
    "#         print (\"Model saved at epoch :\", epoch)\n",
    "#         torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "#         best_epoch = epoch\n",
    "        \n",
    "#print(\"Best model found at epoch : \", best_epoch)        \n",
    "#torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU5dnw8d+Vyb6QhIQ1AcK+yhpwwQW1KFqXqlRxq/rWUq3WpU/7atv3se1jbW1tfayt1QctPq11qdWi1rpbEJRFwg4BIWCAECALWxayzMz1/nGGZAjJMAkzmSzX9/OZT852n1yHIXPNuc+9iKpijDHGtCQq0gEYY4zp2CxRGGOMCcgShTHGmIAsURhjjAnIEoUxxpiAoiMdQChlZmZqTk5OpMMwxphOY9WqVWWq2ivQMV0qUeTk5JCXlxfpMIwxptMQkZ0nO8aqnowxxgRkicIYY0xAliiMMcYEZInCGGNMQJYojDHGBBTWRCEis0TkCxEpEJEHm9mfKiL/FJF1IrJJRG4Ltqwxxpj2EbZEISIu4CngEmAMcL2IjGly2F1AvqpOAGYAvxWR2CDLGmOMaQfhvKOYBhSo6g5VrQNeAa5scowCKSIiQDJwAHAHWdYYY0w7CGeiyAJ2+60X+bb5+wMwGigGNgD3qqo3yLIAiMhcEckTkbzS0tJQxW6MMR1KrdvD0oIyHn13C5f9fgm7yqvb7XeHs2e2NLOt6SxJFwNrgQuAocCHIrIkyLLORtV5wDyA3Nxcm4XJGNMlqCpb91eyZFspS7aVseLLcmrqvQ37lxSUcmPGoHaJJZyJoggY4LeejXPn4O824FF1ptkrEJEvgVFBljXGmE4nv/gIeTsPUFHjprLWTVWtm8oaNxW1blwiPHPzFABq3V6u+MOn1Lobk8OovimcO6IXZw/LZNrgnu0WczgTxUpguIgMBvYAc4AbmhyzC7gQWCIifYCRwA7gUBBljTGmw3F7vBSWV7F5bwVb9h1hy94Kbj9nCGcOzQBg6fYyfv6vzc2WjYtufBoQH+Pi0tP6IcA5IzKZPiyT3inx7XEJJwhbolBVt4jcDbwPuID5qrpJRO7w7X8GeBj4XxHZgFPd9ICqlgE0VzZcsRpjTLBUlYpaN4er6xnQMxEAr1d58B/r2by3gq37K467CwCYkpPekCjG9k/lpjMGkhwXQ0p8NEmxLpLjY0iOiyY5LhpVxWnfA/993cT2vbgWiFPr0zXk5uaqjR5rjAmVpdvLeHfDPkoraimtrKWkoobSilpq6r30TIpl9X/ObDj27F/9m6KDRwHISktgdL8URvXtwah+KUwemE7/tIRIXUZAIrJKVXMDHdOlhhk3xpjWOlRdx/qiw6zbfYh1RYe45awczhnuTM+wdV8FLyw/cRTuhBgXPeKj8XgVV5Tz7f/hK8eRHB/NiD4ppCbEtOs1hJslCmNMt6Kq/HlpIWt2H2Ld7kMUNmlmOrJvSkOiOGNoBg9dNobePeLolRxH7x7x9EqJIznuxI/O80f1bpf4I8EShTGmyyo5UsPqXQfZvLeC+2eOAEBEeHHFLraVVALOA+RxWalMyE5jwoBUpuY0tiYa1bcHo/r2iEjsHYklCmNMl+D2eNmyr4LVuw6yaqfzOvbMAOC6qQManhN865wh1Hu9TMhOY2TfFGJcNj5qIJYojDGdUnWdm0PV9Q0f/p8XHuCGZ1ccd0xSrItJA9OZPDCN6KjGfrzXTh2ACZ4lCmNMp1BeWUvezoOs/PIAKwsPsLH4COeP7M1ztzgNdiZkpzEkM4mJA9KYNCidKQPTGdk3peFhs2k7SxTGmIirqfdQWeumosZNRU092emJ9EyKBeDFFTuZ/+mXbC+tOq6MK0qodXsa1pPiovn392e0Z9jdhiUKY0zY1bm9lFXWNlQTVdW6+eqTS3yJwU2d5/gOak9cN5GvTXLGAT1a52F7aRXxMVFMGpDO1Jx0pg7uyaSB6c22PjKhZ//KxpiQ23e4hjW7DrJ610FW7zrEhj2HGd2vB2/eNR1whqfwb5Ya4xJS4p2eyslx0STGuhr2XT6hP5MHpTOufyqx0fbQORIsURhjglLn9lLpG8CusvbYq57hvVMahrJ4deVunvhoK8WHa04oX1vvwetVoqIEV5Sw8PszSI6LJiU+mrjoqIZhK5rq0yOePj0iM8aRcViiMMYATke0vYdr2Lq/guo6D5ee1q9h+9ifvE91nafZcj+5fAy3TR8MQFxMFMWHa0iJi2biwLSGFkcTB6SRlhh7XLnBmUnhvSATMpYojOli/rG6CFeUEBftIi46itjoqIaf/g+Jt+6v4LOCMrbur2Tr/gq27qugotYNQL/U+IZEISLERkdR5/aS7KsaanjFR9MvtfHb/oyRvfng/nMZ1iuZKGtt1GVYojCmEztYVUf+3iNMH5YJON/+/+Pv62hprM+HrxzLzWfmALDoixJ+8c6W4/anJ8Ywok8KI/umHDeO0YofXUisq+XqoWNSE2K63DhHxhKFMZ2O16t8WlDG3/J28+Gm/cRGR/H5jy8kMTYaVbhqUhZ1bi+1bi91vlet20Odx0uvlLiG8+Tm9OT6aQMY0Sel4ZWZHNtsMoiLdp2wzXQfliiM6SSKDlbz97wiXltVxJ5DztAUInBWTgYHqupIjI0mKkp4/Nrg5jCYPDCdyQPTwxmy6SIsURjTQflPYLP7QDXnPrawoUopOz2Ba3MHMHtKdoed58B0HZYojOkAquvcbNlXwea9R9i89wj5xUcor6rjkx+cD8CAnonkDkqnX2oC100dwJlDMuxhsWk3liiMCbM6t5fDR+s5fLSOw0fryUyOY1CG0zR04ZYSHn47ny/Lq5p9AF1aUdvwXOFvc8+05GAiIqyJQkRmAb/Dmff6OVV9tMn+HwA3+sUyGuilqgdEpBCoADyA+2RT9RkTbnsPH2Xb/kqqfJ3NqmrdVNU5YxR5vcoPLx3dcOw35n9Owf4KDh+tp6pJ/4NvnzeEH17iHBsf42JHWRXRUcKwPsmM6deDMf17MLqf8zrWlBWwJGEiJmyJQkRcwFPATKAIWCkib6lq/rFjVPUx4DHf8ZcD96vqAb/TnK+qZeGK0ZiWqCqrdx0kKy2Rvr5+Au9s2MfDb+c3e3yMS45LFCVHahp6J7uihNSEGNISYkhNjKGvXy/jiQPS+Nc9ZzOsd7K1LDIdVjjvKKYBBaq6A0BEXgGuBJr/S4PrgZfDGI8xJ/VlWRULVhfxxtpidh2o5t4LhzfMjDaoZyJnDc0gydfZLCnO5SzHRpMUF90wPAXAH2+cTIwrirTEGJLjolvsf5AQ62Js/9R2uz5j2iKciSIL2O23XgSc3tyBIpIIzALu9tuswAciosD/qOq8FsrOBeYCDBw4MARhm+6mvLKWt9fvZcGaPazdfahhe58ecaTEN/6JfGVMH74ypk9Q5xzSKznkcRoTKeFMFM19hWqhvyiXA581qXaarqrFItIb+FBEtqjq4hNO6CSQeQC5ubktnd90Q4er66lxe6ip93C03kNNvZeaemd9aK/khoHsHn47nzfWFgPOjGizxvXj6slZnDEkwya9MYbwJooiwH++wWyguIVj59Ck2klVi30/S0RkAU5V1gmJwphjauo9xMc01vNP+8VH1Lq9zR77/746mtvPGQLAVZOzOXS0nqsmZXHRmL4kxNqzAmP8hTNRrASGi8hgYA9OMrih6UEikgqcB9zkty0JiFLVCt/yRcB/hTFW04kdqq7jiY+28fGW/Xxw33kNH/S9UuKodXuJj4kiPtpFfIzLWY5x0S+1sZPaeSN6cd6IXpEK35gOL2yJQlXdInI38D5O89j5qrpJRO7w7X/Gd+hVwAeq6j/PYR9gge8BYDTwkqq+F65YTedU7/Hy4vKd/PdH2zh8tJ4ogaXby7hwtPMc4dMHLohwhMZ0DaItDTPZCeXm5mpeXl6kwzDt4JOtpTz8dj4FJZUAnDU0g/+8bAyj+/WIcGTGdC4isupk/dSsZ7bpdP7zjY28sHwnAIMyEvnRpaO5aEyfkw6BbYxpG0sUptOZPiyTBWv2cPcFw7hteo51VDMmzCxRmA6tqtbNX5bt5Gi9h+/5Or5dPLYPpw8+n/Sk2JOUNsaEgiUK0yFV1zkJYt7iHRyoqsMVJcyenM3AjERExJKEMe3IEoXpUKrr3LzgSxDlVXUATBqYxv1fGcGAnjbvgjGRYInCdBhHauq54DefUFZZCzgD5t0/cwTnDs+0B9XGRJAlChNRNfUe4qKjEBF6xMcwNSedvYdruO8rwzlvRC9LEMZ0AJYoTERU1NTz0opdPLvkS568fiJnDc0E4Ddfn0BirMsShDEdiCUK0672H6lh/mdf8tLyXVTUugF4Z8PehkSRFGf/JY3paOyv0rSLgpIK5i3ewYI1e6j3OKMBnDGkJ98+bygzbJwlYzo0SxSmXby1bi+v5hUhApee1pe55w5l4oC0SIdljAmCJQoTcl6v8uHm/Xi9yiWn9QPgljMHcbCqjm+ePZiczKQIR2iMaQ1LFCYkat0eVn55kEVflPDh5v3sLK8mKy2Br4zpQ4wriozkOB7+2rhIh2mMaQNLFOaUrNp5gD8u3M7S7eUcrfc0bM9KS+CbZw/G24VGJzamu7JEYYJWU+9h2Y5yEmNcnD4kA4Daei8fbykBYHS/HswY6UwClDsonWhXVCTDNcaEiCUKc1J7Dx/liQ+38cbaPdS6vVwwqndDosjN6cmvZ4/nvBG96NMjPsKRGmPCwRKFadGRmnr+55Pt/OnTL6mpd+aePi0rlckDG1srxUZHcW3ugJZOYYzpAixRmGatLDzAt19YxQHfwHxfPa0f3794JIOtxZIx3U5YE4WIzAJ+hzNn9nOq+miT/T8AbvSLZTTQS1UPnKysCa/hvZNxe7xMzUnnh5eOZvLA9EiHZEzn5q6DXcugbGt4zn/abEgIz99p2ObMFhEXsBWYCRQBK4HrVTW/heMvB+5X1QtaW/YYmzO77VbsKOd/lxby39dNJD7GmTFuV3k1A3om2LhLxrRVVTls+wC2vgfb/w21R8L3u+5aCb1GtLpYpOfMngYUqOoOXzCvAFcCLX3YXw+83Maypo0KSip49N0v+GjzfgBOH7yLW6cPBmBgRmIkQzOm81GFks1OYtj6Huz+HPD7Mt5rNAyYBlFh+OiNTw39OX3CmSiygN1+60XA6c0dKCKJwCzg7jaUnQvMBRg4cOCpRdzNfLBpH3e9tJp6j5IY6+Lb5w7l6/Zg2pjW8Xrgy0/gi3ed5HBoV+O+qBgYfA6MmAUjLob0nIiFeSrCmSiaq69oqZ7rcuAzVT3Q2rKqOg+YB07VU2uD7K4WfVHC3S+tod6jXDM5mwcuGUnvFGveakzQqg/Amr/Cyufg0M7G7YmZTlIYMQuGng9xKZGLMUTCmSiKAP+vp9lAcQvHzqGx2qm1ZU0rFZRU8O0XVlHn8XLb9BweumyMPYcwJljFa2Hls7DhNXDXONvSBjkPk0dcAllTIKprdTYNZ6JYCQwXkcHAHpxkcEPTg0QkFTgPuKm1ZU3bDO2VzI2nD6LG7bEkYdqfuxZqjkBiRuf5QHXXQv6b8PmzUPR54/ZhX4Gp34LhMyHKFbn4wixsiUJV3SJyN/A+ThPX+aq6SUTu8O1/xnfoVcAHqlp1srLhirW7UFVEBBHhPy8bjSqWJEx4eb1wYAfsWQV78pyf+zaApw5csdAjC1KzIXWA72c2pGY1rsdGuN/O4SLIex5W/xmqSp1tcakw6SaY+k3IGBrZ+NpJ2JrHRoI1j23Zxj2H+a+383nqhsn0SomLdDimq6os8SWFVVCUB8WroeZwk4PEaaFTc+jk54tPg5iExnIAIk2W/fedWvjHUeDIHlDfYJd9xsG0b8FpX498AguhSDePNR3EF/squPlPKzhYXc/Ti7bz0OVjIh2SCYX6GijJh33rYe862Lve+WCLFE89VJeduD25L2TnOnX3WVOg/0QnUdRVO/Ee3u18c294HVvf4ySTYBJKuERFw5irYdpcGHiGX2LqXixRdHE7Siu58TknSZw/shcPXDIy0iGZtqg54lTZ7FvvJIR966F0C3jdkY7seLHJ0H9SY1LIzoUe/Vs4NhEyhzuv5qhCdblTTdVQ86HHLx87zn89lOLTIMFmYrRE0YXtKq/mhmdXUFZZy/RhGTx90xTiorvuA7eIqT8KhZ9BwYfw5WKorQzt+b1uqGim0Z9EQeZI6Dce+o53fvYcGsGHqgLJvUP3+0UgKTM05zKnxBJFF7Xn0FGuf3Y5+47UMC2nJ89+I7dhaI6w8XqhcAlsfst5EDnhekjpG97fGSkHdsC2j5zhGQo/BffR8P4+Vyz0HuOXFCZAn7Fdqq7cdFyWKLqoN9bsYc+ho0wckMb826aSGNvMW+2uhaVPOrfXIy6GtDb2bK8qg7Uvwqr/dT5Aj/n4YafZ4KSbYPjFEB3btvN3BPU1sPPTxuRwYPvx+/tNgGEzneaSLVW1tJUIpPQDV0xoz2tMkKzVUxdRXlnLoi9KuXpyFiKCqvLnpYVcNTmb1IRmPmA8bvj7LbDl7cZtvcc29ijNzg1chaDq3D3kPQ+b/wneemd7Sn+YMAfKtzlDGhyrQ0/MdLZPugl6jw7uojxu5wN5/0Yo3QoZw2DkrFPv6XpwJ6x/FTa/2UyLnBZUlh5/1xCX6vS6HX6RkxxS+pxaTMZESDCtnixRdGKHq+t5f9M+/rm+mKXby/F4lbe/ezbjsk4yOJjXC2/d7dwFxKU6Y9HsWAR1fnXrCT2dD8ERF8PQCxof6FWV+909+L5VS5Rz7JRbnW/VLt/dS2UpbHgVVr8ApZsbz501BSbdDOOubhzIrKrMSQj7N/leG6FkC3hqj489Ot65Sxl7lZPQgq16qTniVImte8VJcG3R9zTn+oZfBNlTG6/TmE7MEkUXVOf28q8Nxby9bi+Lt5VS73Hev+go4ZzhmXxv5khOyw6QKFThvR/CiqchJhFufgMGnu6Mlb/zM9j6Pmx9Fw4WNpaJioaBZ0JiT+cuweNMZkRKf5j8DecuIS3AYIKqsGc1rHkBNr7eONRydILTQubAdqjc33zZtIFO+/WMYU67/F3LaGjdEp3gJLKxVzkf3rFNRrv1emDHQic5bH678Y4gOh5GXebc4bTU4qap2BRIygjuWGM6kbAlChGZp6pz2xxZmHSHRFHv8TL1kY84VF1PlMCZQzO4fHx/Lh7bl/SkIJ4BLHoUFv3SGdXyhr/BsAtPPEYVyrb5hkp+3/lwPtbpCGm8exh+Ueu/VddVO1VVa144/pt9bLLzsLbvOOchbZ9xThVV06GTjxQ7QylsWgC7VzRuj0l07jDGXe0klw1/h/V/h8p9jccMOttJDmOuhPgerYvbmC7qlBKFiPRsqQywTlWzTzG+kOsOiQLguSU7iIuOYta4fq3rZb38aXjvQaeq6Ov/63xgBuPoQSj42Ol1O/rywHcPrXHgSychZQ53BlVr7bg/h3Y3Jo09LbzvPYc4ra/GXwfpg049ZmO6mFNNFB5gJ8d3ilffepaqdrgmLF01USzeWsqaXYf4zvlDiXG1cRC1NS/Cm99xlq/4A0y+OXQBdgQHd0L+G7DxH1CxF0Z91UkQ2VO7bW9aY4JxqkN47AAuVNVdTXeIyO5mjjdhcLi6nh+8to79R2rpmxrHdVPb0IQ1/y3n4TXAxb/oekkCnLuF6fc6L2NMSAX6evoE0NJM3b8OQyymGT95ayP7j9QyeWAas6e0ocpn+0J4/ZugXjjvATjzrtAHaYzp0lq8o1DVpwLs+314wjH+3tmwlzfWFpMQ4+K3107EFdXKKpTdn8MrNzqtlKZ9G2b8MDyBGmO6tBbvKETkF37LM9snHHNMSUUNP16wAYAfXTqKwZmtHKph30Z4cTbUVzl19bMetbp6Y0ybBKp6muW3/KtwB2IaqSo/fH0DB6vrOWd4Jjed0crWOuXb4YWrnF7Hoy5zHl53lpnEjDEdjn16dEDVdR5q3B5S4qP59ezxrZ+F7o3vQFUJDD4PrvmT9SA2xpySQJ8gvUXkezjNYY8tN1DVx8MaWTeWFBfNC//ndHaUVdEvNeHkBfztWgG7lzsD/V33V4iJD0+QxphuI9AdxbNACpDst+z/OikRmSUiX4hIgYg82MIxM0RkrYhsEpFP/LYXisgG376u1zmiGV6v4vU6/VqiooRhvZNbf5KlTzo/p37Teh8bY0IiUKunn53KiUXEBTwFzASKgJUi8paq5vsdkwb8EZilqrtEpHeT05yvqs3Mrdg1Pb+0kA/z9/HY7AkM6Jl48gJNlRXAln85cxdM+3boAzTGdEvhrLyeBhSo6g4AEXkFuBLI9zvmBuAfxzr1qWpJGOPp0ApKKvj1e1uodXvZvPdI2xLFsj8A6gxXYcNeG2NCJJwPs7MA/x7cRb5t/kYA6SKySERWicg3/PYp8IFve4sDEIrIXBHJE5G80tLSkAXfnuo9Xr736jpq3V5mT8nmorFtmBWushTWvewsn/Xd0AZojOnWwnlH0VxTnaYDS0UDU4ALgQRgmYgsV9WtwHRVLfZVR30oIltUdfEJJ1SdB8wDZ6ynkF5BO/njwu2sLzpMVloCD10+pm0nWfksuGtgxCXQa2RoAzTGdGsnTRRNWzv5HAZWqeraAEWLAP8xJ7KBpjPEFwFlqloFVInIYmACsFVVi8GpjhKRBThVWSckis5uQ9Fhfv/vbQA89vXx9Ihvw3SXddXw+bPOst1NGGNCLJiqp1zgDpxqoyxgLjADeFZE/m+AciuB4SIyWERigTnAW02OeRM4R0SiRSQROB3YLCJJIpICICJJwEXAxuAvq/P4xTubcXuVW8/K4ayhmW07ydoX4egBZ+a4QWeFNkBjTLcXTNVTBjBZVSsBROQnwGvAucAqWhggUFXdInI38D7gAuar6iYRucO3/xlV3Swi7wHrAS/wnKpuFJEhwAJfR7No4CVVfe9ULrQj8nqVcVk92H2wmv+4aEQbT+KBZb5huc76rg3TYYwJuZPOcCcim4EJqlrnW48D1qrqaBFZo6qT2iHOoHTW+ShUtfW9r4/JfxNe/YYz8c89ayDKFdrgjDFd2qnOR3HMS8ByEXnTt3458LKvSii/5WImWG1OEqrwma+D3Zl3W5IwxoTFSROFqj4sIu8AZ+O0ZLpDVY99bb8xnMF1ZRv3HGbt7kNcPr4/qYlteIANsGu5MwVoQjpMsrfCGBMewbR6+h3wN1X9XTvE0238ZVkhr+YVUXzoKP931qi2nWSpb1qQqbdDbCuHITfGmCAF0+ppNfD/fOM1PSYiAeuyzMlV17l5Z8M+AK6Zkt22k5Rtgy/eAVccTGuxP6IxxpyykyYKVf2zql6K049hK/ArEdkW9si6sPc37aOy1s2kgWkM7dWGgf+gcbiOiddDctMhsowxJnRaM4THMGAUkANsCUs03cTrq/YAcM3kNt5NVJbA2pcBcR5iG2NMGJ00UYjIsTuI/wI2AVNU9fKwR9ZFFR86ymfby4iNjuLy8f3bdpLPnwVPLYy8FDKHhzZAY4xpIpjmsV8CZ3an4b7DacGaPajCzDF92tbaqa7KGdcJbLgOY0y7CKZ57DMiki4i04B4v+1dbtyl9jCiTwpnDOnJ7LY+xF7zIhw9CNlTYeAZoQ3OGGOaEUzz2NuBe3EG9VsLnAEsAy4Ib2hd08wxfZg5po1zRXg9vofY2HAdxph2E8zD7HuBqcBOVT0fmAR0zokfOrvNb8GhndBzCIy6LNLRGGO6iWASRY2q1oAzzpOqbgFswoNWqqn38LN/bmLNroNtO8Fxw3XcZcN1GGPaTTCJosg3t/UbOBMIvcmJ80qYk/gwfz/Pf1bIQ29uan1hTz288wMoXg2JGTDhhtAHaIwxLQjmYfZVvsWfishCIBXockN+h9trq4oAWv8Qu7IEXr0Fdi0FVyx89bcQ24b5tI0xpo1aOxXqSN/Uo6YV9h+pYcm2UmJcwhUTWtF3omgV/O0mqCiGlP5w3V8he0r4AjXGmGa0pmc2ODPdmVZ6Y80evAoXjOpNelJscIVWvwDPz3KSxMAzYe4iSxLGmIho7R2FtcdsJVX1q3YacJKjAXcdvP9DWPmcsz71drj4lxAdZIIxxpgQa22isDaZrbRhz2G2lVSSkRTLjJG9Ah9cWeLMVrdrme95xOMw+eb2CdQYY1oQzFhPqSLy3yKSB7wpIr8VkdRgTi4is0TkC98Q5Q+2cMwMEVkrIptE5JPWlO0M0hJiufWsHG4+cxAxrgD/3EWr4H/Oc5JESn+47T1LEsaYDiGYObNfBzYCf/ZtuhlnDu2rT1LOhTMs+UygCFgJXK+q+X7HpAFLgVmquktEeqtqSTBlm9NZ58xm9Qvwr++Bp855HvH1P0NKG3tvG2NMK4RqzuyhqnqN3/rPRGRtEOWmAQWqusMXzCvAlRw/z/YNwD9UdReAqpa0omzXsPgx+PfPnWV7HmGM6YCCafV0VETOPrYiItOBo0GUywJ2+60X+bb5GwGki8giEVklIt9oRdlj8cwVkTwRySst7Vgjizz2/hZe+XwX1XXu5g8oK4BFvwIErvi900fCkoQxpoMJ5o7iDuAvfs8lDgK3BFGuuRZSTeu5ooEpwIVAArBMRJYHWdbZ6PTrmAdO1VMQcbWL0opanvlkBwAXju5DYmwz/9Qf/Bi89TDpJpj8jRP3G2NMBxAwUYhIFE4nuwki0gNAVY8Eee4iwL89aDYnDv1RBJSpahVQJSKLgQlBlu3Q3ly7B49XuXBUb3qlxJ14QMFHsPU9iE2BCx5q/wCNMSZIAaueVNUL3O1bPtKKJAHOA+jhIjJYRGKBOcBbTY55EzhHRKJFJBE4HdgcZNkO7fXVznSnzQ7Z4amH937kLJ/3A3twbYzp0IKpevpQRL4P/A2oOrZRVQ8EKqSqbhG5G3gfcAHzVXWTiNzh2/+Mqm4WkfeA9YAXeE5VNwI0V7b1lxcZW/dXsHnvEVITYrhgdO8TD1j5Jyj7whku/HTr7G6M6diCSRT/x/fzLr9tCgw5WUFVfQd4p8m2Z5qsPwY8FkzZzmLxVueh+oWjexMX3WQ48KpyWPQLZ/niX0B0M9VSxhjTgQQzeuzg9gikK/mswPsOm9UAABb4SURBVJle/OxhmSfuXPgI1ByGoRfAiFntHJkxxrReMD2z7/J1jDu2ni4i3wlvWJ3bzDF9+cro3kxvmij2bYRVz4O4nP4SNpWpMaYTCKYfxbdU9dCxFVU9CHwrfCF1fjecPpDnbplKnx7xjRtV4b0HQb1Ox7reoyIXoDHGtEIwiSJKpPGrr294DesV1lpb3obCJZCQDjM67dBVxphuKJhE8T7wqohcKCIXAC9jM9y16KUVu1haUEa9x9u4sb4G3v+xs3z+jyGxZ2SCM8aYNgim1dMDwLeBO3F6TH8APBfOoDqrqlo3P3lrIx6vsuahi0hN8OXh5U/BoZ3QewxMuS2yQRpjTCsF0+rJCzzte5kAPi88QL1HmTAgjdSEGGfjkb2w+LfO8qxfgqu1U4AYY0xknfRTS0SGA78ExgANT2dV9aT9KLqbpb5msdOHZjRu/PhnUF8Foy6DITMiEpcxxpyKYJ5RPI9zN+EGzgf+ArwQzqA6q88KygEam8UW5cG6l53Z6i76eQQjM8aYtgsmUSSo6sc4kxztVNWfAheEN6zOp7yylvy9R4iNjmLKoHTweuHdB5ydZ94FPa3fojGmcwqmwrzGN4rsNt/4S3uAZgYw6t6W7XDuJqbmpBMf44J1f4M9eZDcB875jwhHZ4wxbRfMHcV9QCJwD87cETcT3HwU3UptvZfs9ATOGpoJdVXw0U+cHV/5KcSlRDI0Y4w5JcG0elrpW6wErG1nC66Zks01U7Jxe7yw7q9QsRf6TYDxcyIdmjHGnJIWE4WIBJz/QVWvCH04nV+0KwpW+571n34HRAVz02aMMR1XoDuKM3HmrX4ZWEHz05MaYFd5NQmxLmcmu5ItUPS5M3PdmCsjHZoxxpyyQF93+wI/AsYBvwNm4kxb+omqftIewXUWj33wBVMf+YgFa4pgje9u4rRrIDYpsoEZY0wItJgoVNWjqu+p6i3AGUABsEhEvttu0XUCqsqy7U5Hu9P6Jjr9JgAmfSOCURljTOgErEAXkTgRuRr4K84Md08C/wj25CIyS0S+EJECETlhyFQRmSEih0Vkre/1kN++QhHZ4NueF/wlta8v9ldQVllHnx5xDD2wGKrLnTGdsiZHOjRjjAmJQA+z/4xT7fQu8LNjc1kHyzcc+VM4VVZFwEoReUtV85scukRVL2vhNOerallrfm97a+iNPTQTWeOb5XXyN2xSImNMlxHoYfbNQBUwArjHf0oKQFW1x0nOPQ0oUNUdACLyCnAl0DRRdGrHxne6MKsePv7YGa5j/HURjsoYY0In0DOKKFVN8b16+L1SgkgSAFk4raaOKfJta+pMEVknIu+KyFj/EIAPRGSViMwN6mraWb3Hy3Jfj+xzqz5wZq8b9VWbb8IY06WEc8zr5upetMn6amCQqlaKyKXAG8Bw377pqlosIr2BD0Vki6ouPuGXOElkLsDAgQNDF30QCkoqqXF7GZqZQMrmV5yNk25u1xiMMSbcwtkbrAgY4LeeDRT7H6CqR1S10rf8DhAjIpm+9WLfzxJgAU5V1glUdZ6q5qpqbq9evUJ/FQGM7teDNQ/N5PkZNXBoF6QOgCHnt2sMxhgTbuFMFCuB4SIyWERigTnAcb29RaTvsfm4RWSaL55yEUkSkRTf9iTgIqBVD9PbS4/4GAYWvu6sTLrJemIbY7qcsFU9qarbN9rs+4ALmK+qm0TkDt/+Z4DZwJ0i4gaOAnNUVUWkD7DAl0OigZdUtUPN0+3xKgJE1RyEzf8EBCbeGOmwjDEm5MI6L6evOumdJtue8Vv+A/CHZsrtACaEM7ZTtWRbKd//+zp+PWA5F3hqYegFkDbg5AWNMaaTsXqSNlq6vZyyylrG7n/T2WAPsY0xXZQlijb6rKCMcfIlfaq3QUJPp1msMcZ0QZYo2uBgVR35e49wfYxvbMQJcyA6LrJBGWNMmFiiaINlO8qJ01q+5lrqbLBqJ2NMF2aJog0+LSjjkqjPSdIqyJoCfcZEOiRjjAkbSxRtsLSgjOuiFzkrdjdhjOniLFG0kqry1CVpnBG1GY1JhHHXRDokY4wJK0sUrSQijN3ndDCXsVdBfDDjIxpjTOdliaK1PG5Y+5KzbNVOxphuwBJFK6gqTz37NFTuw5sxHAaeEemQjDEm7CxRtMLW/ZUM37MAAJl0k81iZ4zpFixRtMLqTVu4IGoNHlzIhOsjHY4xxrQLSxStELPxb0SLl319zoOUPpEOxxhj2oUliiB5vcrIgwsBiM21h9jGmO7DEkWQvizey2jdgZsoMsfPjHQ4xhjTbixRBKlo7UKixcuu+FFIXEqkwzHGmHZjiSJIw46uBaAu+6wIR2KMMe3LEkWQsg6tAmDUGZdEOBJjjGlfYU0UIjJLRL4QkQIRebCZ/TNE5LCIrPW9Hgq2bLuqOQLFa0FcMMA62RljupewzZktIi7gKWAmUASsFJG3VDW/yaFLVPWyNpZtF7vXL2SAenD3yyU6LjkSIRhjTMSE845iGlCgqjtUtQ54BbiyHcqG3O7VHwCwOmpspEIwxpiICWeiyAJ2+60X+bY1daaIrBORd0Xk2CdxsGURkbkikicieaWlpaGI+wSZZZ8DkDD8vLCc3xhjOrJwJormBkLSJuurgUGqOgH4PfBGK8o6G1XnqWququb26tWrzcG25GjFIYbUF+DWKHImXxDy8xtjTEcXzkRRBAzwW88Giv0PUNUjqlrpW34HiBGRzGDKtpfCtf8mWrxsix5OSo/0SIRgjDERFc5EsRIYLiKDRSQWmAO85X+AiPQVcYZgFZFpvnjKgynbXqq3LgKgLHNqJH69McZEXNhaPamqW0TuBt4HXMB8Vd0kInf49j8DzAbuFBE3cBSYo6oKNFs2XLEGkrZ/BQDRQ+35hDGmexLnc7lryM3N1by8vNCdsLYCzy8Hogpld2+jb6/M0J3bGGM6ABFZpaq5gY6xntmB7FqBCy9kTbYkYYzptixRBFK4BIDoIedEOBBjjIkcSxQBuHc4iYKcsyMbiDHGRFDYHmZ3erUVsHcNbqIo6TGR/pGOx5hupr6+nqKiImpqaiIdSpcQHx9PdnY2MTExrS5riaIFB7YsoSde1ulwxmVmRDocY7qdoqIiUlJSyMnJwdeK3rSRqlJeXk5RURGDBw9udXmremrBgU0fA7A7dTKuKPtPakx7q6mpISMjw5JECIgIGRkZbb47s0TRgviiZQB4BkyPcCTGdF+WJELnVP4tLVE0p7aCftWbcWsUfU6zjnbGmO7NEkUzjm5figsvG3QI44dkRzocY0wEHDp0iD/+8Y+tLnfppZdy6NChMEQUOZYomlG20Xk+sT1pIomx9rzfmO6opUTh8XgClnvnnXdIS0sLV1gRYZ+Czcg65AwDYvNjG9Nx5Dz4rxb3/eKq07jh9IEAvLRiFz9asKHFYwsf/WpQv+/BBx9k+/btTJw4kZiYGJKTk+nXrx9r164lPz+fr33ta+zevZuamhruvfde5s6d68SZk0NeXh6VlZVccsklnH322SxdupSsrCzefPNNEhISWnHVHYPdUTRVW0HUXmd+7HFnXBzpaIwxEfLoo48ydOhQ1q5dy2OPPcbnn3/OI488Qn6+MyPz/PnzWbVqFXl5eTz55JOUl5efcI5t27Zx1113sWnTJtLS0nj99dfb+zJCwu4omtq9AtQDWbkQlxLpaIwxPsHeCdxw+sCGu4tQmjZt2nF9EJ588kkWLFgAwO7du9m2bRsZGcf3uRo8eDATJ04EYMqUKRQWFoY8rvZgdxRNlG/6NwD5ceMjHIkxpiNJSkpqWF60aBEfffQRy5YtY926dUyaNKnZPgpxcXENyy6XC7fb3S6xhpoliiY8vvGdPnWPinAkxphISklJoaKiotl9hw8fJj09ncTERLZs2cLy5cvbObr2ZVVP/moryTi8EbdGkTby3EhHY4yJoIyMDKZPn864ceNISEigT58+DftmzZrFM888w/jx4xk5ciRnnHFGBCMNP0sUfnTXclx4WaPDmDTM+k8Y09299NJLzW6Pi4vj3XffbXbfsecQmZmZbNy4sWH797///ZDH116s6slPxRcLAVgTNZahvZIjHI0xxnQMYU0UIjJLRL4QkQIReTDAcVNFxCMis/22FYrIBhFZKyIhnN+0Ze7tzvOJw71PJ8oGAjTGGCCMVU8i4gKeAmYCRcBKEXlLVfObOe5XwPvNnOZ8VS0LV4zHqa0k9aDzfCJ5hE1UZIwxx4TzjmIaUKCqO1S1DngFuLKZ474LvA6UhDGWk9u9HBcedieMZOrIQRENxRhjOpJwJoosYLffepFvWwMRyQKuAp5pprwCH4jIKhGZ29IvEZG5IpInInmlpaVtj7bwUwAGT7mYiQO61jgtxhhzKsKZKJqr5Ncm608AD6hqc6NsTVfVycAlwF0i0mx7VVWdp6q5qprbq1evtkfrSxTknNP2cxhjTBcUzkRRBAzwW88Gipsckwu8IiKFwGzgjyLyNQBVLfb9LAEW4FRlhUdtJbpnNV5xUdF7cth+jTGm60pOdlpKFhcXM3v27GaPmTFjBnl5gdvmPPHEE1RXVzesd4Rhy8OZKFYCw0VksIjEAnOAt/wPUNXBqpqjqjnAa8B3VPUNEUkSkRQAEUkCLgI2Ei67lyPqYZ1nMMuK6sL2a4wxXV///v157bXX2ly+aaLoCMOWh63Vk6q6ReRunNZMLmC+qm4SkTt8+5t7LnFMH2CBb+q+aOAlVX0vXLF6dizBBSz3jubaQenh+jXGmLb6aWqYznu4xV0PPPAAgwYN4jvf+Y5z6E9/ioiwePFiDh48SH19PT//+c+58srj2+gUFhZy2WWXsXHjRo4ePcptt91Gfn4+o0eP5ujRow3H3XnnnaxcuZKjR48ye/Zsfvazn/Hkk09SXFzM+eefT2ZmJgsXLmwYtjwzM5PHH3+c+fPnA3D77bdz3333UVhYGPbhzMPaM1tV3wHeabKt2QShqrf6Le8AJoQzNn812z4hCShMmUxGctxJjzfGdH1z5szhvvvua0gUr776Ku+99x73338/PXr0oKysjDPOOIMrrriixfmon376aRITE1m/fj3r169n8uTGqu1HHnmEnj174vF4uPDCC1m/fj333HMPjz/+OAsXLiQzM/O4c61atYrnn3+eFStWoKqcfvrpnHfeeaSnp7Nt2zZefvllnn32Wa699lpef/11brrpppD9W9gQHrWVJJStx61RxOacFelojDHNCfDNP1wmTZpESUkJxcXFlJaWkp6eTr9+/bj//vtZvHgxUVFR7Nmzh/3799O3b99mz7F48WLuueceAMaPH8/48Y2jUr/66qvMmzcPt9vN3r17yc/PP25/U59++ilXXXVVwyi2V199NUuWLOGKK64I+3Dmlih2LydKPazTYZw2NOvkxxtjuo3Zs2fz2muvsW/fPubMmcOLL75IaWkpq1atIiYmhpycnGaHF/fX3N3Gl19+yW9+8xtWrlxJeno6t95660nPo9q00WijpsOZ+1dxhUK3H+tJs6fyvagH+b37a0zN6RnpcIwxHcicOXN45ZVXeO2115g9ezaHDx+md+/exMTEsHDhQnbu3Bmw/LnnnsuLL74IwMaNG1m/fj0AR44cISkpidTUVPbv33/cAIMtDW9+7rnn8sYbb1BdXU1VVRULFizgnHPapzl/t7+jOORJYG3imRyJqicnIzHS4RhjOpCxY8dSUVFBVlYW/fr148Ybb+Tyyy8nNzeXiRMnMmpU4Hlr7rzzTm677TbGjx/PxIkTmTbNaeU/YcIEJk2axNixYxkyZAjTp09vKDN37lwuueQS+vXrx8KFCxu2T548mVtvvbXhHLfffjuTJk1ql1nzJNDtTGeTm5urJ2uj3JKqWjdJcd0+bxrTYWzevJnRo0dHOowupbl/UxFZpaq5gcp1+6qnYyxJGGNM8yxRGGOMCcgShTGmw+pKVeORdir/lpYojDEdUnx8POXl5ZYsQkBVKS8vJz4+vk3lrWLeGNMhZWdnU1RUxClNH2AaxMfHk52d3aayliiMMR1STEwMgwcPjnQYBqt6MsYYcxKWKIwxxgRkicIYY0xAXapntoiUAoEHX2lZJlAWwnAiratdD3S9a+pq1wNd75q62vXAidc0SFUDziPdpRLFqRCRvJN1Y+9Mutr1QNe7pq52PdD1rqmrXQ+07Zqs6skYY0xAliiMMcYEZImi0bxIBxBiXe16oOtdU1e7Huh619TVrgfacE32jMIYY0xAdkdhjDEmIEsUxhhjAur2iUJEZonIFyJSICIPRjqeUBCRQhHZICJrRaRtU/5FkIjMF5ESEdnot62niHwoItt8P9MjGWNrtXBNPxWRPb73aa2IXBrJGFtDRAaIyEIR2Swim0TkXt/2Tvs+BbimTvk+iUi8iHwuIut81/Mz3/ZWv0fd+hmFiLiArcBMoAhYCVyvqvkRDewUiUghkKuqnbKjkIicC1QCf1HVcb5tvwYOqOqjvoSerqoPRDLO1mjhmn4KVKrqbyIZW1uISD+gn6quFpEUYBXwNeBWOun7FOCarqUTvk8iIkCSqlaKSAzwKXAvcDWtfI+6+x3FNKBAVXeoah3wCnBlhGPq9lR1MXCgyeYrgT/7lv+M8wfcabRwTZ2Wqu5V1dW+5QpgM5BFJ36fAlxTp6SOSt9qjO+ltOE96u6JIgvY7bdeRCf+j+FHgQ9EZJWIzI10MCHSR1X3gvMHDfSOcDyhcreIrPdVTXWaahp/IpIDTAJW0EXepybXBJ30fRIRl4isBUqAD1W1Te9Rd08U0sy2rlAXN11VJwOXAHf5qj1Mx/M0MBSYCOwFfhvZcFpPRJKB14H7VPVIpOMJhWauqdO+T6rqUdWJQDYwTUTGteU83T1RFAED/NazgeIIxRIyqlrs+1kCLMCpYuvs9vvqkI/VJZdEOJ5Tpqr7fX/IXuBZOtn75Kv3fh14UVX/4dvcqd+n5q6ps79PAKp6CFgEzKIN71F3TxQrgeEiMlhEYoE5wFsRjumUiEiS70EcIpIEXARsDFyqU3gLuMW3fAvwZgRjCYljf6w+V9GJ3iffg9I/AZtV9XG/XZ32fWrpmjrr+yQivUQkzbecAHwF2EIb3qNu3eoJwNfU7QnABcxX1UciHNIpEZEhOHcR4Ex1+1JnuyYReRmYgTMc8n7gJ8AbwKvAQGAX8HVV7TQPh1u4phk41RkKFALfPlZ33NGJyNnAEmAD4PVt/hFOnX6nfJ8CXNP1dML3SUTG4zysduHcFLyqqv8lIhm08j3q9onCGGNMYN296skYY8xJWKIwxhgTkCUKY4wxAVmiMMYYE5AlCmOMMQFZojCmFUTE4zeK6NpQjjgsIjn+o8sa01FERzoAYzqZo74hEYzpNuyOwpgQ8M0B8ivf+P+fi8gw3/ZBIvKxb0C5j0VkoG97HxFZ4JsrYJ2InOU7lUtEnvXNH/CBr0etMRFlicKY1kloUvV0nd++I6o6DfgDTm9/fMt/UdXxwIvAk77tTwKfqOoEYDKwybd9OPCUqo4FDgHXhPl6jDkp65ltTCuISKWqJjezvRC4QFV3+AaW26eqGSJShjMZTr1v+15VzRSRUiBbVWv9zpGDMxT0cN/6A0CMqv48/FdmTMvsjsKY0NEWlls6pjm1fsse7Dmi6QAsURgTOtf5/VzmW16KMyoxwI0401ECfAzcCQ2Ty/RoryCNaS37tmJM6yT4Zgw75j1VPdZENk5EVuB8Abvet+0eYL6I/AAoBW7zbb8XmCci38S5c7gTZ1IcYzoce0ZhTAj4nlHkqmpZpGMxJtSs6skYY0xAdkdhjDEmILujMMYYE5AlCmOMMQFZojDGGBOQJQpjjDEBWaIwxhgT0P8HnrlLEBana20AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_plots(train_losses, val_losses, train_accs, test_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('loss_Des_utype.png')\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, test_accs, '-', linewidth=2, label='validation')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('accuracy_Des_utype.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1)\n",
    "# print(per_epoch_train_loss)\n",
    "# print(per_epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction(training_data_des, ground_truths):\n",
    "    for epoch in range(0,30):\n",
    "        model = Des()\n",
    "        model.load_state_dict(torch.load(\"data/Des_utype_2layer/Des_\"+str(epoch)+\".pt\")) \n",
    "        predictions =[]\n",
    "        for i in range (0,len(training_data_des)):\n",
    "            prediction_joint = model(training_data_des[i])\n",
    "            pred = torch.argmax(prediction_joint, dim=1)\n",
    "            predictions.append(pred.item())\n",
    "        #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "        accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "        macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "        print('epoch :', epoch, 'Testing accuracy, macro_f1:', accuracy, macro_f1)\n",
    "        #return accuracy, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare testing data for neural net #########\n",
    "testing_data_des =  nn_input(test_data,df)\n",
    "#testing_data_net =  nn_input_network(test_data,df)\n",
    "test_gt = find_groundtruth(test_data, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Testing accuracy, macro_f1: 0.583969465648855 0.4358745335268584\n",
      "epoch : 1 Testing accuracy, macro_f1: 0.6641221374045801 0.5996017346185988\n",
      "epoch : 2 Testing accuracy, macro_f1: 0.6793893129770993 0.6247393561786085\n",
      "epoch : 3 Testing accuracy, macro_f1: 0.683206106870229 0.6403867767110448\n",
      "epoch : 4 Testing accuracy, macro_f1: 0.6984732824427481 0.6592394366036727\n",
      "epoch : 5 Testing accuracy, macro_f1: 0.6984732824427481 0.6575313542502428\n",
      "epoch : 6 Testing accuracy, macro_f1: 0.7022900763358778 0.6632889266817839\n",
      "epoch : 7 Testing accuracy, macro_f1: 0.7099236641221374 0.6685800436773199\n",
      "epoch : 8 Testing accuracy, macro_f1: 0.7099236641221374 0.6685800436773199\n",
      "epoch : 9 Testing accuracy, macro_f1: 0.7213740458015268 0.677286732908886\n",
      "epoch : 10 Testing accuracy, macro_f1: 0.7175572519083969 0.6732797068597756\n",
      "epoch : 11 Testing accuracy, macro_f1: 0.7099236641221374 0.6649915228464498\n",
      "epoch : 12 Testing accuracy, macro_f1: 0.7175572519083969 0.6847331093691675\n",
      "epoch : 13 Testing accuracy, macro_f1: 0.7061068702290076 0.6697449329028277\n",
      "epoch : 14 Testing accuracy, macro_f1: 0.7137404580152672 0.6782216803269435\n",
      "epoch : 15 Testing accuracy, macro_f1: 0.7175572519083969 0.6816007143147044\n",
      "epoch : 16 Testing accuracy, macro_f1: 0.7213740458015268 0.6849632511889322\n",
      "epoch : 17 Testing accuracy, macro_f1: 0.7175572519083969 0.6816007143147044\n",
      "epoch : 18 Testing accuracy, macro_f1: 0.7175572519083969 0.6816007143147044\n",
      "epoch : 19 Testing accuracy, macro_f1: 0.7251908396946565 0.6876750950390303\n",
      "epoch : 20 Testing accuracy, macro_f1: 0.7251908396946565 0.6876750950390303\n",
      "epoch : 21 Testing accuracy, macro_f1: 0.7213740458015268 0.6849632511889322\n",
      "epoch : 22 Testing accuracy, macro_f1: 0.7251908396946565 0.6923255487303424\n",
      "epoch : 23 Testing accuracy, macro_f1: 0.7175572519083969 0.6846544870923142\n",
      "epoch : 24 Testing accuracy, macro_f1: 0.7137404580152672 0.6812694984720379\n",
      "epoch : 25 Testing accuracy, macro_f1: 0.7099236641221374 0.6785599190294871\n",
      "epoch : 26 Testing accuracy, macro_f1: 0.7099236641221374 0.6785599190294871\n",
      "epoch : 27 Testing accuracy, macro_f1: 0.7099236641221374 0.6785599190294871\n",
      "epoch : 28 Testing accuracy, macro_f1: 0.7137404580152672 0.6819364867158985\n",
      "epoch : 29 Testing accuracy, macro_f1: 0.7137404580152672 0.6819364867158985\n"
     ]
    }
   ],
   "source": [
    "make_prediction(testing_data_des, test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
