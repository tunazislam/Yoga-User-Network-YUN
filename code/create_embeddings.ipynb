{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ffc5670>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji #pip install emoji --upgrade\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained emoji2vec \n",
    "\n",
    "e2v = gsm.KeyedVectors.load_word2vec_format('pre-trained/emoji2vec.bin', binary=True)\n",
    "#happy_emoji_vector = e2v['🇯🇵']    # Produces an embedding vector of length 300\n",
    "#w2v = gsm.KeyedVectors.load_word2vec_format('data/word2vec/w2v_model')\n",
    "#happy_word_vector = w2v ['happy']\n",
    "#print(happy_emoji_vector)\n",
    "#print(happy_word_vector)\n",
    "#vec = we2v['I am really happy right now! 😄']\n",
    "#vec = Word_Emoji2Vec.from_word2vec_paths['love life 😄']\n",
    "#print(vec)\n",
    "#happy_word_emoji_vector = np.concatenate((happy_word_vector, happy_emoji_vector), axis=0)\n",
    "#print(happy_word_emoji_vector, len(happy_word_emoji_vector), happy_word_emoji_vector.shape)#600 (600,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-trained word2vec\n",
    "\n",
    "#w2v = gsm.KeyedVectors.load_word2vec_format(\"pre-trained/numberbatch-19.08.txt.gz\") #problem need to fix\n",
    "w2v = gsm.KeyedVectors.load_word2vec_format('pre-trained/GoogleNews-vectors-negative300.bin', binary=True) #ok\n",
    "#happy_word_vector = w2v ['happy']\n",
    "#print(happy_word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights to use in from_pretrained()\n",
    "weights_text = torch.FloatTensor(w2v.vectors)\n",
    "#print (weights_text, weights_text.size()) #torch.Size([3000000, 300])\n",
    "weights_emoji = torch.FloatTensor(e2v.vectors)\n",
    "#print (weights_emoji, weights_emoji.size()) #torch.Size([1661, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(w2v ['Valium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### text preprocessing #########\n",
    "\n",
    "def remove_string_noise(input_str):\n",
    "    input_str = re.sub(r\"http\\S+\", \"\", input_str)\n",
    "    \n",
    "    #give special char you want to remove\n",
    "    #do not put space between chars, and space (\" \") is not a special char\n",
    "    punctuation_noise =\"!\\\"$%&'#()*+,-./:;<=>?@[\\]^_`{|}~\" #print string.punctuation \n",
    "    #number_noise = \"0123456789\"\n",
    "    special_noise = \"\"\n",
    "\n",
    "    #all_noise = punctuation_noise + number_noise + special_noise\n",
    "    all_noise = punctuation_noise + special_noise\n",
    "\n",
    "    for c in all_noise:\n",
    "        if c in input_str:\n",
    "            input_str = input_str.replace(c, \" \")#replace with space\n",
    "    fresh_str = ' '.join(input_str.split())\n",
    "    return fresh_str\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "#     #HappyEmoticons\n",
    "#     emoticons_happy = set([\n",
    "#         ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "#         ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "#         '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "#         'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "#         '<3'\n",
    "#         ])\n",
    "\n",
    "#     # Sad Emoticons\n",
    "#     emoticons_sad = set([\n",
    "#         ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "#         ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "#         ':c', ':{', '>:\\\\', ';('\n",
    "#         ])\n",
    "\n",
    "#     #combine sad and happy emoticons\n",
    "#     emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    sw = set(stopwords.words('english'))\n",
    "    operators = set(('no', 'not', 'nor', 'none'))\n",
    "    stop_words = set(sw) - operators\n",
    "    stop_words.update([ 'amp', 'rt'])  ###as we are using set so we used .update....otherwise .extends\n",
    "    \n",
    "#     tweet = re.sub(r':', '', tweet)\n",
    "#     tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#     tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "    filtered_tweet = []\n",
    "    for w in word_tokens:\n",
    "        #if w not in stop_words and w not in emoticons:\n",
    "        if w not in stop_words:\n",
    "            filtered_tweet.append(w)\n",
    "    lemmatized_tweet = []\n",
    "    for word in filtered_tweet:\n",
    "        lemmatized_tweet.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "\n",
    "    #print (sentences)\n",
    "    #return ' '.join(filtered_tweet)\n",
    "    return ' '.join(lemmatized_tweet)\n",
    "    #return ' '.join(sentences)\n",
    "    \n",
    "def pre_processing_tweets (df):\n",
    "    #p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.MENTION, p.OPT.RESERVED)\n",
    "    p.set_options(p.OPT.RESERVED)\n",
    "    \n",
    "    clean_text = []\n",
    "    #for i in range (0, df.shape[0]):\n",
    "        #clean_text.append(p.clean(str(df['text'][i])))#python 3\n",
    "\n",
    "    for i in df:\n",
    "        clean_text.append(p.clean(str(i)))#python 3\n",
    "    #print(clean_text) #all ok\n",
    "    fresh_text1 = []\n",
    "    for i in range (0, df.shape[0]):\n",
    "        #fresh_text1.append(remove_string_noise(clean_text[i].encode('ascii', 'ignore').decode(\"utf-8\"))) #can remove other emojis and no \\UF..\n",
    "        fresh_text1.append(remove_string_noise(clean_text[i]))\n",
    "    #print(fresh_text1) #all ok\n",
    "    \n",
    "    #Call clean_tweet method-2 for extra preprocessing\n",
    "    filtered_tweet = []\n",
    "    for i in range (0, len(fresh_text1)):\n",
    "        filtered_tweet.append(clean_tweets(fresh_text1[i].lower()))\n",
    "\n",
    "#     #####for bigram\n",
    "    \n",
    "#     df_clean = pd.DataFrame({'clean': filtered_tweet})\n",
    "#     #df_clean = df_clean.dropna().drop_duplicates()\n",
    "#     sent = [row.split() for row in df_clean['clean']]\n",
    "#     #print (sent_word, type(sent_word))\n",
    "\n",
    "#     ## create bigram\n",
    "#     #Creates the relevant phrases from the list of sentences:\n",
    "#     phrases = Phrases(sent, min_count=1, progress_per=10000)\n",
    "#     #print (phrases, type(phrases))\n",
    "\n",
    "#     #The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:\n",
    "#     bigram = Phraser(phrases)\n",
    "#     #print (bigram)\n",
    "\n",
    "#     #Transform the corpus based on the bigrams detected:\n",
    "#     sentences = bigram[sent]\n",
    "#     training_data  = [sentences[i] for i in range(0,len(sentences))]\n",
    "#     return training_data\n",
    "    \n",
    "    return filtered_tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 15k user location and description\n",
    "\n",
    "#df = pd.read_csv(\"data/yoga_user_loc_desc_mentioned_yoga_2600.csv\") \n",
    "#print (df) #[2597 rows x 4 columns] Name, Location, Description\n",
    "\n",
    "#Load 1300 user location, description, yoga tweets, utype, umotivation\n",
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "#print (df) #[1308 rows x 7 columns] name, location, description, text\n",
    "\n",
    "#for user description\n",
    "proc_des = pre_processing_tweets(df['description'])\n",
    "#print(proc_des, len(proc_des)) #15168\n",
    "\n",
    "\n",
    "df_clean_des = pd.DataFrame({'clean': proc_des})\n",
    "#print(df_clean_des.head(2))\n",
    "token_des = [row.split() for row in df_clean_des['clean']]\n",
    "#print(token_des, len(token_des)) #2597\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for user location\n",
    "proc_loc = pre_processing_tweets(df['location'])\n",
    "#print(proc_loc, len(proc_loc)) #2597\n",
    "\n",
    "df_clean_loc = pd.DataFrame({'clean': proc_loc})\n",
    "token_loc= [row.split() for row in df_clean_loc['clean']]\n",
    "#print(token_loc, len(token_loc)) #2597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from emoji import UNICODE_EMOJI\n",
    "#print(emoji.UNICODE_EMOJI.keys())\n",
    "def is_emoji(s):\n",
    "    count = 0\n",
    "    for emoji in UNICODE_EMOJI:\n",
    "    #for bl in emoji.UNICODE_EMOJI.keys():\n",
    "        #print (bl)\n",
    "        #print('s.count(bl)', s.count(bl))\n",
    "        count += s.count(emoji)\n",
    "        #print('count', count)\n",
    "        if count > 4:\n",
    "            return False\n",
    "    return bool(count)\n",
    "print (is_emoji(\"😘\")) #true\n",
    "print(is_emoji(\"🇺🇸\")) #false why??? we can solve it using count > 4 now true\n",
    "print(is_emoji(\"🇺🇸love😘\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_using_word_embedding(training_data, model_w2v):    \n",
    "    word_to_ix = {}\n",
    "    for sent in training_data:\n",
    "        #print (\"sent\", sent)\n",
    "        for word in sent:\n",
    "            #print (\"word\", word)\n",
    "            if word not in word_to_ix:\n",
    "                if word in model_w2v.vocab:\n",
    "                    word_to_ix[word] = model_w2v.vocab[word].index\n",
    "                    #print \"(word_to_ix\", word_to_ix)\n",
    "    return word_to_ix\n",
    "def create_word_embeddings_w2v(weights):\n",
    "    word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(weights))\n",
    "    return word_embeddings\n",
    "\n",
    "def prepare_sequence_word_em(seq, to_ix):\n",
    "    idxs = []\n",
    "    #for w in seq:\n",
    "    if seq in to_ix:\n",
    "        idxs.append(to_ix[seq])\n",
    "    else:\n",
    "        idxs.append(to_ix[\"my_unknown\"])         \n",
    "\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### build word_to_ix and emoji_to_ix for Location\n",
    "\n",
    "word_to_ix = build_vocab_using_word_embedding(token_loc, w2v)\n",
    "# Adding a new key value pair for unknown word\n",
    "word_to_ix.update( {'my_unknown' : len(word_to_ix)} ) \n",
    "#print (word_to_ix, len(word_to_ix)) #618\n",
    "\n",
    "word_embeddings = create_word_embeddings_w2v(weights_text)\n",
    "#print(word_embeddings) #Embedding(3000000, 300)\n",
    "\n",
    "\n",
    "emo_to_ix = build_vocab_using_word_embedding(token_loc, e2v)\n",
    "# Adding a new key value pair for unknown emoji\n",
    "emo_to_ix.update( {'my_unknown' : len(emo_to_ix)} ) \n",
    "#print (emo_to_ix, len(emo_to_ix)) #3\n",
    "\n",
    "emoji_embeddings = create_word_embeddings_w2v(weights_emoji)\n",
    "#print(emoji_embeddings) #Embedding(1661, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare senquence including text and emoji for whole user location\n",
    "\n",
    "\n",
    "#token_loc = [['맠동', '잼젠'], ['hyderabad', 'india', '🇮🇳'], ['bharat'], ['india'], ['retweets', 'r', 'not', 'endorsements']]\n",
    "#print(len(token_loc))#5\n",
    "#user_loc_embedd = []\n",
    "user_loc_embedd_d = {}\n",
    "for i in range(len(token_loc)):\n",
    "    #print (token_loc[i])\n",
    "    sentence_embedd = []\n",
    "    text =[]\n",
    "    emoji = []\n",
    "    for j in range(len(token_loc[i])):\n",
    "        #print (token_loc[i][j])\n",
    "        if is_emoji(token_loc[i][j]) == False:  \n",
    "            #print (token_loc[i][j])\n",
    "            sequence = prepare_sequence_word_em(token_loc[i][j], word_to_ix)\n",
    "            #print(sequence)\n",
    "            embeds = word_embeddings(sequence)\n",
    "            text.append(embeds)\n",
    "            #text.append(torch.tensor(w2v[i])) #problem with OOV word\n",
    "            #build the sentence embedding\n",
    "            sentence_embedd.append(embeds)\n",
    "\n",
    "        if is_emoji(token_loc[i][j]) == True:\n",
    "            #print (token_loc[i][j])\n",
    "            sequence_emo = prepare_sequence_word_em(token_loc[i][j], emo_to_ix)\n",
    "            #print(sequence)\n",
    "            embeds_emo = emoji_embeddings(sequence_emo)\n",
    "            emoji.append(embeds_emo)\n",
    "            #emoji.append(torch.tensor(e2v[i])) #problem with OOV emoji\n",
    "            #build the sentence embedding\n",
    "            sentence_embedd.append(embeds_emo)\n",
    "    #sys.exit()\n",
    "        \n",
    "#     print('len(text):', len(text)) \n",
    "#     print('len(emoji):', len(emoji)) \n",
    "#     print('len(sentence_embedd)', len(sentence_embedd))\n",
    "#     #print(sentence_embedd)\n",
    "    #user_loc_embedd.append(sentence_embedd)\n",
    "    user_loc_embedd_d[df.name[i]] = sentence_embedd\n",
    "     \n",
    "#print('len(user_loc_embedd)', len(user_loc_embedd))  #1308  \n",
    "# print(text[0])\n",
    "# print(emoji[0])\n",
    "#print(e2v['💙'])\n",
    "# print (user_loc_embedd_d['000mrs000'])\n",
    "# print (user_loc_embedd_d['1freespirityogi'])\n",
    "torch.save(user_loc_embedd_d, 'data/locationEmbeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### build word_to_ix and emoji_to_ix for user Description\n",
    "\n",
    "word_to_ix = build_vocab_using_word_embedding(token_des, w2v)\n",
    "# Adding a new key value pair for unknown word\n",
    "word_to_ix.update( {'my_unknown' : len(word_to_ix)} ) \n",
    "#print (word_to_ix, len(word_to_ix)) #3482\n",
    "\n",
    "word_embeddings = create_word_embeddings_w2v(weights_text)\n",
    "#print(word_embeddings) #Embedding(3000000, 300)\n",
    "\n",
    "\n",
    "emo_to_ix = build_vocab_using_word_embedding(token_des, e2v)\n",
    "# Adding a new key value pair for unknown emoji\n",
    "emo_to_ix.update( {'my_unknown' : len(emo_to_ix)} ) \n",
    "#print (emo_to_ix, len(emo_to_ix)) #132\n",
    "\n",
    "emoji_embeddings = create_word_embeddings_w2v(weights_emoji)\n",
    "#print(emoji_embeddings) #Embedding(1661, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare senquence including text and emoji for whole user description\n",
    "\n",
    "#user_des_embedd = []\n",
    "user_des_embedd_d = {}\n",
    "for i in range(len(token_des)):\n",
    "    #print (token_des[i])\n",
    "    sentence_embedd = []\n",
    "    #text =[]\n",
    "    #emoji = []\n",
    "    for j in range(len(token_des[i])):\n",
    "        #print (token_des[i][j])\n",
    "        if is_emoji(token_des[i][j]) == False:  \n",
    "            #print (token_des[i][j])\n",
    "            sequence = prepare_sequence_word_em(token_des[i][j], word_to_ix)\n",
    "            #print(sequence)\n",
    "            embeds = word_embeddings(sequence)\n",
    "            #text.append(embeds)\n",
    "            #text.append(torch.tensor(w2v[i])) #problem with OOV word\n",
    "            #build the sentence embedding\n",
    "            sentence_embedd.append(embeds)\n",
    "\n",
    "        if is_emoji(token_des[i][j]) == True:\n",
    "            #print (token_des[i][j])\n",
    "            sequence_emo = prepare_sequence_word_em(token_des[i][j], emo_to_ix)\n",
    "            #print(sequence)\n",
    "            embeds_emo = emoji_embeddings(sequence_emo)\n",
    "            #emoji.append(embeds_emo)\n",
    "            #emoji.append(torch.tensor(e2v[i])) #problem with OOV emoji\n",
    "            #build the sentence embedding\n",
    "            sentence_embedd.append(embeds_emo)\n",
    "#     print('len(text):', len(text)) \n",
    "#     print('len(emoji):', len(emoji)) \n",
    "#    print('len(sentence_embedd)', len(sentence_embedd))\n",
    "#     #print(sentence_embedd)\n",
    "    #user_des_embedd.append(sentence_embedd)\n",
    "    user_des_embedd_d[df.name[i]] = sentence_embedd\n",
    "#print('len(user_des_embedd)', len(user_des_embedd))  #1308\n",
    "torch.save(user_des_embedd_d, 'data/descriptionEmbeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def startTraining_des(sentence_in, dimensionsOrg):\n",
    "#     for i in range(len(sentence_in)):\n",
    "#         #print (train_lex[i])\n",
    "#         for j in range(len(sentence_in[i])):\n",
    "#             trainTuple= sentence_in[i][j].view(1, 300)\n",
    "#     #print(trainTuple.size()) #torch.Size([1, 300])\n",
    "#     #trainNeuralNet_option2(trainTuplesConcat,targetTuplesConcat,targetTuplesConcatIndex, dimensionsOrg, tag_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###start training description\n",
    "# dimensionsOrg = 300\n",
    "# #print(user_des_embedd[1], len(user_des_embedd[1])) #13\n",
    "# #print(user_des_embedd[1][0], user_des_embedd[1][0].size()) #torch.Size([1, 300])\n",
    "# #print(token_des[1], len(token_des[1]), token_des[1][0], len(token_des[1][0]))\n",
    "# startTraining_des(user_des_embedd, dimensionsOrg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Tweets of user (expensive)####\n",
    "\n",
    "#for user tweets\n",
    "proc_text = pre_processing_tweets(df['text'])\n",
    "#print( len(proc_text)) #1308\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1308\n"
     ]
    }
   ],
   "source": [
    "df_clean_text = pd.DataFrame({'clean': proc_text})\n",
    "token_text = [row.split() for row in df_clean_text['clean']]\n",
    "print(len(token_text)) #1308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### build word_to_ix and emoji_to_ix for user tweets\n",
    "\n",
    "word_to_ix = build_vocab_using_word_embedding(token_text, w2v)\n",
    "# Adding a new key value pair for unknown word\n",
    "word_to_ix.update( {'my_unknown' : len(word_to_ix)} ) \n",
    "#print (word_to_ix, len(word_to_ix)) #75781\n",
    "\n",
    "word_embeddings = create_word_embeddings_w2v(weights_text)\n",
    "#print(word_embeddings) #Embedding(3000000, 300)\n",
    "\n",
    "\n",
    "emo_to_ix = build_vocab_using_word_embedding(token_text, e2v)\n",
    "# Adding a new key value pair for unknown emoji\n",
    "emo_to_ix.update( {'my_unknown' : len(emo_to_ix)} ) \n",
    "#print (emo_to_ix, len(emo_to_ix)) #1409\n",
    "\n",
    "emoji_embeddings = create_word_embeddings_w2v(weights_emoji)\n",
    "#print(emoji_embeddings) #Embedding(1661, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare senquence including text and emoji for whole user tweets\n",
    "\n",
    "#user_text_embedd = []\n",
    "user_text_embedd_d = {}\n",
    "for i in range(len(token_text)):\n",
    "    #print (token_text[i])\n",
    "    sentence_embedd = []\n",
    "    #text =[]\n",
    "    #emoji = []\n",
    "    for j in range(len(token_text[i])):\n",
    "        #print (token_des[i][j])\n",
    "        if is_emoji(token_text[i][j]) == False:  \n",
    "            #print (token_text[i][j])\n",
    "            sequence = prepare_sequence_word_em(token_text[i][j], word_to_ix)\n",
    "            #print(sequence)\n",
    "            embeds = word_embeddings(sequence)\n",
    "            #text.append(embeds)\n",
    "            #text.append(torch.tensor(w2v[i])) #problem with OOV word\n",
    "            #build the sentence embedding\n",
    "            sentence_embedd.append(embeds)\n",
    "\n",
    "        if is_emoji(token_text[i][j]) == True:\n",
    "            #print (token_text[i][j])\n",
    "            sequence_emo = prepare_sequence_word_em(token_text[i][j], emo_to_ix)\n",
    "            #print(sequence)\n",
    "            embeds_emo = emoji_embeddings(sequence_emo)\n",
    "            #emoji.append(embeds_emo)\n",
    "            #emoji.append(torch.tensor(e2v[i])) #problem with OOV emoji\n",
    "            #build the sentence embedding\n",
    "            sentence_embedd.append(embeds_emo)\n",
    "#     print('len(text):', len(text)) \n",
    "#     print('len(emoji):', len(emoji)) \n",
    "#    print('len(sentence_embedd)', len(sentence_embedd))\n",
    "#     #print(sentence_embedd)\n",
    "    #user_text_embedd.append(sentence_embedd)\n",
    "    user_text_embedd_d[df.name[i]] = sentence_embedd\n",
    "    \n",
    "    \n",
    "#print('len(user_text_embedd)', len(user_text_embedd))  #1308\n",
    "torch.save(user_text_embedd_d, 'data/tweetsEmbeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
