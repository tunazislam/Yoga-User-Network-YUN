{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1210282d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## load embeddings #######\n",
    "#loc_emb = torch.load('data/locationEmbeddings.pt') \n",
    "#des_emb = torch.load('data/descriptionEmbeddings.pt') \n",
    "twt_emb = torch.load('data/tweetsEmbeddings.pt') \n",
    "\n",
    "#load network embedding\n",
    "#net_emb = gsm.KeyedVectors.load_word2vec_format('data/userNetworkEmd.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user = net_emb ['000mrs000']\n",
    "#print(user)\n",
    "#print(type(net_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 1300 user location, description, yoga tweets, utype, umotivation\n",
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "#print (df) #[1308 rows x 7 columns] name, location, description, text, utype, umotivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create BiLSTMAttention Model for Description\n",
    "class BiLSTMTwtAtt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMTwtAtt, self).__init__() \n",
    "        self.lstm = nn.LSTM(300, 150//2 , num_layers=1, bidirectional=True ) #BiLSTM with attention \n",
    "        #self.lstm = nn.LSTM(300, 150 , num_layers=1, bidirectional=False) #LSTM with attention\n",
    "        #self.fc2 = nn.Linear(150, 50)\n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "        self.attn_fc = torch.nn.Linear(300, 1) #attention layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)\n",
    "        return (torch.zeros(2 * 1, 1, 150//2), torch.zeros(2 * 1, 1, 150//2)) # <- change here: first dim of hidden needs to be doubled\n",
    "        #return (torch.zeros(1 * 1, 1, 150), torch.zeros(1 * 1, 1, 150))#LSTM with attention\n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size()) #torch.Size([13, 1, 150])\n",
    "        #rnn_out = rnn_out.squeeze(0).unsqueeze(1) \n",
    "        #rnn_out = rnn_out.permute(2,0,1) \n",
    "        rnn_out = rnn_out.permute(1,0,2) \n",
    "        #print(\"permute rnn_out\", rnn_out.size()) #torch.Size([150, 13, 1])\n",
    "        #print(\"state\", state.size()) #torch.Size([2, 1, 75])\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        #print(\"merged_state\", merged_state.size()) #torch.Size([1, 150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).size()) #torch.Size([150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).size()) #torch.Size([150, 1])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).unsqueeze(2).size()) # torch.Size([150, 1, 1])\n",
    "        #merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"merged_state2 :\", merged_state.size()) #torch.Size([150, 1, 1])\n",
    "        merged_state = merged_state.permute(1,0,2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([13, 300])\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        lstm_out, hidden = self.lstm(X.view(len(X),1, -1))\n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('hidden[0] = h_n', hidden[0], hidden[0].size()) # torch.Size([2, 1, 75])\n",
    "        #print('hidden[1] = c_n', hidden[1], hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        h_n, c_n = hidden\n",
    "        #print('h_n', h_n, h_n.size()) # torch.Size([2, 1, 75])\n",
    "        #print('c_n', c_n, hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        attn_out = self.attention(lstm_out, h_n)\n",
    "        #print(\"attn_out\", attn_out.size()) #torch.Size([150, 1])\n",
    "        #logits = self.fc2(attn_out)\n",
    "        #logits = self.fc2(attn_out.permute(1,0))\n",
    "        #print(\"logits\", logits, logits.size())\n",
    "        #return logits \n",
    "        return attn_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Twt, self).__init__()\n",
    "        #self.model_des = BiLSTMDesAtt()\n",
    "        #self.model_loc = LSTMLoc()\n",
    "        #self.model_twt = BiLSTMDesAtt()\n",
    "        self.model_twt = BiLSTMTwtAtt()\n",
    "        #self.model_net = NetworkMLP()\n",
    "\n",
    "        self.fc1 = nn.Linear(150, 200) \n",
    "        self.fc2 = nn.Linear(200, 3)\n",
    "    def forward(self, x_t): \n",
    "        #prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size())\n",
    "        #prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size())\n",
    "        prediction_twt = self.model_twt(x_t)\n",
    "        #prediction_net = self.model_net(x_n)\n",
    "        #concat_pred = torch.cat((prediction_des, prediction_loc, prediction_twt), 1) #concat with dim= 1\n",
    "        #print(concat_pred, concat_pred.size()) \n",
    "#         out = F.log_softmax(self.fc(concat_pred), dim = 1)\n",
    "#         return out\n",
    "        out = self.fc1(prediction_twt)\n",
    "        out = self.fc2(F.relu(out))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net input #########\n",
    "def nn_input(train_data,df):\n",
    "    #ground_truths = []\n",
    "    training_data_twt =[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                sent_tensor_twt = torch.stack(twt_emb[train_data[i]],dim = 1)\n",
    "                training_data_twt.append(sent_tensor_twt[-1])\n",
    "                break\n",
    "    return training_data_twt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Ground Truth #########\n",
    "def find_groundtruth(data, df):\n",
    "    ground_truths = []\n",
    "    for i in range (0, len(data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (data[i] == df.name[j]):\n",
    "                #print(data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #target_type = torch.tensor(utype, dtype=torch.long) #for user type\n",
    "                target_type = torch.tensor(umotivation, dtype=torch.long) #for user motivation\n",
    "                ground_truths.append(target_type)\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_tr(model, training_data_twt, ground_truths):\n",
    "    predictions =[]\n",
    "    for i in range (0,len(training_data_twt)):\n",
    "        prediction_joint = model(training_data_twt[i])\n",
    "        \n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_twt)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_val(model, training_data_twt, ground_truths):\n",
    "    predictions =[]\n",
    "    val_losses = []\n",
    "    loss_function = nn.NLLLoss()\n",
    "    for i in range (0,len(training_data_twt)):\n",
    "        prediction_joint = model(training_data_twt[i])\n",
    "        val_loss = loss_function(prediction_joint, ground_truths[i])\n",
    "        val_losses.append(val_loss.item())\n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_twt)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    return accuracy, macro_f1, val_losses\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########......prepare training and validation data\n",
    "# ground truth training\n",
    "train_gt = find_groundtruth(train_data, df)\n",
    "#####prepare training data for neural net #########\n",
    "#training_data_net =  nn_input_network(train_data,df)\n",
    "#print(training_data_net, len(training_data_net)) #ok\n",
    "training_data_twt =  nn_input(train_data,df)\n",
    "\n",
    "# ground truth validation\n",
    "valid_gt = find_groundtruth(valid_data, df)\n",
    "#####prepare validation data for neural net #########\n",
    "#validation_data_net =  nn_input_network(valid_data,df)\n",
    "validation_data_twt =  nn_input(valid_data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Starting with epoch:  0 ***********************\n",
      "epoch : 0 Train accuracy and macro_f1: 0.5759036144578313 0.38809115552673984\n",
      "epoch : 0 Validation accuracy, macro_f1: 0.5631067961165048 0.37134166602765634\n",
      "train loss per epoch 1.7741679922643914\n",
      "Validation loss per epoch: 1.470151523941929\n",
      "*************** Starting with epoch:  1 ***********************\n",
      "epoch : 1 Train accuracy and macro_f1: 0.6180722891566265 0.4203465851172273\n",
      "epoch : 1 Validation accuracy, macro_f1: 0.6262135922330098 0.42080994386527665\n",
      "train loss per epoch 1.5242395169045553\n",
      "Validation loss per epoch: 1.240496206052095\n",
      "*************** Starting with epoch:  2 ***********************\n",
      "epoch : 2 Train accuracy and macro_f1: 0.6433734939759036 0.4379579888495193\n",
      "epoch : 2 Validation accuracy, macro_f1: 0.6601941747572816 0.4441097976223343\n",
      "train loss per epoch 1.378089241234653\n",
      "Validation loss per epoch: 1.1529233513526547\n",
      "*************** Starting with epoch:  3 ***********************\n",
      "epoch : 3 Train accuracy and macro_f1: 0.6638554216867469 0.4535653879874248\n",
      "epoch : 3 Validation accuracy, macro_f1: 0.6650485436893204 0.44840912876894884\n",
      "train loss per epoch 1.2801683301981315\n",
      "Validation loss per epoch: 1.1249606644065635\n",
      "*************** Starting with epoch:  4 ***********************\n",
      "epoch : 4 Train accuracy and macro_f1: 0.6614457831325301 0.45187191947130945\n",
      "epoch : 4 Validation accuracy, macro_f1: 0.6747572815533981 0.45567517278043596\n",
      "train loss per epoch 1.2071028277780635\n",
      "Validation loss per epoch: 1.1177186615837431\n",
      "*************** Starting with epoch:  5 ***********************\n",
      "epoch : 5 Train accuracy and macro_f1: 0.6710843373493975 0.4585127858436198\n",
      "epoch : 5 Validation accuracy, macro_f1: 0.6747572815533981 0.45567517278043596\n",
      "train loss per epoch 1.1500536812805986\n",
      "Validation loss per epoch: 1.0814999909076877\n",
      "*************** Starting with epoch:  6 ***********************\n",
      "epoch : 6 Train accuracy and macro_f1: 0.6843373493975904 0.4681595316502012\n",
      "epoch : 6 Validation accuracy, macro_f1: 0.6990291262135923 0.4735029606322622\n",
      "train loss per epoch 1.1039111268685782\n",
      "Validation loss per epoch: 1.051728065731456\n",
      "*************** Starting with epoch:  7 ***********************\n",
      "epoch : 7 Train accuracy and macro_f1: 0.6987951807228916 0.4789880020865936\n",
      "epoch : 7 Validation accuracy, macro_f1: 0.6893203883495146 0.46642665011380097\n",
      "train loss per epoch 1.0652997746917767\n",
      "Validation loss per epoch: 1.0394563172916764\n",
      "*************** Starting with epoch:  8 ***********************\n",
      "epoch : 8 Train accuracy and macro_f1: 0.7108433734939759 0.48768585716010676\n",
      "epoch : 8 Validation accuracy, macro_f1: 0.6893203883495146 0.4661054497120071\n",
      "train loss per epoch 1.0319589695104034\n",
      "Validation loss per epoch: 1.0231914587174227\n",
      "*************** Starting with epoch:  9 ***********************\n",
      "epoch : 9 Train accuracy and macro_f1: 0.7120481927710843 0.48848864795300817\n",
      "epoch : 9 Validation accuracy, macro_f1: 0.6990291262135923 0.47195793078146026\n",
      "train loss per epoch 1.0028747477867157\n",
      "Validation loss per epoch: 1.0013995143105683\n",
      "*************** Starting with epoch:  10 ***********************\n",
      "epoch : 10 Train accuracy and macro_f1: 0.7253012048192771 0.4979294566530777\n",
      "epoch : 10 Validation accuracy, macro_f1: 0.7038834951456311 0.4753991528185077\n",
      "train loss per epoch 0.9771056213855809\n",
      "Validation loss per epoch: 1.0064083620074016\n",
      "*************** Starting with epoch:  11 ***********************\n",
      "epoch : 11 Train accuracy and macro_f1: 0.7325301204819277 0.5155500072564559\n",
      "epoch : 11 Validation accuracy, macro_f1: 0.7087378640776699 0.4788253200823709\n",
      "train loss per epoch 0.9542432855424774\n",
      "Validation loss per epoch: 0.9942374422748401\n",
      "*************** Starting with epoch:  12 ***********************\n",
      "epoch : 12 Train accuracy and macro_f1: 0.736144578313253 0.5181352457450946\n",
      "epoch : 12 Validation accuracy, macro_f1: 0.7087378640776699 0.4788253200823709\n",
      "train loss per epoch 0.9336234478608803\n",
      "Validation loss per epoch: 1.0112818693651737\n",
      "*************** Starting with epoch:  13 ***********************\n",
      "epoch : 13 Train accuracy and macro_f1: 0.7373493975903614 0.5189464849414014\n",
      "epoch : 13 Validation accuracy, macro_f1: 0.7087378640776699 0.4788253200823709\n",
      "train loss per epoch 0.9148307292642819\n",
      "Validation loss per epoch: 1.0180761628214596\n",
      "*************** Starting with epoch:  14 ***********************\n",
      "epoch : 14 Train accuracy and macro_f1: 0.7409638554216867 0.5215251212547377\n",
      "epoch : 14 Validation accuracy, macro_f1: 0.7087378640776699 0.4788253200823709\n",
      "train loss per epoch 0.8974597139467287\n",
      "Validation loss per epoch: 1.0062957129359824\n",
      "*************** Starting with epoch:  15 ***********************\n",
      "epoch : 15 Train accuracy and macro_f1: 0.7457831325301205 0.5250028615565219\n",
      "epoch : 15 Validation accuracy, macro_f1: 0.7087378640776699 0.4788253200823709\n",
      "train loss per epoch 0.8814101247468273\n",
      "Validation loss per epoch: 0.9987955799380552\n",
      "*************** Starting with epoch:  16 ***********************\n",
      "epoch : 16 Train accuracy and macro_f1: 0.7385542168674699 0.5197624867807423\n",
      "epoch : 16 Validation accuracy, macro_f1: 0.6990291262135923 0.4730548730548731\n",
      "train loss per epoch 0.8664623452168325\n",
      "Validation loss per epoch: 1.0282649572269575\n",
      "*************** Starting with epoch:  17 ***********************\n",
      "epoch : 17 Train accuracy and macro_f1: 0.7481927710843373 0.5501833939273569\n",
      "epoch : 17 Validation accuracy, macro_f1: 0.6941747572815534 0.4695881697160469\n",
      "train loss per epoch 0.8524541800995039\n",
      "Validation loss per epoch: 1.0214222442648075\n",
      "*************** Starting with epoch:  18 ***********************\n",
      "epoch : 18 Train accuracy and macro_f1: 0.7518072289156627 0.5411280032275311\n",
      "epoch : 18 Validation accuracy, macro_f1: 0.6941747572815534 0.4695881697160469\n",
      "train loss per epoch 0.8394647425252616\n",
      "Validation loss per epoch: 1.020590493256606\n",
      "*************** Starting with epoch:  19 ***********************\n",
      "epoch : 19 Train accuracy and macro_f1: 0.7530120481927711 0.5534208065965777\n",
      "epoch : 19 Validation accuracy, macro_f1: 0.6941747572815534 0.4695881697160469\n",
      "train loss per epoch 0.8272037010124733\n",
      "Validation loss per epoch: 1.0266770727353767\n",
      "*************** Starting with epoch:  20 ***********************\n",
      "epoch : 20 Train accuracy and macro_f1: 0.755421686746988 0.5547474216618465\n",
      "epoch : 20 Validation accuracy, macro_f1: 0.6941747572815534 0.4695881697160469\n",
      "train loss per epoch 0.8156864906633828\n",
      "Validation loss per epoch: 1.030933429062077\n",
      "*************** Starting with epoch:  21 ***********************\n",
      "epoch : 21 Train accuracy and macro_f1: 0.7578313253012048 0.5560899133360292\n",
      "epoch : 21 Validation accuracy, macro_f1: 0.6941747572815534 0.4694512694512694\n",
      "train loss per epoch 0.8048445883246679\n",
      "Validation loss per epoch: 1.0358258928095998\n",
      "*************** Starting with epoch:  22 ***********************\n",
      "epoch : 22 Train accuracy and macro_f1: 0.744578313253012 0.5354989568033285\n",
      "epoch : 22 Validation accuracy, macro_f1: 0.6844660194174758 0.4626061531891128\n",
      "train loss per epoch 0.7945888953789705\n",
      "Validation loss per epoch: 1.0631130456707432\n",
      "*************** Starting with epoch:  23 ***********************\n",
      "epoch : 23 Train accuracy and macro_f1: 0.7614457831325301 0.5586609738222248\n",
      "epoch : 23 Validation accuracy, macro_f1: 0.6941747572815534 0.4697542069371535\n",
      "train loss per epoch 0.7848327560227689\n",
      "Validation loss per epoch: 1.0374814992119532\n",
      "*************** Starting with epoch:  24 ***********************\n",
      "epoch : 24 Train accuracy and macro_f1: 0.7686746987951807 0.5848033464616159\n",
      "epoch : 24 Validation accuracy, macro_f1: 0.6941747572815534 0.5507299990135148\n",
      "train loss per epoch 0.7755082027466115\n",
      "Validation loss per epoch: 1.0373277985942957\n",
      "*************** Starting with epoch:  25 ***********************\n",
      "epoch : 25 Train accuracy and macro_f1: 0.7698795180722892 0.5856233637093529\n",
      "epoch : 25 Validation accuracy, macro_f1: 0.6893203883495146 0.5472438191616273\n",
      "train loss per epoch 0.7665941224894504\n",
      "Validation loss per epoch: 1.0535571398665604\n",
      "*************** Starting with epoch:  26 ***********************\n",
      "epoch : 26 Train accuracy and macro_f1: 0.7746987951807229 0.5890349050686089\n",
      "epoch : 26 Validation accuracy, macro_f1: 0.6941747572815534 0.5507299990135148\n",
      "train loss per epoch 0.7579999692655044\n",
      "Validation loss per epoch: 1.0492468236140835\n",
      "*************** Starting with epoch:  27 ***********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 27 Train accuracy and macro_f1: 0.7759036144578313 0.6095150264829566\n",
      "epoch : 27 Validation accuracy, macro_f1: 0.6990291262135923 0.5542008592293223\n",
      "train loss per epoch 0.7498145029251845\n",
      "Validation loss per epoch: 1.0533821439840696\n",
      "*************** Starting with epoch:  28 ***********************\n",
      "epoch : 28 Train accuracy and macro_f1: 0.7879518072289157 0.6272664449644351\n",
      "epoch : 28 Validation accuracy, macro_f1: 0.7038834951456311 0.5576569477278697\n",
      "train loss per epoch 0.7419737062320724\n",
      "Validation loss per epoch: 1.0476752794121655\n",
      "*************** Starting with epoch:  29 ***********************\n",
      "epoch : 29 Train accuracy and macro_f1: 0.7879518072289157 0.6272664449644351\n",
      "epoch : 29 Validation accuracy, macro_f1: 0.7184466019417476 0.5679419741251465\n",
      "train loss per epoch 0.7344238051393698\n",
      "Validation loss per epoch: 1.0589397962485414\n"
     ]
    }
   ],
   "source": [
    "###########.........Start Training...........\n",
    "model = Twt()\n",
    "##### Hyperparameter\n",
    "#learning_rate=0.05\n",
    "learning_rate=0.01\n",
    "epochs = 30\n",
    "#opt=\"ADAM\"\n",
    "#opt=\"SGD\" \n",
    "opt=\"ADA\"\n",
    "if(opt==\"SGD\"):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "elif(opt==\"ADA\"):\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-06, weight_decay=0.0001)\n",
    "elif(opt==\"ADAM\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "check_val_acc = 0\n",
    "losses = []\n",
    "per_epoch_train_loss =[]\n",
    "per_epoch_val_loss =[]\n",
    "per_epoch_train_f1 =[]\n",
    "per_epoch_val_f1 = []\n",
    "for epoch in range(epochs): \n",
    "    print('*************** Starting with epoch: ', epoch, '***********************')\n",
    "    for i in range (0,len(train_data)):\n",
    "        #model_des.zero_grad()\n",
    "        #model_loc.zero_grad()\n",
    "        model.zero_grad()\n",
    "        #####Run forward pass.\n",
    "      \n",
    "        prediction_joint = model(training_data_twt[i])\n",
    "        \n",
    "        #print(\"prediction_joint :\", torch.argmax(prediction_joint, dim=1)) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        #Compute the loss, gradients, and update the parameters by\n",
    "        #calling optimizer.step()\n",
    "        loss = loss_function(prediction_joint, train_gt[i])\n",
    "        #if (i%200 == 0):\n",
    "            #print (\"loss per example\", loss.item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward(retain_graph=True)  #backpropagation\n",
    "        optimizer.step()\n",
    "    accuracy, macro_f1 = make_prediction_tr(model, training_data_twt, train_gt)\n",
    "    print('epoch :', epoch, 'Train accuracy and macro_f1:', accuracy, macro_f1)\n",
    "    per_epoch_train_f1.append(macro_f1)\n",
    "    val_accuracy, val_macro_f1, val_loss = make_prediction_val(model, validation_data_twt, valid_gt)\n",
    "    per_epoch_val_f1.append(val_macro_f1)\n",
    "    print('epoch :', epoch, 'Validation accuracy, macro_f1:', val_accuracy, val_macro_f1)\n",
    "    per_epoch_train_loss.append(np.mean(losses))\n",
    "    print(\"train loss per epoch\", np.mean(losses))\n",
    "    per_epoch_val_loss.append(np.mean(val_loss))\n",
    "    print('Validation loss per epoch:', np.mean(val_loss))\n",
    "    torch.save(model.state_dict(),\"data/Twt_umotivation_2layer/Twt_\"+str(epoch)+\".pt\")\n",
    "#     if (check_val_acc < val_macro_f1): #early stopping\n",
    "#         check_val_acc = val_macro_f1\n",
    "#         print (\"Model saved at epoch :\", epoch)\n",
    "#         torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "#         best_epoch = epoch\n",
    "        \n",
    "#print(\"Best model found at epoch : \", best_epoch)        \n",
    "#torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_plots(train_losses, val_losses, train_accs, test_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('loss_Twt_umotivation.png')\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, test_accs, '-', linewidth=2, label='validation')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('accuracy_Twt_umotivation.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1)\n",
    "# print(per_epoch_train_loss)\n",
    "# print(per_epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction( training_data_twt, ground_truths):\n",
    "    for epoch in range(0,30):\n",
    "        model = Twt()\n",
    "        model.load_state_dict(torch.load(\"data/Twt_umotivation_2layer/Twt_\"+str(epoch)+\".pt\")) #best #Testing accuracy, macro_f1: 0.7862595419847328 0.7560951140518181\n",
    "        predictions =[]\n",
    "        for i in range (0,len(training_data_twt)):\n",
    "            prediction_joint = model( training_data_twt[i])\n",
    "            pred = torch.argmax(prediction_joint, dim=1)\n",
    "            predictions.append(pred.item())\n",
    "        #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "        accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_twt)\n",
    "        macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "        print('epoch :', epoch, 'Testing accuracy, macro_f1:', accuracy, macro_f1)\n",
    "        #return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare testing data for neural net #########\n",
    "testing_data_twt =  nn_input(test_data,df)\n",
    "#testing_data_net =  nn_input_network(test_data,df)\n",
    "test_gt = find_groundtruth(test_data, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Testing accuracy, macro_f1: 0.648854961832061 0.42165608125787957\n",
      "epoch : 1 Testing accuracy, macro_f1: 0.6870229007633588 0.45839355771216184\n",
      "epoch : 2 Testing accuracy, macro_f1: 0.6793893129770993 0.4510078260078261\n",
      "epoch : 3 Testing accuracy, macro_f1: 0.6870229007633588 0.45867387143982885\n",
      "epoch : 4 Testing accuracy, macro_f1: 0.7061068702290076 0.4725805541448\n",
      "epoch : 5 Testing accuracy, macro_f1: 0.7137404580152672 0.47991394506676577\n",
      "epoch : 6 Testing accuracy, macro_f1: 0.7099236641221374 0.47704077343421597\n",
      "epoch : 7 Testing accuracy, macro_f1: 0.7175572519083969 0.48469960066406426\n",
      "epoch : 8 Testing accuracy, macro_f1: 0.7251908396946565 0.48915293001980303\n",
      "epoch : 9 Testing accuracy, macro_f1: 0.7366412213740458 0.4986258865248227\n",
      "epoch : 10 Testing accuracy, macro_f1: 0.7442748091603053 0.5048438706829967\n",
      "epoch : 11 Testing accuracy, macro_f1: 0.7442748091603053 0.5048438706829967\n",
      "epoch : 12 Testing accuracy, macro_f1: 0.7442748091603053 0.5048438706829967\n",
      "epoch : 13 Testing accuracy, macro_f1: 0.7404580152671756 0.5023315220682555\n",
      "epoch : 14 Testing accuracy, macro_f1: 0.7442748091603053 0.5059426094950424\n",
      "epoch : 15 Testing accuracy, macro_f1: 0.7404580152671756 0.5028922334621817\n",
      "epoch : 16 Testing accuracy, macro_f1: 0.7442748091603053 0.5059426094950424\n",
      "epoch : 17 Testing accuracy, macro_f1: 0.7290076335877863 0.4952920275371975\n",
      "epoch : 18 Testing accuracy, macro_f1: 0.7366412213740458 0.5015038924274594\n",
      "epoch : 19 Testing accuracy, macro_f1: 0.732824427480916 0.49840715809302205\n",
      "epoch : 20 Testing accuracy, macro_f1: 0.7251908396946565 0.492989976112339\n",
      "epoch : 21 Testing accuracy, macro_f1: 0.7290076335877863 0.4961292119186857\n",
      "epoch : 22 Testing accuracy, macro_f1: 0.7290076335877863 0.4954806810300467\n",
      "epoch : 23 Testing accuracy, macro_f1: 0.7251908396946565 0.492989976112339\n",
      "epoch : 24 Testing accuracy, macro_f1: 0.7290076335877863 0.4961292119186857\n",
      "epoch : 25 Testing accuracy, macro_f1: 0.7290076335877863 0.4961292119186857\n",
      "epoch : 26 Testing accuracy, macro_f1: 0.7251908396946565 0.49363613121241423\n",
      "epoch : 27 Testing accuracy, macro_f1: 0.7290076335877863 0.4967496450124245\n",
      "epoch : 28 Testing accuracy, macro_f1: 0.7290076335877863 0.4967496450124245\n",
      "epoch : 29 Testing accuracy, macro_f1: 0.7290076335877863 0.4967496450124245\n"
     ]
    }
   ],
   "source": [
    "make_prediction(testing_data_twt, test_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
