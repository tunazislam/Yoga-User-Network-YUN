{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12bba59d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p  #pip install tweet-preprocessor\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation as punc\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.models as gsm\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import regex\n",
    "import emoji\n",
    "# Internal dependencies\n",
    "import word_emoji2vec as we2v\n",
    "#from word_emoji2vec import Word_Emoji2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed #python -m spacy download en\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## load embeddings #######\n",
    "loc_emb = torch.load('data/locationEmbeddings.pt') \n",
    "des_emb = torch.load('data/descriptionEmbeddings.pt') \n",
    "#twt_emb = torch.load('data/tweetsEmbeddings.pt') \n",
    "\n",
    "#load network embedding\n",
    "#net_emb = gsm.KeyedVectors.load_word2vec_format('data/userNetworkEmd.emd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user = net_emb ['000mrs000']\n",
    "#print(user)\n",
    "#print(type(net_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 1300 user location, description, yoga tweets, utype, umotivation\n",
    "df = pd.read_csv(\"data/yoga_user_name_loc_des_mergetweets_yoga_1300_lb.csv\") \n",
    "#print (df) #[1308 rows x 7 columns] name, location, description, text, utype, umotivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load train users and split into train and validation #######\n",
    "with open(\"data/train.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "random.seed(1)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:830] #80% train  \n",
    "#print(train_data, len(train_data)) #830\n",
    "valid_data = data[830:] #20% validation\n",
    "#print(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create BiLSTMAttention Model for Description\n",
    "class BiLSTMDesAtt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTMDesAtt, self).__init__() \n",
    "        self.lstm = nn.LSTM(300, 150//2 , num_layers=1, bidirectional=True ) #BiLSTM with attention \n",
    "        #self.lstm = nn.LSTM(300, 150 , num_layers=1, bidirectional=False) #LSTM with attention\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "        self.attn_fc = torch.nn.Linear(300, 1) #attention layer\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)\n",
    "        return (torch.zeros(2 * 1, 1, 150//2), torch.zeros(2 * 1, 1, 150//2)) # <- change here: first dim of hidden needs to be doubled\n",
    "        #return (torch.zeros(1 * 1, 1, 150), torch.zeros(1 * 1, 1, 150))#LSTM with attention\n",
    "    def attention(self, rnn_out, state):\n",
    "        #print(\"rnn_out\", rnn_out.size()) #torch.Size([13, 1, 150])\n",
    "        #rnn_out = rnn_out.squeeze(0).unsqueeze(1) \n",
    "        #rnn_out = rnn_out.permute(2,0,1) \n",
    "        rnn_out = rnn_out.permute(1,0,2) \n",
    "        #print(\"permute rnn_out\", rnn_out.size()) #torch.Size([150, 13, 1])\n",
    "        #print(\"state\", state.size()) #torch.Size([2, 1, 75])\n",
    "        merged_state = torch.cat([s for s in state],1)\n",
    "        #print(\"merged_state\", merged_state.size()) #torch.Size([1, 150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).size()) #torch.Size([150])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).size()) #torch.Size([150, 1])\n",
    "        #print(\"merged_state2 :\", merged_state.squeeze(0).unsqueeze(1).unsqueeze(2).size()) # torch.Size([150, 1, 1])\n",
    "        #merged_state = merged_state.squeeze(0).unsqueeze(2)\n",
    "        merged_state = merged_state.squeeze(0).unsqueeze(1).unsqueeze(2)\n",
    "        #print(\"merged_state2 :\", merged_state.size()) #torch.Size([150, 1, 1])\n",
    "        merged_state = merged_state.permute(1,0,2)\n",
    "        # (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)\n",
    "        weights = torch.bmm(rnn_out, merged_state)\n",
    "        #print(\"weights\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        #weights = torch.nn.functional.softmax(weights.squeeze(2)).unsqueeze(2)\n",
    "        weights = F.log_softmax(weights.squeeze(2),dim = 1).unsqueeze(2)\n",
    "         #F.log_softmax(x, dim = 1)\n",
    "        #print(\"weights2 :\", weights.size()) #torch.Size([150, 13, 1])\n",
    "        # (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)\n",
    "        return torch.bmm(torch.transpose(rnn_out, 1, 2), weights).squeeze(2)\n",
    "    # end method attention\n",
    "\n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([13, 300])\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        lstm_out, hidden = self.lstm(X.view(len(X),1, -1))\n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('hidden[0] = h_n', hidden[0], hidden[0].size()) # torch.Size([2, 1, 75])\n",
    "        #print('hidden[1] = c_n', hidden[1], hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        h_n, c_n = hidden\n",
    "        #print('h_n', h_n, h_n.size()) # torch.Size([2, 1, 75])\n",
    "        #print('c_n', c_n, hidden[1].size()) # torch.Size([2, 1, 75])\n",
    "        attn_out = self.attention(lstm_out, h_n)\n",
    "        #print(\"attn_out\", attn_out.size()) #torch.Size([150, 1])\n",
    "        #logits = self.fc2(attn_out)\n",
    "        #logits = self.fc2(attn_out.permute(1,0))\n",
    "        #print(\"logits\", logits, logits.size())\n",
    "        #return logits \n",
    "        return attn_out\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create LSTM Model for Location #############\n",
    "class LSTMLoc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMLoc, self).__init__()\n",
    "        self.lstm = nn.LSTM(300, 150, num_layers=1)\n",
    "        self.fc2 = nn.Linear(150, 50) \n",
    "        self.hidden = self.init_hidden() # <- change here \n",
    "\n",
    "    def init_hidden(self):\n",
    "        #(2*self.num_layers, batch_size, self.hidden_dim // 2)# <- change here: first dim of hidden needs to be doubled\n",
    "        return (torch.zeros(1, 1, 150), torch.zeros(1, 1, 150)) \n",
    "    def forward(self, x):\n",
    "        #x=embeds.permute(1,0,2)\n",
    "        #print('resize', x.view(len(x),1,-1), x.view(len(x),1,-1).size()) #torch.Size([13, 1, 300])\n",
    "        #lstm_out, self.hidden = self.lstm(x.view(len(x),1, -1), self.hidden)\n",
    "        lstm_out, _ = self.lstm(x.view(len(x),1, -1))\n",
    "        #lstm_out, _ = self.lstm(x.view(len(x),1,-1)) \n",
    "        #print('lstm_out', lstm_out, lstm_out.size()) # torch.Size([13, 1, 150])\n",
    "        #print('self.hidden[0]', self.hidden[0], self.hidden[0].size()) # torch.Size([1, 1, 150])\n",
    "        #print(\"lstm_out[-1]\", lstm_out[-1], lstm_out[-1].size())  # torch.Size([1, 150])\n",
    "        #x = self.fc2(lstm_out[-1])  \n",
    "        #out = F.log_softmax(x, dim = 1)\n",
    "        #return out\n",
    "        #return x\n",
    "        return lstm_out[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Create Model \n",
    "class NetworkMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkMLP, self).__init__() \n",
    "        self.fc1 = nn.Linear(300, 150)\n",
    "        \n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        \n",
    "        #self.fc1 = nn.Linear(300, 50)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.size()) # torch.Size([300])\n",
    "        #print('resize', X.view(1,len(X)).size()) #resize torch.Size([1, 300])\n",
    "        z1 = self.fc1(X.view(1,len(X)))\n",
    "        #print('z1', z1, z1.size()) # torch.Size([1, 150])\n",
    "        h1 = F.relu(z1) \n",
    "        #logits = self.fc2(h1) #without attention\n",
    "        #print(\"logits\", logits, logits.size()) #torch.Size([1, 3])\n",
    "        #return logits \n",
    "        return h1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointDL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JointDL, self).__init__()\n",
    "        self.model_des = BiLSTMDesAtt()\n",
    "        self.model_loc = LSTMLoc()\n",
    "        #self.model_net = NetworkMLP()\n",
    "        self.fc1 = nn.Linear(300, 200) #2*150 = 300 \n",
    "        self.fc2 = nn.Linear(200, 3) #if we use two-layer classifier\n",
    "#        self.fc1 = nn.Linear(300, 3) #2*150 = 300\n",
    "    def forward(self, x_d, x_l): \n",
    "        prediction_des = self.model_des(x_d)\n",
    "        #print(prediction_des, prediction_des.size()) #torch.Size([1, 3])\n",
    "        prediction_loc = self.model_loc(x_l)\n",
    "        #print(prediction_loc, prediction_loc.size()) #torch.Size([1, 3])\n",
    "        #prediction_net = self.model_net(x_n)\n",
    "        #print(prediction_net, prediction_net.size()) #torch.Size([1, 3])\n",
    "        #concat_pred = torch.cat((prediction_des, prediction_loc, prediction_net), 1) #concat with dim= 1\n",
    "        concat_pred = torch.cat((prediction_des, prediction_loc), 1) #concat with dim= 1\n",
    "        #print(concat_pred, concat_pred.size()) #torch.Size([1, 6])\n",
    "        out = self.fc1(concat_pred)\n",
    "        out = self.fc2(F.relu(out))\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net input #########\n",
    "def nn_input(train_data,df):\n",
    "    #ground_truths = []\n",
    "    training_data_des =[]\n",
    "    training_data_loc=[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                if (not des_emb[train_data[i]]) and (not loc_emb[train_data[i]]):\n",
    "                    print ('no description and location for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    training_data_loc.append(torch.zeros(1, 300))\n",
    "                    break\n",
    "                \n",
    "                elif (des_emb[train_data[i]]) and (not loc_emb[train_data[i]]): \n",
    "                    print ('no location for user: ', train_data[i])\n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    training_data_loc.append(torch.zeros(1, 300))\n",
    "                    break\n",
    "                    \n",
    "                elif (not des_emb[train_data[i]]) and (loc_emb[train_data[i]]): \n",
    "                    print ('no description for user: ', train_data[i])\n",
    "                    training_data_des.append(torch.zeros(1, 300))\n",
    "                    sent_tensor_loc = torch.stack(loc_emb[train_data[i]],dim = 1)\n",
    "                    training_data_loc.append(sent_tensor_loc[-1])\n",
    "                    break    \n",
    "               \n",
    "                else:\n",
    "                    sent_tensor_des = torch.stack(des_emb[train_data[i]],dim = 1)\n",
    "                    training_data_des.append(sent_tensor_des[-1])\n",
    "                    sent_tensor_loc = torch.stack(loc_emb[train_data[i]],dim = 1)\n",
    "                    training_data_loc.append(sent_tensor_loc[-1])\n",
    "                    break\n",
    "    return training_data_des, training_data_loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare data for neural net #########\n",
    "def nn_input_network(train_data,df):\n",
    "    training_data =[]\n",
    "    for i in range (0, len(train_data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (train_data[i] == df.name[j]):\n",
    "                #print(train_data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                #print (\"net_emb[train_data[i]] : \", net_emb[train_data[i]], type(net_emb[train_data[i]]), torch.Tensor(net_emb[train_data[i]]), type(torch.Tensor(net_emb[train_data[i]])))\n",
    "                #count = 0\n",
    "                if(train_data[i] not in net_emb ):\n",
    "                    net_emb[train_data[i]] = np.zeros(300) #For users not appearing in the mention network, we set their network embedding vectors as 0.\n",
    "                    #count = count + 1\n",
    "                #print(count)\n",
    "                #print(net_emb[train_data[i]]) #ok\n",
    "                ####.....convert ndarray to torch.tensor........\n",
    "                net_emb_tensor = torch.Tensor(net_emb[train_data[i]])\n",
    "                #print(net_emb_tensor) #ok\n",
    "                training_data.append(net_emb_tensor)\n",
    "                break\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Ground Truth #########\n",
    "def find_groundtruth(data, df):\n",
    "    ground_truths = []\n",
    "    for i in range (0, len(data)):\n",
    "    #for i in range (0, 10):\n",
    "        for j in range (0,df.shape[0]):\n",
    "            if (data[i] == df.name[j]):\n",
    "                #print(data[i]) #print username\n",
    "                utype =  [int(df.utype[j])]\n",
    "                umotivation = [int(float(df.umotivation[j]))]\n",
    "                target_type = torch.tensor(utype, dtype=torch.long) #for user type\n",
    "                #target_type = torch.tensor(umotivation, dtype=torch.long) #for user motivation\n",
    "                ground_truths.append(target_type)\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_tr(model, training_data_des, training_data_loc, ground_truths):\n",
    "    predictions =[]\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i])\n",
    "        \n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    return accuracy, macro_f1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction_val(model, training_data_des, training_data_loc, ground_truths):\n",
    "    predictions =[]\n",
    "    val_losses = []\n",
    "    loss_function = nn.NLLLoss()\n",
    "    for i in range (0,len(training_data_des)):\n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i])\n",
    "        val_loss = loss_function(prediction_joint, ground_truths[i])\n",
    "        val_losses.append(val_loss.item())\n",
    "        #prediction = model(data[i])\n",
    "        pred = torch.argmax(prediction_joint, dim=1)\n",
    "        #print(\"pred :\", pred) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        predictions.append(pred.item())\n",
    "    #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "    macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "    \n",
    "    #print(type(predictions), type(ground_truths))\n",
    "    #print(\"predictions\", predictions)\n",
    "    #print(\"ground_truths\", ground_truths)\n",
    "    \n",
    "    return accuracy, macro_f1, val_losses\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no description for user:  bchi49\n",
      "no location for user:  Christoph_Tran\n",
      "no description for user:  viecestlavie\n",
      "no description for user:  crystalization_\n",
      "no location for user:  yogitimesonline\n",
      "no description for user:  mimmosamami\n",
      "no description for user:  wenmarbyoga\n",
      "no location for user:  cipherEquality\n",
      "no description for user:  YogaLifeLine\n",
      "no location for user:  thewaywecame\n"
     ]
    }
   ],
   "source": [
    "##########......prepare training and validation data\n",
    "# ground truth training\n",
    "train_gt = find_groundtruth(train_data, df)\n",
    "#####prepare training data for neural net #########\n",
    "#training_data_net =  nn_input_network(train_data,df)\n",
    "#print(training_data_net, len(training_data_net)) #ok\n",
    "training_data_des, training_data_loc =  nn_input(train_data,df)\n",
    "\n",
    "# ground truth validation\n",
    "valid_gt = find_groundtruth(valid_data, df)\n",
    "#####prepare validation data for neural net #########\n",
    "#validation_data_net =  nn_input_network(valid_data,df)\n",
    "validation_data_des, validation_data_loc =  nn_input(valid_data,df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Starting with epoch:  0 ***********************\n",
      "epoch : 0 Train accuracy and macro_f1: 0.5289156626506024 0.4332092072662161\n",
      "epoch : 0 Validation accuracy, macro_f1: 0.5339805825242718 0.43213850047880736\n",
      "train loss per epoch 1.0465757748269173\n",
      "Validation loss per epoch: 0.9675794783437136\n",
      "*************** Starting with epoch:  1 ***********************\n",
      "epoch : 1 Train accuracy and macro_f1: 0.6120481927710844 0.5273250344025384\n",
      "epoch : 1 Validation accuracy, macro_f1: 0.6504854368932039 0.5646468147653945\n",
      "train loss per epoch 1.0064868782119578\n",
      "Validation loss per epoch: 0.904814008225515\n",
      "*************** Starting with epoch:  2 ***********************\n",
      "epoch : 2 Train accuracy and macro_f1: 0.6590361445783133 0.6139934072662275\n",
      "epoch : 2 Validation accuracy, macro_f1: 0.6699029126213593 0.6215747086827227\n",
      "train loss per epoch 0.9738140764485401\n",
      "Validation loss per epoch: 0.8544044110526159\n",
      "*************** Starting with epoch:  3 ***********************\n",
      "epoch : 3 Train accuracy and macro_f1: 0.6710843373493975 0.6423179224578276\n",
      "epoch : 3 Validation accuracy, macro_f1: 0.6893203883495146 0.66110112081119\n",
      "train loss per epoch 0.9450176917016506\n",
      "Validation loss per epoch: 0.8116254540323053\n",
      "*************** Starting with epoch:  4 ***********************\n",
      "epoch : 4 Train accuracy and macro_f1: 0.6963855421686747 0.678138280271024\n",
      "epoch : 4 Validation accuracy, macro_f1: 0.6990291262135923 0.6830700543961811\n",
      "train loss per epoch 0.9190304800939847\n",
      "Validation loss per epoch: 0.7756813295836588\n",
      "*************** Starting with epoch:  5 ***********************\n",
      "epoch : 5 Train accuracy and macro_f1: 0.7096385542168675 0.6986450877066015\n",
      "epoch : 5 Validation accuracy, macro_f1: 0.6941747572815534 0.6818149587168113\n",
      "train loss per epoch 0.8953742652503123\n",
      "Validation loss per epoch: 0.7452648785507795\n",
      "*************** Starting with epoch:  6 ***********************\n",
      "epoch : 6 Train accuracy and macro_f1: 0.7156626506024096 0.7078151801844824\n",
      "epoch : 6 Validation accuracy, macro_f1: 0.6990291262135923 0.6900143678160919\n",
      "train loss per epoch 0.8736448739615241\n",
      "Validation loss per epoch: 0.7199699629857702\n",
      "*************** Starting with epoch:  7 ***********************\n",
      "epoch : 7 Train accuracy and macro_f1: 0.7265060240963855 0.7189162554612268\n",
      "epoch : 7 Validation accuracy, macro_f1: 0.6893203883495146 0.6782621726487004\n",
      "train loss per epoch 0.8536004760201616\n",
      "Validation loss per epoch: 0.7009031801547819\n",
      "*************** Starting with epoch:  8 ***********************\n",
      "epoch : 8 Train accuracy and macro_f1: 0.7325301204819277 0.7252852246949336\n",
      "epoch : 8 Validation accuracy, macro_f1: 0.6893203883495146 0.6782621726487004\n",
      "train loss per epoch 0.8351514200010453\n",
      "Validation loss per epoch: 0.6864456845429338\n",
      "*************** Starting with epoch:  9 ***********************\n",
      "epoch : 9 Train accuracy and macro_f1: 0.7349397590361446 0.7286381412832353\n",
      "epoch : 9 Validation accuracy, macro_f1: 0.6893203883495146 0.6784025223331582\n",
      "train loss per epoch 0.818191743987691\n",
      "Validation loss per epoch: 0.6764323451709979\n",
      "*************** Starting with epoch:  10 ***********************\n",
      "epoch : 10 Train accuracy and macro_f1: 0.7397590361445783 0.734567584784697\n",
      "epoch : 10 Validation accuracy, macro_f1: 0.6941747572815534 0.6903087671850349\n",
      "train loss per epoch 0.8026130166545363\n",
      "Validation loss per epoch: 0.6699489997790277\n",
      "*************** Starting with epoch:  11 ***********************\n",
      "epoch : 11 Train accuracy and macro_f1: 0.7421686746987952 0.737203368646293\n",
      "epoch : 11 Validation accuracy, macro_f1: 0.6941747572815534 0.6902342119512191\n",
      "train loss per epoch 0.7882852128590925\n",
      "Validation loss per epoch: 0.6661493482737286\n",
      "*************** Starting with epoch:  12 ***********************\n",
      "epoch : 12 Train accuracy and macro_f1: 0.7481927710843373 0.7435845343578499\n",
      "epoch : 12 Validation accuracy, macro_f1: 0.6990291262135923 0.6944726263786133\n",
      "train loss per epoch 0.7750568055382915\n",
      "Validation loss per epoch: 0.6644934494301532\n",
      "*************** Starting with epoch:  13 ***********************\n",
      "epoch : 13 Train accuracy and macro_f1: 0.755421686746988 0.7514608074321872\n",
      "epoch : 13 Validation accuracy, macro_f1: 0.6990291262135923 0.6928104575163397\n",
      "train loss per epoch 0.7627967810293534\n",
      "Validation loss per epoch: 0.6644098809332524\n",
      "*************** Starting with epoch:  14 ***********************\n",
      "epoch : 14 Train accuracy and macro_f1: 0.7590361445783133 0.7546118915850717\n",
      "epoch : 14 Validation accuracy, macro_f1: 0.6990291262135923 0.6895889972813048\n",
      "train loss per epoch 0.7513980013263752\n",
      "Validation loss per epoch: 0.6653260684852461\n",
      "*************** Starting with epoch:  15 ***********************\n",
      "epoch : 15 Train accuracy and macro_f1: 0.763855421686747 0.7593512369296519\n",
      "epoch : 15 Validation accuracy, macro_f1: 0.7087378640776699 0.6997129383795376\n",
      "train loss per epoch 0.7407411407394593\n",
      "Validation loss per epoch: 0.6673470952195449\n",
      "*************** Starting with epoch:  16 ***********************\n",
      "epoch : 16 Train accuracy and macro_f1: 0.772289156626506 0.7665083467693522\n",
      "epoch : 16 Validation accuracy, macro_f1: 0.7184466019417476 0.7079121123892257\n",
      "train loss per epoch 0.7307476181847133\n",
      "Validation loss per epoch: 0.6696766076374401\n",
      "*************** Starting with epoch:  17 ***********************\n",
      "epoch : 17 Train accuracy and macro_f1: 0.7783132530120482 0.7726011916834233\n",
      "epoch : 17 Validation accuracy, macro_f1: 0.7135922330097088 0.7058796344436358\n",
      "train loss per epoch 0.7213520182547231\n",
      "Validation loss per epoch: 0.672737582758503\n",
      "*************** Starting with epoch:  18 ***********************\n",
      "epoch : 18 Train accuracy and macro_f1: 0.7843373493975904 0.7783902722332252\n",
      "epoch : 18 Validation accuracy, macro_f1: 0.7135922330097088 0.7058796344436358\n",
      "train loss per epoch 0.7124869001334794\n",
      "Validation loss per epoch: 0.6762271801064026\n",
      "*************** Starting with epoch:  19 ***********************\n",
      "epoch : 19 Train accuracy and macro_f1: 0.7879518072289157 0.7821503057852907\n",
      "epoch : 19 Validation accuracy, macro_f1: 0.7184466019417476 0.7096461127863852\n",
      "train loss per epoch 0.7041017903772419\n",
      "Validation loss per epoch: 0.6804795344767062\n",
      "*************** Starting with epoch:  20 ***********************\n",
      "epoch : 20 Train accuracy and macro_f1: 0.7927710843373494 0.7878629990880469\n",
      "epoch : 20 Validation accuracy, macro_f1: 0.7184466019417476 0.7096461127863852\n",
      "train loss per epoch 0.6961447728794588\n",
      "Validation loss per epoch: 0.684758263305553\n",
      "*************** Starting with epoch:  21 ***********************\n",
      "epoch : 21 Train accuracy and macro_f1: 0.8 0.7952031352962883\n",
      "epoch : 21 Validation accuracy, macro_f1: 0.7135922330097088 0.705556133236283\n",
      "train loss per epoch 0.6885732077831261\n",
      "Validation loss per epoch: 0.6896552253623032\n",
      "*************** Starting with epoch:  22 ***********************\n",
      "epoch : 22 Train accuracy and macro_f1: 0.8 0.7952031352962883\n",
      "epoch : 22 Validation accuracy, macro_f1: 0.7184466019417476 0.7131082318292469\n",
      "train loss per epoch 0.6813517073445479\n",
      "Validation loss per epoch: 0.6947159545080176\n",
      "*************** Starting with epoch:  23 ***********************\n",
      "epoch : 23 Train accuracy and macro_f1: 0.8 0.7952699151745956\n",
      "epoch : 23 Validation accuracy, macro_f1: 0.7184466019417476 0.7113348741941224\n",
      "train loss per epoch 0.6744479198849397\n",
      "Validation loss per epoch: 0.7000300218758074\n",
      "*************** Starting with epoch:  24 ***********************\n",
      "epoch : 24 Train accuracy and macro_f1: 0.8048192771084337 0.800672286924096\n",
      "epoch : 24 Validation accuracy, macro_f1: 0.7135922330097088 0.7091307814992026\n",
      "train loss per epoch 0.667835529081703\n",
      "Validation loss per epoch: 0.7056849565610145\n",
      "*************** Starting with epoch:  25 ***********************\n",
      "epoch : 25 Train accuracy and macro_f1: 0.8096385542168675 0.8059154923558527\n",
      "epoch : 25 Validation accuracy, macro_f1: 0.7184466019417476 0.7131641597330926\n",
      "train loss per epoch 0.6614864137704701\n",
      "Validation loss per epoch: 0.7116144022280441\n",
      "*************** Starting with epoch:  26 ***********************\n",
      "epoch : 26 Train accuracy and macro_f1: 0.8180722891566266 0.8152660875052113\n",
      "epoch : 26 Validation accuracy, macro_f1: 0.7184466019417476 0.7131641597330926\n",
      "train loss per epoch 0.6553783761490334\n",
      "Validation loss per epoch: 0.7176393601338117\n",
      "*************** Starting with epoch:  27 ***********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 27 Train accuracy and macro_f1: 0.8180722891566266 0.8148539652992812\n",
      "epoch : 27 Validation accuracy, macro_f1: 0.7184466019417476 0.7131641597330926\n",
      "train loss per epoch 0.6494899701374373\n",
      "Validation loss per epoch: 0.7241526841393953\n",
      "*************** Starting with epoch:  28 ***********************\n",
      "epoch : 28 Train accuracy and macro_f1: 0.8204819277108434 0.8174745784973082\n",
      "epoch : 28 Validation accuracy, macro_f1: 0.7184466019417476 0.7151881447956691\n",
      "train loss per epoch 0.6438053856526804\n",
      "Validation loss per epoch: 0.7310256192153229\n",
      "*************** Starting with epoch:  29 ***********************\n",
      "epoch : 29 Train accuracy and macro_f1: 0.8240963855421687 0.82104295058158\n",
      "epoch : 29 Validation accuracy, macro_f1: 0.7233009708737864 0.7222164000528984\n",
      "train loss per epoch 0.638306358192475\n",
      "Validation loss per epoch: 0.7375626009934156\n"
     ]
    }
   ],
   "source": [
    "###########.........Start Training...........\n",
    "model = JointDL()\n",
    "##### Hyperparameter\n",
    "#learning_rate=0.05\n",
    "learning_rate=0.01\n",
    "epochs = 30\n",
    "#opt=\"ADAM\"\n",
    "#opt=\"SGD\" \n",
    "opt=\"ADA\"\n",
    "if(opt==\"SGD\"):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "elif(opt==\"ADA\"):\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-06, weight_decay=0.0001)\n",
    "elif(opt==\"ADAM\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "    \n",
    "loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "check_val_acc = 0\n",
    "losses = []\n",
    "per_epoch_train_loss =[]\n",
    "per_epoch_val_loss =[]\n",
    "per_epoch_train_f1 =[]\n",
    "per_epoch_val_f1 = []\n",
    "for epoch in range(epochs): \n",
    "    print('*************** Starting with epoch: ', epoch, '***********************')\n",
    "    for i in range (0,len(train_data)):\n",
    "        #model_des.zero_grad()\n",
    "        #model_loc.zero_grad()\n",
    "        model.zero_grad()\n",
    "        #####Run forward pass.\n",
    "      \n",
    "        prediction_joint = model(training_data_des[i], training_data_loc[i])\n",
    "        \n",
    "        #print(\"prediction_joint :\", torch.argmax(prediction_joint, dim=1)) #ok\n",
    "        #print(\"ground_truths :\", ground_truths[i]) #ok\n",
    "        #Compute the loss, gradients, and update the parameters by\n",
    "        #calling optimizer.step()\n",
    "        loss = loss_function(prediction_joint, train_gt[i])\n",
    "        #if (i%200 == 0):\n",
    "            #print (\"loss per example\", loss.item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward(retain_graph=True)  #backpropagation\n",
    "        optimizer.step()\n",
    "    accuracy, macro_f1 = make_prediction_tr(model, training_data_des, training_data_loc, train_gt)\n",
    "    print('epoch :', epoch, 'Train accuracy and macro_f1:', accuracy, macro_f1)\n",
    "    per_epoch_train_f1.append(macro_f1)\n",
    "    val_accuracy, val_macro_f1, val_loss = make_prediction_val(model, validation_data_des, validation_data_loc, valid_gt)\n",
    "    per_epoch_val_f1.append(val_macro_f1)\n",
    "    print('epoch :', epoch, 'Validation accuracy, macro_f1:', val_accuracy, val_macro_f1)\n",
    "    per_epoch_train_loss.append(np.mean(losses))\n",
    "    print(\"train loss per epoch\", np.mean(losses))\n",
    "    per_epoch_val_loss.append(np.mean(val_loss))\n",
    "    print('Validation loss per epoch:', np.mean(val_loss))\n",
    "    \n",
    "    torch.save(model.state_dict(),\"data/DL_utype_2layer/joint_DL_\"+str(epoch)+\".pt\")\n",
    "#     if (check_val_acc < val_macro_f1): #early stopping\n",
    "#         check_val_acc = val_macro_f1\n",
    "#         print (\"Model saved at epoch :\", epoch)\n",
    "#         torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "#         best_epoch = epoch\n",
    "        \n",
    "#print(\"Best model found at epoch : \", best_epoch)        \n",
    "#torch.save(model.state_dict(),\"data/joint_DLT.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_plots(train_losses, val_losses, train_accs, test_accs):\n",
    "    \"\"\"Plot\n",
    "\n",
    "        Plot two figures: loss vs. epoch and accuracy vs. epoch\n",
    "    \"\"\"\n",
    "    n = len(train_losses)\n",
    "    xs = np.arange(n)\n",
    "\n",
    "    # plot losses\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_losses, '--', linewidth=2, label='train loss')\n",
    "    ax.plot(xs, val_losses, '-', linewidth=2, label='validation loss')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.savefig('loss_DL_utype_2layer_cls.png')\n",
    "\n",
    "    # plot train and test accuracies\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs, train_accs, '--', linewidth=2, label='train')\n",
    "    ax.plot(xs, test_accs, '-', linewidth=2, label='validation')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Macro-avg F1\")\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.savefig('accuracy_DL_utype_2layer_cls.png')\n",
    "    \n",
    "save_plots(per_epoch_train_loss, per_epoch_val_loss, per_epoch_train_f1, per_epoch_val_f1)\n",
    "# print(per_epoch_train_loss)\n",
    "# print(per_epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########........Load the trained model and make prediction and calculate accuracy.....\n",
    "def make_prediction(training_data_des, training_data_loc, ground_truths):\n",
    "    for epoch in range(0,30):\n",
    "        model = JointDL()\n",
    "        model.load_state_dict(torch.load(\"data/DL_utype_2layer/joint_DL_\"+str(epoch)+\".pt\")) \n",
    "        predictions =[]\n",
    "        for i in range (0,len(training_data_des)):\n",
    "            prediction_joint = model(training_data_des[i], training_data_loc[i])\n",
    "            pred = torch.argmax(prediction_joint, dim=1)\n",
    "            predictions.append(pred.item())\n",
    "        #accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /10\n",
    "        accuracy = np.sum(np.array(predictions) == np.array(ground_truths)) /len(training_data_des)\n",
    "        macro_f1 = f1_score(ground_truths, predictions, average='macro')\n",
    "        print('epoch :', epoch, 'Testing accuracy, macro_f1:', accuracy, macro_f1)\n",
    "        #return accuracy, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "########.....Load Test data.......\n",
    "with open(\"data/test.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "test_data = data[:] \n",
    "#print(test_data, len(test_data)) #262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no location for user:  vaibhavatttc\n"
     ]
    }
   ],
   "source": [
    "#####prepare testing data for neural net #########\n",
    "testing_data_des, testing_data_loc =  nn_input(test_data,df)\n",
    "#testing_data_net =  nn_input_network(test_data,df)\n",
    "test_gt = find_groundtruth(test_data, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "############........Calculate Validation Accuracy.......\n",
    "# test_accuracy, test_macro_f1 = make_prediction(testing_data_des, testing_data_loc, testing_data_net, test_gt)\n",
    "# print('Testing accuracy, macro_f1:', test_accuracy, test_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 Testing accuracy, macro_f1: 0.5267175572519084 0.42826791007398773\n",
      "epoch : 1 Testing accuracy, macro_f1: 0.6793893129770993 0.5864226490936945\n",
      "epoch : 2 Testing accuracy, macro_f1: 0.7251908396946565 0.6578747737284322\n",
      "epoch : 3 Testing accuracy, macro_f1: 0.732824427480916 0.6728561445542578\n",
      "epoch : 4 Testing accuracy, macro_f1: 0.7251908396946565 0.6725675500803597\n",
      "epoch : 5 Testing accuracy, macro_f1: 0.7251908396946565 0.6704510218463707\n",
      "epoch : 6 Testing accuracy, macro_f1: 0.7290076335877863 0.6859921986750895\n",
      "epoch : 7 Testing accuracy, macro_f1: 0.732824427480916 0.6829913643601856\n",
      "epoch : 8 Testing accuracy, macro_f1: 0.7251908396946565 0.6833462074397189\n",
      "epoch : 9 Testing accuracy, macro_f1: 0.7251908396946565 0.6885224983361214\n",
      "epoch : 10 Testing accuracy, macro_f1: 0.7251908396946565 0.6885224983361214\n",
      "epoch : 11 Testing accuracy, macro_f1: 0.7290076335877863 0.6912347618610695\n",
      "epoch : 12 Testing accuracy, macro_f1: 0.7290076335877863 0.6895197689113558\n",
      "epoch : 13 Testing accuracy, macro_f1: 0.732824427480916 0.6928418803418804\n",
      "epoch : 14 Testing accuracy, macro_f1: 0.732824427480916 0.6926939893261137\n",
      "epoch : 15 Testing accuracy, macro_f1: 0.7213740458015268 0.6838521400778209\n",
      "epoch : 16 Testing accuracy, macro_f1: 0.7175572519083969 0.6805255867231413\n",
      "epoch : 17 Testing accuracy, macro_f1: 0.7175572519083969 0.6805255867231413\n",
      "epoch : 18 Testing accuracy, macro_f1: 0.7213740458015268 0.6838521400778209\n",
      "epoch : 19 Testing accuracy, macro_f1: 0.7213740458015268 0.6838521400778209\n",
      "epoch : 20 Testing accuracy, macro_f1: 0.7213740458015268 0.6854186944870174\n",
      "epoch : 21 Testing accuracy, macro_f1: 0.7175572519083969 0.6826710665754491\n",
      "epoch : 22 Testing accuracy, macro_f1: 0.7251908396946565 0.6893038821954485\n",
      "epoch : 23 Testing accuracy, macro_f1: 0.7251908396946565 0.6893038821954485\n",
      "epoch : 24 Testing accuracy, macro_f1: 0.7251908396946565 0.6892831541218638\n",
      "epoch : 25 Testing accuracy, macro_f1: 0.7213740458015268 0.6865832008302045\n",
      "epoch : 26 Testing accuracy, macro_f1: 0.7251908396946565 0.6938821412505624\n",
      "epoch : 27 Testing accuracy, macro_f1: 0.7251908396946565 0.6983071129412594\n",
      "epoch : 28 Testing accuracy, macro_f1: 0.732824427480916 0.7076613086409855\n",
      "epoch : 29 Testing accuracy, macro_f1: 0.732824427480916 0.7076613086409855\n"
     ]
    }
   ],
   "source": [
    "make_prediction(testing_data_des, testing_data_loc, test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
